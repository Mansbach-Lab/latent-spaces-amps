{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Notebook that will go through a handful of analyses step by step instead of all at once as in total_model_analysis.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Anaconda\\envs\\amp21\\lib\\site-packages\\scipy\\__init__.py:138: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion} is required for this version of \"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "from transvae import trans_models\n",
    "from transvae.transformer_models import TransVAE\n",
    "from transvae.rnn_models import RNN, RNNAttn\n",
    "from transvae.wae_models import WAE\n",
    "from transvae.aae_models import AAE\n",
    "from transvae.tvae_util import *\n",
    "from transvae import analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import IPython.display as Disp\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial will demonstrate how to visualize and interpret model memory and how to evaluate model performance on a number of metrics. Full scripts for training models, generating samples and calculating attention weights are provided and instructions on how to use those scripts are included in the README. The functions demonstrated in this tutorial do not have pre-written high throughput scripts but can be used on larger input sizes if desired. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Reconstruction Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A set of metrics on reconstruction accuracy of the different models is presented below. Some parameters need to be selected:\n",
    "<ul>\n",
    "    <li>data size: int --how many samples from the data to laod\n",
    "    <li>data selection: string  --training, testing, full_no_shuffle\n",
    "    <li>model_src: string --path to model checkpoint\n",
    "    <li>models : RNN, WAE, AAE, RNNAttn, TransVAE --model selectiong from listed\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are we using cuda?:  False\n"
     ]
    }
   ],
   "source": [
    "num_sequences =100\n",
    "data_selection = \"data\"\n",
    "model_src = \"checkpointz/amp_aae//sunistarv4_emb128_latent128//300_aae-128_peptide.ckpt\"\n",
    "#if loss log files are present you can uncomment and run the cell below to plots loss curves\n",
    "#src = 'checkpointz//amp_aae//sunistarv4_emb128_latent128//log_aae-128_peptide.txt' #src of the loss output file\n",
    "model = AAE(load_fn=model_src)\n",
    "#manual override of HARDWARE device for CKPT loading\n",
    "model = AAE()\n",
    "model.params['HARDWARE']= 'cpu'\n",
    "model.load(checkpoint_path=model_src)\n",
    "print('Are we using cuda?: ',next(model.model.parameters()).is_cuda)\n",
    "\n",
    "save_dir= \"model_analyses//\"+model.name+\"test\" #each model will have its own directory\n",
    "if not os.path.exists(save_dir):os.mkdir(save_dir) \n",
    "save_dir= save_dir+\"//\" #actually enter the folder that was created above\n",
    "\n",
    "save_df = pd.DataFrame() #this will hold the number variables and save to CSV\n",
    "\n",
    "gpu = False\n",
    "\n",
    "if \"data\" in data_selection:\n",
    "    data = pd.read_csv('data//peptide_test.txt').to_numpy() \n",
    "data_1D = data[:num_sequences,0] #gets rid of extra dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Needs :\"src\" above with log files from training \"not on github repo\"\n",
    "# tot_loss = analysis.plot_loss_by_type(src,loss_types=['tot_loss'])\n",
    "# plt.savefig(save_dir+'tot_loss.png',dpi=200)\n",
    "# recon_loss = analysis.plot_loss_by_type(src,loss_types=['recon_loss'])\n",
    "# plt.savefig(save_dir+'recon_loss.png',dpi=200)\n",
    "# kld_loss = analysis.plot_loss_by_type(src,loss_types=['kld_loss'])\n",
    "\n",
    "# plt.savefig(save_dir+'kld_loss.png',dpi=200)\n",
    "# prob_bce_loss = analysis.plot_loss_by_type(src,loss_types=['prop_bce_loss'])\n",
    "# plt.savefig(save_dir+'prob_bce_loss.png',dpi=200)\n",
    "# if 'aae' in src:\n",
    "#     disc_loss = analysis.plot_loss_by_type(src,loss_types=['disc_loss'])\n",
    "#     plt.savefig(save_dir+'disc_loss.png',dpi=200)\n",
    "# if 'wae' in src:\n",
    "#     mmd_loss = analysis.plot_loss_by_type(src,loss_types=['mmd_loss'])\n",
    "#     plt.savefig(save_dir+'mmd_loss.png',dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  100\n",
      "MQQANFCEYIRELREEAKMPLRKLAAIMDIDQGTLSKLE <- Original\n",
      "MQQANFCEYIRELREEAKMPLRKLAAIMDIDQGTLSKLE <- Reconstruction\n",
      "\n",
      "\n",
      "MIFSNKWFAAKLFLKTTDFLIFMVKLLPEDEQSQNLKSTFCNYLS <- Original\n",
      "MIFSNKWAFAFLKKLTTDFLIKMVLFLPEQDESELSTKFNCQFLYR <- Reconstruction\n",
      "\n",
      "\n",
      "NQQLLGAM <- Original\n",
      "NQQLLGAM <- Reconstruction\n",
      "\n",
      "\n",
      "CDKVPAVPWMRRSCRIPPRPRTGRGNGRGLRCAGGRPPAARWSL <- Original\n",
      "CDAVKPVPMCGRPPCSRRIPRPTGGRNRGGLCRGARPPAGWALSR <- Reconstruction\n",
      "\n",
      "\n",
      "MPKKTIIKARDEQIQAMKEKKQAIERLREAAIT <- Original\n",
      "MPKKTIIKARDEQIQAMKEKKQAIERLREAAIT <- Reconstruction\n",
      "\n",
      "\n",
      "MAGRTALSALINKLMDRSKTVFFRHFNLLISLQSLSHWLYRTHVK <- Original\n",
      "MAGRTALSALINKLMDRSKTVFFRHFNLLISLQSLHSLYWRTHVK <- Reconstruction\n",
      "\n",
      "\n",
      "PPCIVDPHPTPMGPID <- Original\n",
      "PPCIVDPHPTPMGPID <- Reconstruction\n",
      "\n",
      "\n",
      "CDFIF <- Original\n",
      "CDIFF <- Reconstruction\n",
      "\n",
      "\n",
      "MANSDSIDNNNQGSMYNKDKTVTTNQRRF <- Original\n",
      "MANSDSIDNNNQGSMYNKDKTVTTNQRRF <- Reconstruction\n",
      "\n",
      "\n",
      "MLSFYVDTANISASYVDNSNQTQVFPLTYQGVAITFITVAGGYMA <- Original\n",
      "MLSFYVDTANISASYVDNSNQTQVFPLTYQGVIATFIAGVTGYTL <- Reconstruction\n",
      "\n",
      "\n",
      "MFVTRIYLEFWLWLHCDSKPTGRQLRAPVATQK <- Original\n",
      "MFVTRIYLEFWLWLHCDSKPTGRQLPARVAQTK <- Reconstruction\n",
      "\n",
      "\n",
      "FKLGSFLKKAWKSKLAKKLRAKGKEMLKDYAKGLLEGGSEEVPGQ <- Original\n",
      "FKLGSFLKKAWKSKLAKLKKRAGKMELKDKYLGAGELSEGGPEVQ <- Reconstruction\n",
      "\n",
      "\n",
      "MSWFVEIIQPLMRQASAGIQSVKKLKTSAMNSLFNLQNRITNQSRQQ <- Original\n",
      "MSWFVEIIQPRLMQASGAMKLIKSAKVQNSQLTFLSNGNRSITQQQN <- Reconstruction\n",
      "\n",
      "\n",
      "ESKEWIRRS <- Original\n",
      "ESKEWIRRS <- Reconstruction\n",
      "\n",
      "\n",
      "MQGVLFIVASLIAFATSQDCP <- Original\n",
      "MQGVLFIVASLIAFATSQDCP <- Reconstruction\n",
      "\n",
      "\n",
      "MAWTGLVG <- Original\n",
      "MAWGTLVG <- Reconstruction\n",
      "\n",
      "\n",
      "GLRKRLRKFRNKIKEKKKKIGQKIQGLLPKLAPRTDY <- Original\n",
      "GLRKRLRKFRNKIKEKKKKIGKQIQGLLKPLAPTRDY <- Reconstruction\n",
      "\n",
      "\n",
      "GMEVMHERNAHNFPLDLAALEVPSLNG <- Original\n",
      "GMEVMHERNAHNFPLDLAALEVPSLNG <- Reconstruction\n",
      "\n",
      "\n",
      "GYGCPFNQYQCHSHCSGIRGYKGGYCKGLFKQTCNCY <- Original\n",
      "GYGCPFNQYHDHCQKSIGGRYHVGGKGCFLKQTCNCY <- Reconstruction\n",
      "\n",
      "\n",
      "MAIMTALALLALFLIKESKDKDYEL <- Original\n",
      "MAIMTALALLALFLIKESKDKDYEL <- Reconstruction\n",
      "\n",
      "\n",
      "MKINPTTSRP <- Original\n",
      "MKINPTTSRP <- Reconstruction\n",
      "\n",
      "\n",
      "MGGNRGRDTSARHRPSGSHPDTGTIRDGSEEALEAHGVK <- Original\n",
      "MGGNRGRDTSARHRPSGSHPDTGTIRDGESEALEHAGVK <- Reconstruction\n",
      "\n",
      "\n",
      "MGGKLLANPLCNKYLTRAIMPATDSSKQMGSYIRVQNWWIF <- Original\n",
      "MGGKLLANLPCNKYLTRAIMPATDSSKQMGSYIRVQNWWIF <- Reconstruction\n",
      "\n",
      "\n",
      "MYRFLLVILSFFFV <- Original\n",
      "MYRFLLVILSFFFV <- Reconstruction\n",
      "\n",
      "\n",
      "MGVECVISGSCAVDKPHSRPEQQIVLGHTEYLDFLRRSKPDQVQRV <- Original\n",
      "MGVECIVSGSCAVKDPHSRPEQHLRIVTEQNYLKLRLSCMDQPVVQ <- Reconstruction\n",
      "\n",
      "\n",
      "MGVIIKLLEELVQNMQ <- Original\n",
      "MGVIIKLLEELVQNMSQ <- Reconstruction\n",
      "\n",
      "\n",
      "LQKYYWRVRGGRWAVLS <- Original\n",
      "LQKYYWRVRGGRWAVLS <- Reconstruction\n",
      "\n",
      "\n",
      "MSPASSPSW <- Original\n",
      "MSPASSPSW <- Reconstruction\n",
      "\n",
      "\n",
      "KPKGIDNRVRGRFKGQYLMPNVGYGSDKRTRHMLSSKFRKVLVHNVRE <- Original\n",
      "KPKGIDNRVRGFRKGQYLMNPVGYSGDKTRRSMHLKSFLKVGDVIDR <- Reconstruction\n",
      "\n",
      "\n",
      "MQGSIGFW <- Original\n",
      "MQGSIGFW <- Reconstruction\n",
      "\n",
      "\n",
      "MCLLLKINHILMNINSQPEPSWTFLHGNTFWSLLKQKRFAIYCKVEEYIN <- Original\n",
      "MCLLLKIHNLGINSIQPMSNEPAHFPGNFTWSFLQLKAIYKCRKYEVNEV <- Reconstruction\n",
      "\n",
      "\n",
      "DVSASLAVLPDNFPRYPVGKFFQYDTWRQSTQRL <- Original\n",
      "DVSASLAVLPNDFPRPYGVFKQFYDTWRQSTQRL <- Reconstruction\n",
      "\n",
      "\n",
      "MRGFGDHYTETHRLNIIQESRLARKVIYHAVY <- Original\n",
      "MGRFDGHYTETHRLNIIQELQSRVAIKRHVYAY <- Reconstruction\n",
      "\n",
      "\n",
      "VLSVGVDVTNVEAGKK <- Original\n",
      "VLSVGVDVTNVEAGKK <- Reconstruction\n",
      "\n",
      "\n",
      "NLSSQIPLKRLLKTWTNRYPDAK <- Original\n",
      "NLSSQIPLKRLLKTWTNRYPDAK <- Reconstruction\n",
      "\n",
      "\n",
      "MKPLATGGAAAAAWSAAATAASSSRRRAAALLLRQGRAMTAELS <- Original\n",
      "MKPLATGGAAAAAWSAAATASASRSRARLAALLRQRGMATAELS <- Reconstruction\n",
      "\n",
      "\n",
      "CVISAGWNHKIRCKLTGNC <- Original\n",
      "CVISAGWNHKIRCKLTGNC <- Reconstruction\n",
      "\n",
      "\n",
      "MINISKKTTNFTKNNYRKNEYRKRSQTKSITAYIR <- Original\n",
      "MINISKKTTNFTKNNYRKNEYRKRSQTSKITAYIR <- Reconstruction\n",
      "\n",
      "\n",
      "MVWILLLSDMDLSTHALTAGKHLHAFGVYQDLIGGEALASNQSLYLM <- Original\n",
      "MIVWLLLSDMLSDTAHGLAHKTLTKCVLGHQVGLGHLDTAESLNLSRY <- Reconstruction\n",
      "\n",
      "\n",
      "MKKSKLIIIIGAILLTGFSSCGIFKKGCGCPKFGVIKCADFRCAGVQMMG <- Original\n",
      "MKKSKLIIIITLIGALGSFSGKCIGFCKEIRKVGCKFAVRKACSMNRTCY <- Reconstruction\n",
      "\n",
      "\n",
      "MVVLTHHGMLKLVSTF <- Original\n",
      "MVVLTHHGMLKLVSTF <- Reconstruction\n",
      "\n",
      "\n",
      "MRLLACIQSLHSGLLMKPPSFRYH <- Original\n",
      "MRLLACIQLSHSGLLMKPPSFRYH <- Reconstruction\n",
      "\n",
      "\n",
      "MSILDFAKDLLYDLKYAQDESAPEPGNLRMSERPIIFLVHSMGGLIVKEV <- Original\n",
      "MSILDFAKDLLYDFRAHPDEKDQNPGLPEMGPLSAMIILYVSHGLGKRLNV <- Reconstruction\n",
      "\n",
      "\n",
      "SSGSTVCKMMCRLGYGHLYPSCGCR <- Original\n",
      "SSGSTVCKMMCRLGYGHLYPSCGCR <- Reconstruction\n",
      "\n",
      "\n",
      "RVYDFGL <- Original\n",
      "RVYDFGL <- Reconstruction\n",
      "\n",
      "\n",
      "MIQIISFAK <- Original\n",
      "MIQIISFAK <- Reconstruction\n",
      "\n",
      "\n",
      "LLIIARRRIRKQAHAHSK <- Original\n",
      "LLIIARRRIRKQAHAHSK <- Reconstruction\n",
      "\n",
      "\n",
      "YEKSQLFELAFFISARSYTDLILTPCAVFIPPIQHDDKWEMLG <- Original\n",
      "YEKSQLFELAFFISARSYTDLILTPCAVFIPPIQDHKDWEMLG <- Reconstruction\n",
      "\n",
      "\n",
      "MVHTSMLVRPSLCRNEQWYTTMYVQTQVEVLGSVRATH <- Original\n",
      "MYASGSYRLLPCDGPSTYETYQGYCQTNPLVSGVAMAQ <- Reconstruction\n",
      "\n",
      "\n",
      "MNREEINKMFGVTDEQLDRMAAEYESGDWEGVVGPIVPGGGRSC <- Original\n",
      "MNREEINKMFGVTDEQLRDAMEVQAEWGESGVVYPGVIGPGRSGD <- Reconstruction\n",
      "\n",
      "\n",
      "MFSFAWCKIEMLGL <- Original\n",
      "MFSFAWCKIEMLGL <- Reconstruction\n",
      "\n",
      "\n",
      "MKSIFGIVKLLTSL <- Original\n",
      "MKSIFGIVKLLTSL <- Reconstruction\n",
      "\n",
      "\n",
      "MRPWMTKNEHLHKGRKMSDADSLIFLFAKIKMENQFSF <- Original\n",
      "MRPWMTKNEHLHKGRKMSDADSLIFLFAKIEFMQNKSV <- Reconstruction\n",
      "\n",
      "\n",
      "DRASDGRNAAANEKASDVIALALKGCCSNPVCHLEHSNMCGRRR <- Original\n",
      "DRASDGRNAANAKEASDVIALALKGCDHVLCENPHSCSGMGRCR <- Reconstruction\n",
      "\n",
      "\n",
      "DLSSNQIYGDIPYSVSKLKQLENL <- Original\n",
      "DLSSNQIYGDIPYSVSKLKQLENL <- Reconstruction\n",
      "\n",
      "\n",
      "LEGPEPVRGQPGSKQQLGLLGGMIAGIGALVCVASVSGQAARRARTST <- Original\n",
      "LEPGERVPGPQGSKQQLGLGLGMIAGGIAVLCVSAVSGAQARRARSTT <- Reconstruction\n",
      "\n",
      "\n",
      "ILGPVLGLVGNALGGLIKKI <- Original\n",
      "ILGPVLGLVGNALGGLIKKI <- Reconstruction\n",
      "\n",
      "\n",
      "MPLLLTQYTTGPVLLFYASPFLTTIQRQLENFQKEVYGYYVN <- Original\n",
      "MPLLLTQYTTGPVLLFYASPFLTIQTRQLNEFQKEYVGYYNV <- Reconstruction\n",
      "\n",
      "\n",
      "MIWVALGIIAAQSVLAWVIYFVERADHRRYILDNLGPPPP <- Original\n",
      "MWIVALGIIAAQVSLAWIVVYFERADHRRYLIDNGLPPPM <- Reconstruction\n",
      "\n",
      "\n",
      "MDLAVDEASNNIVASAFIGQFAVFGADGGIGVVPELDVLFQFLL <- Original\n",
      "MDLVADSEANVNISAAFIGQFFAVGDAGIGGVPVNQLVLECLFLL <- Reconstruction\n",
      "\n",
      "\n",
      "ARILAVERYLKDQ <- Original\n",
      "ARILAVERYLKDQ <- Reconstruction\n",
      "\n",
      "\n",
      "IQISHFGRLLAEI <- Original\n",
      "IQISHFGRLLAEI <- Reconstruction\n",
      "\n",
      "\n",
      "MKLVTCPECGGNILEGAPNEYGFECDTCPYPYKEEDLI <- Original\n",
      "MKLVCETRWANGGPINSEVEGWTLCDQNCYWPPYAEESVI <- Reconstruction\n",
      "\n",
      "\n",
      "MFPCFNSRNSLRF <- Original\n",
      "MFPCFNSRNSLRF <- Reconstruction\n",
      "\n",
      "\n",
      "MKRTYQPSKTRRKRTHGFLVRMRTAGGRAVINARRAKGRKRLAV <- Original\n",
      "MKRTYQPSKTRRKRTHGFLVRMRTAGRGAVINARRAKGRKRLAV <- Reconstruction\n",
      "\n",
      "\n",
      "MKPRPTNVQAEKNMREDFKIGRRGAVLSRVGQ <- Original\n",
      "MKPRPTNVQAEKNMREDFKIGRRGAVLSRVGQ <- Reconstruction\n",
      "\n",
      "\n",
      "NLCERASKTWTGNCGNTKHCDNQCKSWEGAKHGACHKRSGKWKCFCYFNC <- Original\n",
      "NLCERASKTWTGNCGNTKHCDNQCKSWEGAKHGACHKRSGKWKCFCYFNC <- Reconstruction\n",
      "\n",
      "\n",
      "MPLTYPPHPKEEASPFRAERI <- Original\n",
      "MPLTYPPHPKEEASPFRAERI <- Reconstruction\n",
      "\n",
      "\n",
      "VTHRETGEVMV <- Original\n",
      "VTHRETGEVMV <- Reconstruction\n",
      "\n",
      "\n",
      "PRNYSDRVNCIRIPPNTPPAQVKKLLF <- Original\n",
      "PRNYSDRVNCIRIPNPPTPAQVKKLLF <- Reconstruction\n",
      "\n",
      "\n",
      "MFGKTAGEALVDLLIDWGVEHIYGMPGDSINSIIEAL <- Original\n",
      "MFGKTAGEALVDLLIDGWEVIHYGMPGDSINSIEIAL <- Reconstruction\n",
      "\n",
      "\n",
      "MTRRVTIQTPLGEQLQFRQLQGREELSQVF <- Original\n",
      "MTRRVTIQTPLGEQLQFRQLQGREELSQFV <- Reconstruction\n",
      "\n",
      "\n",
      "GGAKGVGQSTTTAVVTSLLAIFVANFFLSWLMFQGTGNVEIG <- Original\n",
      "GGAKGVGSQTTTVANALMILAFVFANVSIFLQLMETGFGWNNG <- Reconstruction\n",
      "\n",
      "\n",
      "MKLVNLMAVLMLVVPVVTSAGHPVDINRASAEELAAA <- Original\n",
      "MKLVNLMAVLMLVVPVVTSAGHPVDINRASAEELAAA <- Reconstruction\n",
      "\n",
      "\n",
      "MLIHYSSGWRMIKGYLKTIRYCFSGIGGLSQIGIYRESK <- Original\n",
      "MLIHYSSGWRMIKGYLKTIRYCFSGIGLGSQIGIYRSEK <- Reconstruction\n",
      "\n",
      "\n",
      "MATIPSTINGVTVEFSSTVNKYKDSHHLKSGL <- Original\n",
      "MATIPSTINGVTVEFSSTVNKYDKHSSHLKGL <- Reconstruction\n",
      "\n",
      "\n",
      "MLSTTLPPTLTLNLTLTMPP <- Original\n",
      "MLSTTLPPTLTLNLTLTMPP <- Reconstruction\n",
      "\n",
      "\n",
      "LRYLVQKRKIKN <- Original\n",
      "LRYLVQKRKIKN <- Reconstruction\n",
      "\n",
      "\n",
      "MTIPRLELVAILIIQHKSTQFFT <- Original\n",
      "MTIPRLELVAILIIQHKSTQFFT <- Reconstruction\n",
      "\n",
      "\n",
      "MNDLSSPKNPIVMIALLLGFMIAGVQMNSHKDHFITQPAANLSR <- Original\n",
      "MNDLSSPKNPIVMIALLLGIFMAGVMQNSHKDHIFTQPANALSR <- Reconstruction\n",
      "\n",
      "\n",
      "MCINAMVTLFIGLSSPDTLS <- Original\n",
      "MCINAMVTLFIGLSSPDTLS <- Reconstruction\n",
      "\n",
      "\n",
      "LVQPRGPRSGPGPWQGGRRKFRRQRPRLSHKGPMPF <- Original\n",
      "LVQPRGPRSGPGPWQGGRRKFRRQRPLMSKIERHML <- Reconstruction\n",
      "\n",
      "\n",
      "MRDLKTYLSAAPVLSTLWFGSLAGLLIEINRFFPDALTFPFFNSSY <- Original\n",
      "MRDLKTYLASAPVLSTWLLGASFGLILEIRNFAPFLTDFPFSNFYS <- Reconstruction\n",
      "\n",
      "\n",
      "GSFFATPDDRH <- Original\n",
      "GSFFATPDDRH <- Reconstruction\n",
      "\n",
      "\n",
      "ELQSDELLFAAK <- Original\n",
      "ELQSDELLFAAK <- Reconstruction\n",
      "\n",
      "\n",
      "MARVLGVSPSTVSREIRRIGDWEIDLVMGAKHRGDWSR <- Original\n",
      "MARVLGVSPSTVSREIRRIGDWEIDLVMGAKHRGDWSR <- Reconstruction\n",
      "\n",
      "\n",
      "PIPLNVLFH <- Original\n",
      "PIPLNVLFH <- Reconstruction\n",
      "\n",
      "\n",
      "MGTGRSGAAMGANRLMSLLSRSTRGM <- Original\n",
      "MGTGRSGAAMGANRLMSLLSRSTRGM <- Reconstruction\n",
      "\n",
      "\n",
      "MKVRNSLRSLKAKPGAQVVRRRGRVFVINKKDPRFKARQG <- Original\n",
      "MKVRNSLRSLKAKPGAQVVRRRRGVFVINKDPKFRKARQG <- Reconstruction\n",
      "\n",
      "\n",
      "MTTNAAAVAAPRVRTGGPKDDGPQLLEHALGWVLVVVFAMLVTRLGLL <- Original\n",
      "MTTNAAAVAAPRVRTGGPKDGDLPQELHLAGWLVVVFMVALVTLRMGLL <- Reconstruction\n",
      "\n",
      "\n",
      "MRFWGIRVGMLIATLIAVSMLTF <- Original\n",
      "MRFWGIRVGMLIATLIAVSMLTF <- Reconstruction\n",
      "\n",
      "\n",
      "CLDGTLPGYHLHRGFGSGANSWLIQLE <- Original\n",
      "CLDGTLPGYHLHGRFGSGANSWLIQLE <- Reconstruction\n",
      "\n",
      "\n",
      "LKYFDAPGSAMYMQEYLYSLKNHRYTATMLQHITEDRDGAEG <- Original\n",
      "LKYFADPGSAMYMQEYLYSLKNHRYTATMLHQITEDGDRAEG <- Reconstruction\n",
      "\n",
      "\n",
      "MEGKEYIVQDGDCIIFKFNV <- Original\n",
      "MEGKEYIVQDGDCIIFKFNV <- Reconstruction\n",
      "\n",
      "\n",
      "DASDGQADDLAAG <- Original\n",
      "DASDGQADDLAAG <- Reconstruction\n",
      "\n",
      "\n",
      "MQISVFLTFIMPARFTYFSLKGLHYDPSKPNDSALFMLGLLFI <- Original\n",
      "MQISVFLTFIMPARFTYSFLKGLHDYPSPKNSADLMFLGLLIF <- Reconstruction\n",
      "\n",
      "\n",
      "EAAELAKGSFK <- Original\n",
      "EAAELAKGSFK <- Reconstruction\n",
      "\n",
      "\n",
      "LENNNAVVTYNLQ <- Original\n",
      "LENNNAVVTYNLQ <- Reconstruction\n",
      "\n",
      "\n",
      "LKDLEEGILALMREMDNGTPRAGQILKQTYDKFDTNMRSDDALLKN <- Original\n",
      "LKDLEEGILALMMERDGENRTIARPQTFKMKNVDHTDQLDGDLLKST <- Reconstruction\n",
      "\n",
      "\n",
      "ETENGGWT <- Original\n",
      "ETENGGWT <- Reconstruction\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "LOAD=False\n",
    "if LOAD: #this allows loading of reconstructed sequences from a file to save time\n",
    "    recon_src = 'slurm_analyses//rnn-128_peptide_latent_32//saved_info.csv'\n",
    "    recon_df = pd.read_csv(recon_src)\n",
    "    reconstructed_seq = recon_df['reconstructions'].to_list()[:num_sequences]\n",
    "    props = torch.Tensor(recon_df['predicted properties'][:num_sequences])\n",
    "else:\n",
    "    model.params['BATCH_SIZE'] = 100 #batch size must match total size of input data\n",
    "    reconstructed_seq, props = model.reconstruct(data[:num_sequences], log=False, return_mems=False)\n",
    "    for og_token, reconstructed_token in zip(data_1D, reconstructed_seq):\n",
    "        print('{} <- Original'.format(og_token))\n",
    "        print('{} <- Reconstruction'.format(reconstructed_token))\n",
    "        print('\\n')\n",
    "\n",
    "if gpu:\n",
    "    torch.cuda.empty_cache() #free allocated CUDA memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experimental section using reconstructions from previous run to save time on analyses\n",
    "\n",
    "# training = pd.read_csv('notebooks//example_data//train_test//peptide_train.txt').to_numpy()\n",
    "# train_idx_list = [np.where(data==training[idx][0]) for idx in range(len(training))]\n",
    "\n",
    "# testing = pd.read_csv('notebooks//example_data//train_test//peptide_test.txt').to_numpy()\n",
    "# test_idx_list = [np.where(data==testing[idx][0]) for idx in range(len(testing))]\n",
    "\n",
    "# test=True\n",
    "# train=False\n",
    "# if test:\n",
    "#     batch_recon_len = len(reconstructed_seq)\n",
    "#     reconstructed_seq = [reconstructed_seq[test_idx_list[i][0][0]] for i in range(len(test_idx_list)) if test_idx_list[i][0][0]<batch_recon_len]\n",
    "#     data_1D= [data_1D[test_idx_list[i][0][0]] for i in range(len(test_idx_list)) if test_idx_list[i][0][0]<batch_recon_len]\n",
    "#     props = [props[test_idx_list[i][0][0]] for i in range(len(test_idx_list)) if test_idx_list[i][0][0]<batch_recon_len]\n",
    "#     props=torch.Tensor(props)\n",
    "#     data = testing[:][0]\n",
    "#     true_props_data = pd.read_csv('notebooks//example_data//function_full_no_shuff.txt').to_numpy()\n",
    "#     true_props = true_props_data[0:num_sequences,0]\n",
    "#     true_props= [true_props[test_idx_list[i][0][0]] for i in range(len(test_idx_list)) if test_idx_list[i][0][0]<batch_recon_len]\n",
    "# if train:\n",
    "#     batch_recon_len = len(reconstructed_seq)\n",
    "#     reconstructed_seq = [reconstructed_seq[train_idx_list[i][0][0]] for i in range(len(train_idx_list)) if train_idx_list[i][0][0]<batch_recon_len]\n",
    "#     data_1D= [data_1D[train_idx_list[i][0][0]] for i in range(len(train_idx_list)) if train_idx_list[i][0][0]<batch_recon_len]\n",
    "#     props = [props[train_idx_list[i][0][0]] for i in range(len(train_idx_list)) if train_idx_list[i][0][0]<batch_recon_len]\n",
    "#     props=torch.Tensor(props)\n",
    "#     data = training[:][0]\n",
    "#     true_props_data = pd.read_csv('notebooks//example_data//function_full_no_shuff.txt').to_numpy()\n",
    "#     true_props = true_props_data[0:num_sequences,0]\n",
    "#     true_props= [true_props[train_idx_list[i][0][0]] for i in range(len(train_idx_list)) if train_idx_list[i][0][0]<batch_recon_len]\n",
    "\n",
    "# n_data_1D=[]\n",
    "# n_reconstructed_seq=[]\n",
    "# for idx,seq in enumerate(data_1D):\n",
    "#     if len(seq)<50:\n",
    "#         n_data_1D.append(seq)\n",
    "#         n_reconstructed_seq.append(reconstructed_seq[idx])\n",
    "# data_1D=n_data_1D\n",
    "# reconstructed_seq=n_reconstructed_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_df['reconstructions'] = reconstructed_seq #placing the saves on a line separate from the ops allows for editing\n",
    "save_df['predicted properties'] = [prop.item() for prop in props[:len(reconstructed_seq)]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>MCC info:\n",
    "    <li>+1 represents a perfect prediction\n",
    "    <li>0 no better than random prediction\n",
    "    <li>âˆ’1 indicates total disagreement between prediction and observation.\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_props_data = pd.read_csv('data//function_test.txt').to_numpy()\n",
    "true_props = true_props_data[0:num_sequences,0]\n",
    "prop_acc, prop_conf, MCC=calc_property_accuracies(props[:len(reconstructed_seq)],true_props[:len(reconstructed_seq)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_df['property prediction accuracy'] = prop_acc\n",
    "save_df['property prediction confidence'] = prop_conf\n",
    "save_df['MCC'] = MCC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Token accuracies are accuracies per token, \n",
    "<ul>\n",
    "    <li>sequence accuracies are accuracies per sequence\n",
    "    <li>token accuracies are accuracies for each token averaged over all tokens in input dataset\n",
    "    <li>position accuracies are per sequence position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we tokenize the input and reconstructed smiles\n",
    "input_sequences = []\n",
    "for seq in data_1D:\n",
    "    input_sequences.append(peptide_tokenizer(seq))\n",
    "output_sequences = []\n",
    "for seq in reconstructed_seq:\n",
    "    output_sequences.append(peptide_tokenizer(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "seq_accs, tok_accs, pos_accs, seq_conf, tok_conf, pos_conf  = calc_reconstruction_accuracies(input_sequences, output_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_df['sequence accuracy'] = seq_accs\n",
    "save_df['sequence confidence'] = seq_conf\n",
    "save_df['token accuracy'] = tok_accs\n",
    "save_df['token confidence'] = tok_conf\n",
    "\n",
    "save_df = pd.concat([pd.DataFrame({'position_accs':pos_accs,'position_confidence':pos_conf }), save_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the accuracy on token position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pos_accs)\n",
    "#plt.plot(pos_accs_pep)\n",
    "plt.xlabel('Sequence Position')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.savefig(save_dir+'token_position_accuracy.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Model Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The memory of a model is analogous to the probability distribution of molecular embeddings that it has learned during training. A single molecular embedding is the size 128 vector at the center of the variational bottleneck. Each model has a built-in method for calculating and returning the model memory for a set of input structures, `calc_mems()`. ***(note - we plot the mean vector rather than the reparameterized vector so we can identify and analyze the meaningful latent dimensions)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if model.model_type =='aae':\n",
    "    mus, _, _ = model.calc_mems(data[:50_000], log=False, save=False) \n",
    "elif model.model_type == 'wae':\n",
    "    mus, _, _ = model.calc_mems(data[:50_000], log=False, save=False) \n",
    "else:\n",
    "    mems, mus, logvars = model.calc_mems(data[:], log=False, save=False) #subset size 1200*35=42000 would be ok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shannon information entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transvae.tvae_util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the list of entropies for each latent dim\n",
    "vae_entropy_mus = calc_entropy(mus)\n",
    "save_df = pd.concat([save_df,pd.DataFrame({'mu_entropies':vae_entropy_mus})], axis=1)\n",
    "if model.model_type != 'wae' and model.model_type!= 'aae': #these don't have a variational type bottleneck\n",
    "    vae_entropy_mems  = calc_entropy(mems)\n",
    "    save_df = pd.concat([save_df,pd.DataFrame({'mem_entropies':vae_entropy_mems})], axis=1)\n",
    "    vae_entropy_logvars = calc_entropy(logvars)\n",
    "    save_df = pd.concat([save_df,pd.DataFrame({'logvar_entropies':vae_entropy_logvars})], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6,3))\n",
    "\n",
    "plt.bar(range(len(vae_entropy_mus)), vae_entropy_mus)\n",
    "plt.xlabel('Latent Dimension')\n",
    "plt.ylabel('Entropy (bits)')\n",
    "plt.savefig(save_dir+'mem_entropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model.model_type != 'wae' and model.model_type!= 'aae':\n",
    "    fig = plt.figure(figsize=(6,3))\n",
    "    plt.bar(range(len(vae_entropy_mems)), vae_entropy_mems)\n",
    "    plt.xlabel('Latent Dimension')\n",
    "    plt.ylabel('Entropy (bits)')\n",
    "    plt.savefig(save_dir+'mu_entropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model.model_type != 'wae' and model.model_type!= 'aae':\n",
    "    fig = plt.figure(figsize=(6,3))\n",
    "    plt.bar(range(len(vae_entropy_logvars)), vae_entropy_logvars)\n",
    "    plt.xlabel('Latent Dimension')\n",
    "    plt.ylabel('Entropy (bits)')\n",
    "    plt.savefig(save_dir+'logvar_entropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that some dimensions have significantly more information contained across the 25 samples than others and they correspond with the selective memory visualization shown above. We can sum the entropy of all dimensions to find the full model entropy. Again, note that we would need a larger sample size to converge the model entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,10))\n",
    "plt.imshow(mus[:100], vmin=-6, vmax=6, aspect='auto', cmap='seismic')\n",
    "plt.rc('font', size=10)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=40)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=30)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=10)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=10)    # fontsize of the tick labels\n",
    "plt.xticks()\n",
    "plt.yticks()\n",
    "plt.ylabel(\"Embedded peptides\", )\n",
    "plt.xlabel(\"Latent dimensions\")\n",
    "plt.title('Latent Embedding of a 2000 Epoch RNN Model')\n",
    "plt.colorbar(fraction=0.046, pad=0.04)\n",
    "plt.savefig(save_dir+'mus.png', dpi=200, transparency=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Evaluate the trustworthiness of the mapping from raw input data to latent space manifold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn import metrics\n",
    "from sklearn.manifold import trustworthiness\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import NullFormatter\n",
    "import plotly.express as px\n",
    "\n",
    "true_prop_src = 'data\\\\function_test.txt' #if property predictor load the true labels\n",
    "#create random index and re-index ordered memory list creating n random sub-lists (ideally resulting in IID random lists)\n",
    "random_idx = np.random.permutation(np.arange(stop=mus.shape[0]))\n",
    "mus = mus[random_idx]\n",
    "data = data[random_idx]\n",
    "\n",
    "subsample_start=0\n",
    "subsample_length=mus.shape[0] #this may change depending on batch size\n",
    "\n",
    "#(for length based coloring): record all peptide lengths iterating through input\n",
    "pep_lengths = []\n",
    "for idx, pep in enumerate(data[subsample_start:(subsample_start+subsample_length)]):\n",
    "    pep_lengths.append( len(pep[0]) )   \n",
    "#(for function based coloring): pull function from csv with peptide functions\n",
    "s_to_f =pd.read_csv(true_prop_src)    \n",
    "function = s_to_f['peptides'][subsample_start:(subsample_start+subsample_length)]\n",
    "function = function[random_idx] #account for random permutation\n",
    "\n",
    "pca = PCA(n_components=5)\n",
    "pca_batch =pca.fit_transform(X=mus[:])\n",
    "\n",
    "#plot format dictionnaries\n",
    "titles={'text':'{}'.format(model.model_type.replace(\"_\",\" \").upper()),\n",
    "                      'x':0.5,'xanchor':'center','yanchor':'top','font_size':40}\n",
    "general_fonts={'family':\"Helvetica\",'size':30,'color':\"Black\"}\n",
    "colorbar_fmt={'title_font_size':30,'thickness':15,'ticks':'','title_text':'Lengths',\n",
    "                           'ticklabelposition':\"outside bottom\"}\n",
    "\n",
    "fig = px.scatter(pd.DataFrame({\"PC1\":pca_batch[:,0],\"PC2\":pca_batch[:,1], \"lengths\":pep_lengths}),\n",
    "            symbol_sequence=['hexagon2'],x='PC1', y='PC2', color=\"lengths\",\n",
    "            color_continuous_scale='Jet',template='simple_white', opacity=0.9)\n",
    "fig.update_traces(marker=dict(size=9))\n",
    "fig.update_layout(title=titles,xaxis_title=\"PC1\", yaxis_title=\"PC2\",font=general_fonts)\n",
    "fig.update_coloraxes(colorbar=colorbar_fmt)\n",
    "fig.write_image(save_dir+'pca_length.png', width=900, height=600)\n",
    "\n",
    "fig = px.scatter(pd.DataFrame({\"PC1\":pca_batch[:,0],\"PC2\":pca_batch[:,1], \n",
    "                                \"Function\":list(map(lambda itm: \"AMP\" if itm==1 else \"NON-AMP\",function))}),\n",
    "                                x='PC1', y='PC2', color=\"Function\",symbol_sequence=['x-thin-open','circle'],\n",
    "                                template='simple_white',symbol='Function', opacity=0.8) \n",
    "fig.update_traces(marker=dict(size=9))\n",
    "fig.update_layout(title=titles,xaxis_title=\"PC1\",yaxis_title=\"PC2\",font=general_fonts)\n",
    "fig.write_image(save_dir+'pca_function.png', width=900, height=600)\n",
    "# #create n subsamples and calculate silhouette score for each\n",
    "# latent_mem_func_subsamples = []\n",
    "# pca_func_subsamples = []\n",
    "# n=20\n",
    "# for s in range(n):\n",
    "#     s_len = len(mus)//n #sample lengths\n",
    "#     mem_func_sil = metrics.silhouette_score(mus[s_len*s:s_len*(s+1)], function[s_len*s:s_len*(s+1)], metric='euclidean')\n",
    "#     latent_mem_func_subsamples.append(mem_func_sil)\n",
    "#     XY = [i for i in zip(pca_batch[s_len*s:s_len*(s+1),0], pca_batch[s_len*s:s_len*(s+1),1])]\n",
    "#     pca_func_sil = metrics.silhouette_score(XY, function[s_len*s:s_len*(s+1)], metric='euclidean')\n",
    "#     pca_func_subsamples.append(pca_func_sil)\n",
    "# save_df = pd.concat([save_df,pd.DataFrame({'latent_mem_func_silhouette':latent_mem_func_subsamples})], axis=1)\n",
    "# save_df = pd.concat([save_df,pd.DataFrame({'pca_func_silhouette':pca_func_subsamples})], axis=1)\n",
    "\n",
    "# Plot the explained variances\n",
    "features = range(pca.n_components_)\n",
    "plt.bar(features, pca.explained_variance_ratio_*100, color='black')\n",
    "plt.xlabel('PCA features')\n",
    "plt.ylabel('variance %')\n",
    "plt.xticks(features)\n",
    "plt.savefig(save_dir+'variance_explained.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Trustworthiness, Continuity, Steadiness, Cohesiveness </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import coranking #coranking.readthedocs.io\n",
    "from coranking.metrics import trustworthiness, continuity, LCMC\n",
    "from transvae.snc import SNC #github.com/hj-n/steadiness-cohesiveness\n",
    "\n",
    "trust_subsamples = []\n",
    "cont_subsamples = []\n",
    "lcmc_subsamples = []\n",
    "steadiness_subsamples = []\n",
    "cohesiveness_subsamples = []\n",
    "\n",
    "n=35\n",
    "parameter = { \"k\": 50,\"alpha\": 0.1 } #for steadiness and cohesiveness\n",
    "for s in range(n):\n",
    "    s_len = len(mus)//n #sample lengths\n",
    "    Q = coranking.coranking_matrix(mus[s_len*s:s_len*(s+1)], pca_batch[s_len*s:s_len*(s+1)])\n",
    "    trust_subsamples.append( np.mean(trustworthiness(Q, min_k=1, max_k=50)) )\n",
    "    cont_subsamples.append( np.mean(continuity(Q, min_k=1, max_k=50)) )\n",
    "    lcmc_subsamples.append( np.mean(LCMC(Q, min_k=1, max_k=50)) )\n",
    "    print(n,trust_subsamples[s],cont_subsamples[s],lcmc_subsamples[s])\n",
    "    \n",
    "    metrics = SNC(raw=mus[s_len*s:s_len*(s+1)], emb=pca_batch[s_len*s:s_len*(s+1)], iteration=300, dist_parameter=parameter)\n",
    "    metrics.fit() #solve for steadiness and cohesiveness\n",
    "    steadiness_subsamples.append(metrics.steadiness())\n",
    "    cohesiveness_subsamples.append(metrics.cohesiveness())\n",
    "    print(metrics.steadiness(),metrics.cohesiveness())\n",
    "    Q=0 #trying to free RAM\n",
    "    metrics=0\n",
    "    torch.cuda.empty_cache() #free allocated CUDA memory\n",
    "    \n",
    "save_df = pd.concat([save_df,pd.DataFrame({'latent_to_PCA_trustworthiness':trust_subsamples})], axis=1)\n",
    "save_df = pd.concat([save_df,pd.DataFrame({'latent_to_PCA_continuity':cont_subsamples})], axis=1)\n",
    "save_df = pd.concat([save_df,pd.DataFrame({'latent_to_PCA_lcmc':lcmc_subsamples})], axis=1)\n",
    "save_df = pd.concat([save_df,pd.DataFrame({'latent_to_PCA_steadiness':steadiness_subsamples})], axis=1)\n",
    "save_df = pd.concat([save_df,pd.DataFrame({'latent_to_PCA_cohesiveness':cohesiveness_subsamples})], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_df.to_csv(save_dir+\"saved_info.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amp21",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "686011dad3443993f8e7a9bdd4f15acb7c0d9d5c12f48e0b358ae8d8d9013b02"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
