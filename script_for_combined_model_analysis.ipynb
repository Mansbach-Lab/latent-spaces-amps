{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8307e206",
   "metadata": {},
   "source": [
    "<h3> This notebook will run the total model analysis script and take care of the PCA benchmarks that are run seperately from the aforementioned script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22c7e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run total_model_analysis.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fc2219",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import coranking #coranking.readthedocs.io\n",
    "from coranking.metrics import trustworthiness, continuity, LCMC\n",
    "from transvae.snc import SNC #github.com/hj-n/steadiness-cohesiveness\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "from transvae import trans_models\n",
    "from transvae.transformer_models import TransVAE\n",
    "from transvae.rnn_models import RNN, RNNAttn\n",
    "from transvae.wae_models import WAE\n",
    "from transvae.aae_models import AAE\n",
    "from transvae.tvae_util import *\n",
    "from transvae import analysis\n",
    "import glob\n",
    "import re\n",
    "\n",
    "\n",
    "gpu = True\n",
    "\n",
    "example_data = 'data\\\\peptides\\\\datasets\\\\uniprot_v3\\\\peptide_train.txt'\n",
    "test_train='train'\n",
    "ckpt_list = glob.glob(\"\"+\"checkpointz\\\\to_slurm//**//*.ckpt\", recursive = True) #grab all checkpoints\n",
    "analyses_list = glob.glob(\"model_analyses\\\\train//**/*.csv\", recursive=True) #grab all analyses\n",
    "print('current working directory: ',os.getcwd())\n",
    "\n",
    "for i in range(len(ckpt_list)):\n",
    "    \n",
    "    #search the current directory for the model name and load that model\n",
    "    model_dic = {'trans':'TransVAE','aae':'AAE','rnnattn':'RNNAttn','rnn':'RNN','wae':'WAE'}\n",
    "    model_src = ckpt_list[i]\n",
    "    print('working on: ',model_src,'\\n')\n",
    "    model_name = list(filter(None,[key for key in model_dic.keys() if key in model_src.split('//')[-1]]))\n",
    "    model = locals()[model_dic[model_name[0]]](load_fn=model_src) #use locals to call model specific constructor\n",
    "    \n",
    "    #load the analysis file corresponding to the model from the CC outputs\n",
    "    for idx in range(len(analyses_list)):\n",
    "        if analyses_list[idx].split(\"\\\\\")[-2].find(model_src.split(\"\\\\\")[-2].split(\"_\")[0]) != -1 and analyses_list[idx].split(\"\\\\\")[-2].find(model_src.split(\"\\\\\")[-2].split(\"_\")[1]) != -1:\n",
    "            if analyses_list[idx].find(\"rnnattn\")  != -1 and model_src.find(\"rnnattn\") == -1: continue\n",
    "            save_dir = analyses_list[idx]\n",
    "            cur_analysis = pd.read_csv(save_dir)\n",
    "    print(\"analysis: \",save_dir, \"checkpoint: \",model_src)\n",
    "    save_df = cur_analysis #this will hold the number variables and save to CSV\n",
    "    \n",
    "    #load the true labels\n",
    "    data = pd.read_csv(example_data).to_numpy() \n",
    "    data_1D = data[:,0] #gets rid of extra dimension\n",
    "    \n",
    "    #moving into memory and entropy\n",
    "    if model.model_type =='aae':\n",
    "        mus, _, _ = model.calc_mems(data[:65_000], log=False, save=False) \n",
    "    elif model.model_type == 'wae':\n",
    "        mus, _, _ = model.calc_mems(data[:65_000], log=False, save=False) \n",
    "    else:\n",
    "        mems, mus, logvars = model.calc_mems(data[:65_000], log=False, save=False) #subset size 1200*35=42000 would be ok\n",
    "\n",
    "    #create random index and re-index ordered memory list creating n random sub-lists (ideally resulting in IID random lists)\n",
    "    random_idx = np.random.permutation(np.arange(stop=mus.shape[0]))\n",
    "    mus[:] = mus[random_idx]\n",
    "    data = data[random_idx]\n",
    "    mus = mus[:30_000]#limit the quantity of data to speed up\n",
    "    data = data[:30_000]\n",
    "    \n",
    "    #need to perform PCA to be able to compare dimensionality reduction quality\n",
    "    pca = PCA(n_components=2)\n",
    "    pca_batch =pca.fit_transform(X=mus) \n",
    "    \n",
    "    #now ready to calculation dimensionality reduction accuracy with metrics\n",
    "    trust_subsamples = []\n",
    "    cont_subsamples = []\n",
    "    lcmc_subsamples = []\n",
    "    steadiness_subsamples = []\n",
    "    cohesiveness_subsamples = []\n",
    "    if 'test' in test_train: #different number of bootsraps for train vs test\n",
    "        n=15\n",
    "    else:\n",
    "        n=15\n",
    "    parameter = { \"k\": 50,\"alpha\": 0.1 } #for steadiness and cohesiveness\n",
    "    for s in range(n):\n",
    "        s_len = len(mus)//n\n",
    "        Q = coranking.coranking_matrix(mus[s_len*s:s_len*(s+1)], pca_batch[s_len*s:s_len*(s+1)])\n",
    "        trust_subsamples.append( np.mean(trustworthiness(Q, min_k=1, max_k=50)) )\n",
    "        cont_subsamples.append( np.mean(continuity(Q, min_k=1, max_k=50)) )\n",
    "        lcmc_subsamples.append( np.mean(LCMC(Q, min_k=1, max_k=50)) )\n",
    "        print(s,trust_subsamples[s],cont_subsamples[s],lcmc_subsamples[s])\n",
    "\n",
    "        metrics = SNC(raw=mus[s_len*s:s_len*(s+1)], emb=pca_batch[s_len*s:s_len*(s+1)], iteration=300, dist_parameter=parameter)\n",
    "        metrics.fit() #solve for steadiness and cohesiveness\n",
    "        steadiness_subsamples.append(metrics.steadiness())\n",
    "        cohesiveness_subsamples.append(metrics.cohesiveness())\n",
    "        print(metrics.steadiness(),metrics.cohesiveness())\n",
    "        Q=0 #trying to free RAM\n",
    "        metrics=0\n",
    "        torch.cuda.empty_cache() #free allocated CUDA memory\n",
    "\n",
    "    save_df = pd.concat([save_df,pd.DataFrame({'latent_to_PCA_trustworthiness':trust_subsamples})], axis=1)\n",
    "    save_df = pd.concat([save_df,pd.DataFrame({'latent_to_PCA_continuity':cont_subsamples})], axis=1)\n",
    "    save_df = pd.concat([save_df,pd.DataFrame({'latent_to_PCA_lcmc':lcmc_subsamples})], axis=1)\n",
    "    save_df = pd.concat([save_df,pd.DataFrame({'latent_to_PCA_steadiness':steadiness_subsamples})], axis=1)\n",
    "    save_df = pd.concat([save_df,pd.DataFrame({'latent_to_PCA_cohesiveness':cohesiveness_subsamples})], axis=1)  \n",
    "    \n",
    "    save_df.to_csv(save_dir, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f455bf",
   "metadata": {},
   "source": [
    "<H3> This cell concatenates missing saved_info information (usually not necessary when ran in order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e63ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "analyses_list = glob.glob(\"model_analyses\\\\test//**/*o.csv\", recursive=True) #grab all analyses\n",
    "old_analyses_list = glob.glob(\"model_analyses\\\\old\\\\test//**/*o.csv\", recursive=True)\n",
    "\n",
    "for csv,old_csv in zip(analyses_list,old_analyses_list):\n",
    "    print(csv)\n",
    "    analysis = pd.read_csv(csv)\n",
    "    old_analysis = pd.read_csv(old_csv)\n",
    "    old_analysis = old_analysis.drop(columns=old_analysis.loc[:,'mu_entropies':'latent_to_PCA_cohesiveness'].columns)\n",
    "    new_analysis = pd.concat([old_analysis,analysis], axis=1)\n",
    "    new_analysis.to_csv(csv,index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b565230",
   "metadata": {},
   "source": [
    "<H3> This cell runs the python peptides package and finds physicochemical properties of peptide sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0c63bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import peptides\n",
    "dict_list=[]\n",
    "for seq in data:\n",
    "    pep = peptides.Peptide(seq[0])\n",
    "    dict_list.append(\n",
    "        {\"aliphatic_index\":pep.aliphatic_index(),\n",
    "     \"boman\":pep.boman(),\n",
    "     \"charge_ph3\":pep.charge(pH=3)/len(seq[0]),\n",
    "     \"charge_ph7\":pep.charge(pH=7)/len(seq[0]),\n",
    "     \"charge_ph11\":pep.charge(pH=11)/len(seq[0]),\n",
    "    \"hydrophobic_moment\":pep.hydrophobic_moment()/len(seq[0]),\n",
    "    \"hydrophobicity\":pep.hydrophobicity(),\n",
    "    \"instability_index\":pep.instability_index(),\n",
    "    \"isoelectric_point\":pep.isoelectric_point(),\n",
    "    \"molecular_weight\":pep.molecular_weight()} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf549049",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dict_list)\n",
    "df.to_csv('data/train_physicochem_props.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f50cd9",
   "metadata": {},
   "source": [
    "<H3> Special Extra section to perform particular analysis on select Peptides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a255adb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gpu = True\n",
    "\n",
    "num_sequences = 500_000\n",
    "batch_size = 200 #setting for reconstruction\n",
    "example_data = 'data\\\\peptides\\\\datasets\\\\uniprot_v3\\\\peptide_test.txt'\n",
    "save_dir_loc = 'model_analyses\\\\sample\\\\' #folder in which to save outpts\n",
    "save_dir_name = 'test' #appended to identify data: train|test|other|etc...\n",
    "\n",
    "reconstruct=True #True:reconstruct data here; False:load reconstructions from file\n",
    "recon_src = \"checkpointz//analyses_ckpts//\" #directory in which all reconstructions are stored\n",
    "true_prop_src = 'data\\\\peptides\\\\datasets\\\\uniprot_v3\\\\function_test.txt' #if property predictor load the true labels\n",
    "subset_src = \"\" #(optional) this file should have the true sequences for a subset of the \"example data\" above\n",
    "\n",
    "ckpt_list = glob.glob(\"\"+\"checkpointz\\\\to_slurm//**//*.ckpt\", recursive = True) #grab all checkpoint\n",
    "print('current working directory: ',os.getcwd())\n",
    "\n",
    "\n",
    "for i in range(len(ckpt_list)):\n",
    "    #search the current directory for the model name and load that model\n",
    "    model_dic = {'trans':'TransVAE','aae':'AAE','rnnattn':'RNNAttn','rnn':'RNN','wae':'WAE'}\n",
    "    model_src = ckpt_list[i]\n",
    "    print('working on: ',model_src,'\\n')\n",
    "    model_name = list(filter(None,[key for key in model_dic.keys() if key in model_src.split('\\\\')[-1]]))\n",
    "    model = locals()[model_dic[model_name[0]]](load_fn=model_src) #use locals to call model specific constructor\n",
    "    \n",
    "    #create save directory for the current model according to latent space size\n",
    "    latent_size = re.findall('(latent[\\d]{2,3})', model_src)\n",
    "    save_dir= save_dir_loc+model.name+\"_\"+latent_size[0]+\"_\"+save_dir_name\n",
    "    if not os.path.exists(save_dir):os.mkdir(save_dir) \n",
    "    save_dir= save_dir+\"//\" \n",
    "    \n",
    "     #load the true labels\n",
    "    data = pd.read_csv(example_data).to_numpy() \n",
    "    data_1D = data[:num_sequences,0] #gets rid of extra dimension\n",
    "    true_props_data = pd.read_csv(true_prop_src).to_numpy()\n",
    "    true_props = true_props_data[0:num_sequences,0]\n",
    "    \n",
    "    ##moving into memory and entropy\n",
    "    if model.model_type =='aae':\n",
    "        mus, _, _ = model.calc_mems(data[:65_000], log=False, save=False) #50_000\n",
    "    elif model.model_type == 'wae':\n",
    "        mus, _, _ = model.calc_mems(data[:65_000], log=False, save=False) \n",
    "    else:\n",
    "        mems, mus, logvars = model.calc_mems(data[:65_000], log=False, save=False) #subset size 1200*35=42000 would be ok\n",
    "    #create random index and re-index ordered memory list\n",
    "    random_idx = np.random.permutation(np.arange(stop=mus.shape[0]))\n",
    "    mus = mus[random_idx]\n",
    "    shuf_data = data[random_idx]\n",
    "\n",
    "    subsample_start=0\n",
    "    subsample_length=mus.shape[0] #mus shape depends on batch size!\n",
    "\n",
    "    #(for length based coloring): record all peptide lengths iterating through input\n",
    "    pep_lengths = []\n",
    "    for idx, pep in enumerate(shuf_data[subsample_start:(subsample_start+subsample_length)]):\n",
    "        pep_lengths.append( len(pep[0]) )   \n",
    "    #(for function based coloring): pull function from csv with peptide functions\n",
    "    s_to_f =pd.read_csv(true_prop_src)    \n",
    "    function = s_to_f['peptides'][subsample_start:(subsample_start+subsample_length)]\n",
    "    function = function[random_idx] #account for random permutation\n",
    "\n",
    "    pca = PCA(n_components=5,svd_solver='full')\n",
    "    pca_batch =pca.fit_transform(X=mus[:])\n",
    "    \n",
    "    #list of AMPs of interest\n",
    "    probing_amps=np.array(('GKIIKLKASLKLL','GAIIKLKASLKLL','GKIIKLAASLKLL','GKIIKLKASLALL',\n",
    "                           'GKIIKLKAALALL','GKIIALKASLKLL','IGIKLLKSKLKAL'))\n",
    "    probing_amps=np.reshape(probing_amps,(len(probing_amps),1))\n",
    "    \n",
    "    model.params['BATCH_SIZE'] = 7\n",
    "    if model.model_type =='aae' or model.model_type =='wae':\n",
    "        probed_mus,_,_=model.calc_mems(probing_amps,log=False,save=False)\n",
    "    else:\n",
    "        _,probed_mus,_=model.calc_mems(probing_amps,log=False,save=False)\n",
    "    reduced_amp_probes=pca.transform(X=probed_mus[:])\n",
    "    \n",
    "    #plotting\n",
    "    titles={'text':'{}'.format(model.model_type.replace(\"_\",\" \").upper()),\n",
    "            'x':0.5,'xanchor':'center','yanchor':'top','font_size':40}\n",
    "    general_fonts={'family':\"Helvetica\",'size':30,'color':\"Black\"}\n",
    "    colorbar_fmt={'title_font_size':30,'thickness':15,'ticks':'','title_text':'Lengths',\n",
    "                  'ticklabelposition':\"outside bottom\"}\n",
    "    \n",
    "    #need to add the probed amps to the data\n",
    "    converted_function = list(map(lambda itm: \"AMP\" if itm==1 else \"NON-AMP\",function))\n",
    "    function_w_probe_amps = np.append(converted_function, probing_amps.flatten(),axis=0)\n",
    "    pca_w_probe_amps = np.append(pca_batch,reduced_amp_probes, axis=0)\n",
    "    \n",
    "    fig = px.scatter_matrix(pd.DataFrame({\"PC1\":pca_w_probe_amps[:,0],\"PC2\":pca_w_probe_amps[:,1],\n",
    "                                            \"PC3\":pca_w_probe_amps[:,2],\"PC4\":pca_w_probe_amps[:,3],\n",
    "                                            \"PC5\":pca_w_probe_amps[:,4],\"Function\":function_w_probe_amps}),\n",
    "                                            dimensions=[\"PC1\",\"PC2\",\"PC3\",\"PC4\",\"PC5\"],template='simple_white',\n",
    "                                            color='Function',\n",
    "                                            symbol_sequence=['x-thin','circle',\n",
    "                                                             'square-dot','square-dot','square-dot',\n",
    "                                                             'square-dot','square-dot','square-dot','square-dot'],\n",
    "                                            symbol='Function', opacity=0.8)\n",
    "    fig.update_traces(diagonal_visible=False)\n",
    "    fig.update_layout(title=titles,font=general_fonts)\n",
    "    fig.write_image(save_dir+'pca_matrix_function.png', width=5_000, height=2500)\n",
    "    \n",
    "    #now we sample near the existing amps\n",
    "    sample_count=10\n",
    "    amp_sample_list=[]\n",
    "    for idx,amp in enumerate(probed_mus):\n",
    "        print(\"working on amp sample number: \",idx)\n",
    "        current_mu=np.expand_dims(amp.astype(np.float32),0)\n",
    "        nearby_samples = np.random.normal(loc=0,scale=1,size=(sample_count,1,model.params['d_latent'])).astype(np.float32)*0.3 + current_mu\n",
    "        model.params['BATCH_SIZE'] = 25\n",
    "        rnd_token_list=np.empty((sample_count,model.tgt_len)) #store N decoded latent vectors now in token(0-20) form max length 125\n",
    "        for batch in range(0,sample_count,model.params['BATCH_SIZE']):\n",
    "            rnd_token_list[batch:batch+model.params['BATCH_SIZE']] =  model.greedy_decode(torch.tensor(nearby_samples[batch:batch+model.params['BATCH_SIZE']]).squeeze().cuda()).cpu()\n",
    "            decoded_rnd_seqs = decode_mols(torch.tensor(rnd_token_list), model.params['ORG_DICT'])\n",
    "        amp_sample_list.append(decoded_rnd_seqs)\n",
    "    with open(save_dir+'amp_sample_list.txt','w') as f:\n",
    "        for amp in amp_sample_list:\n",
    "            f.write(str(amp)+'\\n')\n",
    "    f.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2cbb468",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('amp21')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5 (default, Sep  3 2020, 21:29:08) [MSC v.1916 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "686011dad3443993f8e7a9bdd4f15acb7c0d9d5c12f48e0b358ae8d8d9013b02"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
