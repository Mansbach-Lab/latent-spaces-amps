{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b252d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "from transvae import trans_models\n",
    "from transvae.transformer_models import TransVAE\n",
    "from transvae.rnn_models import RNN, RNNAttn\n",
    "from transvae.wae_models import WAE\n",
    "from transvae.aae_models import AAE\n",
    "from transvae.tvae_util import *\n",
    "from transvae import analysis\n",
    "import glob\n",
    "import re\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import Isomap\n",
    "from sklearn import metrics\n",
    "from sklearn.manifold import trustworthiness\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import spearmanr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import NullFormatter\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import coranking #coranking.readthedocs.io\n",
    "from coranking.metrics import trustworthiness, continuity, LCMC\n",
    "from transvae.snc import SNC #github.com/hj-n/steadiness-cohesiveness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95ab15f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current working directory:  C:\\Users\\s_renaud\\Documents\\GitHub\\MSCSAM_TBD\\main_model\n",
      "working on:  checkpointz\\to_slurm\\aae_latent128\\300_aae-128_peptide.ckpt \n",
      "\n",
      "data loaded\n",
      "log_aae-128_peptide.txt checkpointz\\to_slurm\\aae_latent128\\log_aae-128_peptide.txt\n",
      "working on amp sample number:  0\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  100\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  100\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6216/3838145843.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    336\u001b[0m         \u001b[0mrnd_token_list\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_count\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtgt_len\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#store N decoded latent vectors now in token(0-20) form max length 125\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    337\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msample_count\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'BATCH_SIZE'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 338\u001b[1;33m             \u001b[0mrnd_token_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'BATCH_SIZE'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgreedy_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnearby_samples\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'BATCH_SIZE'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    339\u001b[0m         \u001b[0mdecoded_rnd_seqs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecode_mols\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrnd_token_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ORG_DICT'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    340\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\MSCSAM_TBD\\main_model\\transvae\\trans_models.py\u001b[0m in \u001b[0;36mgreedy_decode\u001b[1;34m(self, mem, print_step, src_mask)\u001b[0m\n\u001b[0;32m    527\u001b[0m                 \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecoded\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdecode_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    528\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 529\u001b[1;33m                 \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtgt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    530\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m             \u001b[0mprob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\MSCSAM_TBD\\main_model\\transvae\\aae_models.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, tgt, mem)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtgt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtgt_embed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtgt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict_property\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrue_prop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\amp21\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\MSCSAM_TBD\\main_model\\transvae\\aae_models.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, tgt, mem)\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[0mmem\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m         \u001b[0mmem\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 225\u001b[1;33m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgru\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    226\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\amp21\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\amp21\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    847\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    848\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 849\u001b[1;33m             result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0m\u001b[0;32m    850\u001b[0m                              self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0;32m    851\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def loss_plots(loss_src):\n",
    "    tot_loss = analysis.plot_loss_by_type(src,loss_types=['tot_loss'])\n",
    "    plt.savefig(save_dir+'tot_loss.png')\n",
    "    recon_loss = analysis.plot_loss_by_type(src,loss_types=['recon_loss'])\n",
    "    plt.savefig(save_dir+'recon_loss.png')\n",
    "    kld_loss = analysis.plot_loss_by_type(src,loss_types=['kld_loss'])\n",
    "    plt.savefig(save_dir+'kld_loss.png')\n",
    "    prob_bce_loss = analysis.plot_loss_by_type(src,loss_types=['prop_bce_loss'])\n",
    "    plt.savefig(save_dir+'prob_bce_loss.png')\n",
    "    if 'aae' in src:\n",
    "        disc_loss = analysis.plot_loss_by_type(src,loss_types=['disc_loss'])\n",
    "        plt.savefig(save_dir+'disc_loss.png')\n",
    "    if 'wae' in src:\n",
    "        mmd_loss = analysis.plot_loss_by_type(src,loss_types=['mmd_loss'])\n",
    "        plt.savefig(save_dir+'mmd_loss.png')\n",
    "    plt.close('all')\n",
    "    \n",
    "def load_reconstructions(data,data_1D,latent_size, load_src, true_props=None,subset=None):\n",
    "    \n",
    "    recon_src = load_src+model.name+\"_\"+re.split('(\\d{2,3})',latent_size[0])[0]+\"_\"+re.split('(\\d{2,3})',latent_size[0])[1]+\"//saved_info.csv\"\n",
    "    recon_df = pd.read_csv(recon_src)\n",
    "    reconstructed_seq = recon_df['reconstructions'].to_list()[:num_sequences]\n",
    "    props = torch.Tensor(recon_df['predicted properties'][:num_sequences])\n",
    "    true_props_data = pd.read_csv(true_props).to_numpy()\n",
    "    true_props = true_props_data[0:num_sequences,0]\n",
    "    \n",
    "    if subset:\n",
    "        testing = pd.read_csv(subset).to_numpy()\n",
    "        test_idx_list = [np.where(data==testing[idx][0]) for idx in range(len(testing))]\n",
    "\n",
    "\n",
    "        batch_recon_len = len(reconstructed_seq)\n",
    "        reconstructed_seq = [reconstructed_seq[test_idx_list[i][0][0]] for i in range(len(test_idx_list)) if test_idx_list[i][0][0]<batch_recon_len]\n",
    "        data_1D= [data_1D[test_idx_list[i][0][0]] for i in range(len(test_idx_list)) if test_idx_list[i][0][0]<batch_recon_len]\n",
    "        props = [props[test_idx_list[i][0][0]] for i in range(len(test_idx_list)) if test_idx_list[i][0][0]<batch_recon_len]\n",
    "        props=torch.Tensor(props)\n",
    "        data = testing[:][0]\n",
    "        true_props_data = pd.read_csv(true_props).to_numpy()\n",
    "        true_props = true_props_data[0:num_sequences,0]\n",
    "        true_props= [true_props[test_idx_list[i][0][0]] for i in range(len(test_idx_list)) if test_idx_list[i][0][0]<batch_recon_len]\n",
    "\n",
    "    return data, data_1D, true_props, props, reconstructed_seq\n",
    "\n",
    "########################################################################################\n",
    "gpu = True\n",
    "\n",
    "num_sequences = 500_000\n",
    "batch_size = 200 #setting for reconstruction\n",
    "example_data = 'data\\\\peptides\\\\datasets\\\\uniprot_v3\\\\peptide_test.txt'\n",
    "save_dir_loc = 'model_analyses\\\\sample\\\\' #folder in which to save outpts\n",
    "save_dir_name = 'test' #appended to identify data: train|test|other|etc...\n",
    "\n",
    "reconstruct=True #True:reconstruct data here; False:load reconstructions from file\n",
    "recon_src = \"checkpointz//analyses_ckpts//\" #directory in which all reconstructions are stored\n",
    "true_prop_src = 'data\\\\peptides\\\\datasets\\\\uniprot_v3\\\\function_test.txt' #if property predictor load the true labels\n",
    "subset_src = \"\" #(optional) this file should have the true sequences for a subset of the \"example data\" above\n",
    "\n",
    "ckpt_list = glob.glob(\"\"+\"checkpointz\\\\to_slurm//**//*.ckpt\", recursive = True) #grab all checkpoint\n",
    "print('current working directory: ',os.getcwd())\n",
    "\n",
    "\n",
    "for i in range(len(ckpt_list)):\n",
    "    #search the current directory for the model name and load that model\n",
    "    model_dic = {'trans':'TransVAE','aae':'AAE','rnnattn':'RNNAttn','rnn':'RNN','wae':'WAE'}\n",
    "    model_src = ckpt_list[i]\n",
    "    print('working on: ',model_src,'\\n')\n",
    "    model_name = list(filter(None,[key for key in model_dic.keys() if key in model_src.split('\\\\')[-1]]))\n",
    "    model = locals()[model_dic[model_name[0]]](load_fn=model_src) #use locals to call model specific constructor\n",
    "    \n",
    "    #create save directory for the current model according to latent space size\n",
    "    latent_size = re.findall('(latent[\\d]{2,3})', model_src)\n",
    "    save_dir= save_dir_loc+model.name+\"_\"+latent_size[0]+\"_\"+save_dir_name\n",
    "    if not os.path.exists(save_dir):os.mkdir(save_dir) \n",
    "    save_dir= save_dir+\"//\" \n",
    "    save_df = pd.DataFrame() #this will hold the number variables and save to CSV\n",
    "    \n",
    "    #load the true labels\n",
    "    data = pd.read_csv(example_data).to_numpy() \n",
    "    data_1D = data[:num_sequences,0] #gets rid of extra dimension\n",
    "    true_props_data = pd.read_csv(true_prop_src).to_numpy()\n",
    "    true_props = true_props_data[0:num_sequences,0]\n",
    "\n",
    "    print(\"data loaded\")\n",
    "    #get the log.txt file from the ckpt and model name then plot loss curves\n",
    "    loss_src = '_'.join( (\"log\",model_src.split('\\\\')[-1].split('_')[1],model_src.split('\\\\')[-1].split('_')[2][:-4]+\"txt\") )\n",
    "    src= '\\\\'.join([str(i) for i in model_src.split('\\\\')[:-1]])+\"\\\\\"+loss_src\n",
    "    print(loss_src, src)\n",
    "    loss_plots(src)\n",
    "    \n",
    "#     #set the batch size and reconstruct the data\n",
    "#     model.params['BATCH_SIZE'] = batch_size\n",
    "#     if reconstruct:\n",
    "#         reconstructed_seq, props = model.reconstruct(data[:num_sequences], log=False, return_mems=False)\n",
    "#     else:\n",
    "#         data, data_1D, true_props, props, reconstructed_seq = load_reconstructions(data, data_1D,latent_size,\n",
    "#                                                                                    load_src=recon_src,\n",
    "#                                                                                    true_props=true_prop_src)\n",
    "#     if gpu:torch.cuda.empty_cache() #free allocated CUDA memory\n",
    "    \n",
    "#     #save the metrics to the dataframe\n",
    "#     save_df['reconstructions'] = reconstructed_seq #placing the saves on a line separate from the ops allows for editing\n",
    "#     save_df['predicted properties'] = [prop.item() for prop in props[:len(reconstructed_seq)]]\n",
    "#     prop_acc, prop_conf, MCC=calc_property_accuracies(props[:len(reconstructed_seq)],true_props[:len(reconstructed_seq)], MCC=True)\n",
    "#     save_df['property prediction accuracy'] = prop_acc\n",
    "#     save_df['property prediction confidence'] = prop_conf\n",
    "#     save_df['MCC'] = MCC\n",
    "    \n",
    "\n",
    "# #   First we tokenize the input and reconstructed smiles\n",
    "#     input_sequences = []\n",
    "#     for seq in data_1D:\n",
    "#         input_sequences.append(peptide_tokenizer(seq))\n",
    "#     output_sequences = []\n",
    "#     for seq in reconstructed_seq:\n",
    "#         output_sequences.append(peptide_tokenizer(seq))\n",
    "    \n",
    "#     seq_accs, tok_accs, pos_accs, seq_conf, tok_conf, pos_conf = calc_reconstruction_accuracies(input_sequences, output_sequences)\n",
    "#     save_df['sequence accuracy'] = seq_accs\n",
    "#     save_df['sequence confidence'] = seq_conf\n",
    "#     save_df['token accuracy'] = tok_accs\n",
    "#     save_df['token confidence'] = tok_conf\n",
    "#     save_df = pd.concat([pd.DataFrame({'position_accs':pos_accs,'position_confidence':pos_conf }), save_df], axis=1)\n",
    "    \n",
    "    ##moving into memory and entropy\n",
    "    if model.model_type =='aae':\n",
    "        mus, _, _ = model.calc_mems(data[:65_000], log=False, save=False) #50_000\n",
    "    elif model.model_type == 'wae':\n",
    "        mus, _, _ = model.calc_mems(data[:65_000], log=False, save=False) \n",
    "    else:\n",
    "        mems, mus, logvars = model.calc_mems(data[:65_000], log=False, save=False) #subset size 1200*35=42000 would be ok\n",
    "\n",
    "#     ##calculate the entropies\n",
    "#     vae_entropy_mus = calc_entropy(mus)\n",
    "#     save_df = pd.concat([save_df,pd.DataFrame({'mu_entropies':vae_entropy_mus})], axis=1)\n",
    "#     if model.model_type != 'wae' and model.model_type!= 'aae': #these don't have a variational type bottleneck\n",
    "#         vae_entropy_mems  = calc_entropy(mems)\n",
    "#         save_df = pd.concat([save_df,pd.DataFrame({'mem_entropies':vae_entropy_mems})], axis=1)\n",
    "#         vae_entropy_logvars = calc_entropy(logvars)\n",
    "#         save_df = pd.concat([save_df,pd.DataFrame({'logvar_entropies':vae_entropy_logvars})], axis=1)\n",
    "\n",
    "    #create random index and re-index ordered memory list\n",
    "    random_idx = np.random.permutation(np.arange(stop=mus.shape[0]))\n",
    "    mus = mus[random_idx]\n",
    "    shuf_data = data[random_idx]\n",
    "\n",
    "    subsample_start=0\n",
    "    subsample_length=mus.shape[0] #mus shape depends on batch size!\n",
    "\n",
    "    #(for length based coloring): record all peptide lengths iterating through input\n",
    "    pep_lengths = []\n",
    "    for idx, pep in enumerate(shuf_data[subsample_start:(subsample_start+subsample_length)]):\n",
    "        pep_lengths.append( len(pep[0]) )   \n",
    "    #(for function based coloring): pull function from csv with peptide functions\n",
    "    s_to_f =pd.read_csv(true_prop_src)    \n",
    "    function = s_to_f['peptides'][subsample_start:(subsample_start+subsample_length)]\n",
    "    function = function[random_idx] #account for random permutation\n",
    "\n",
    "    pca = PCA(n_components=5,svd_solver='full')\n",
    "    pca_batch =pca.fit_transform(X=mus[:])\n",
    "    \n",
    "#     #Calculate and plot the loading matrix from the PCA fit of the data\n",
    "#     loadings = pca.components_.T*np.sqrt(pca.explained_variance_)\n",
    "#     color=['tab:blue','tab:red','tab:green','tab:orange','tab:purple']\n",
    "#     y_labels=['PC1 Correlation','PC2 Correlation','PC3 Correlation','PC4 Correlation','PC5 Correlation']\n",
    "#     titles=['Latent Dimension Correlations to PC1','Latent Dimension Correlations to PC2',\n",
    "#             'Latent Dimension Correlations to PC3','Latent Dimension Correlations to PC4',\n",
    "#             'Latent Dimension Correlations to PC5']\n",
    "#     for pc in range (loadings.shape[1]):\n",
    "#         plt.figure(figsize=(10,6))\n",
    "#         plt.title(titles[pc])\n",
    "#         plt.ylabel(y_labels[pc])\n",
    "#         plt.xlim(-1,loadings.shape[0]+1)\n",
    "#         plt.xlabel('Latent Dimensions')\n",
    "#         plt.bar(np.linspace(0,loadings.shape[0]-1,loadings.shape[0]),loadings[:,pc])\n",
    "#         plt.savefig(save_dir+'latent_correlations_PC{}.png'.format(pc+1), transparent=None, dpi=600)\n",
    "#         plt.close()\n",
    "#     #Save the histogram of the PC correlations\n",
    "#     for i in range(5):\n",
    "#         plt.figure()\n",
    "#         plt.title('Histogram of the PC correlation values')\n",
    "#         plt.ylabel('Count')\n",
    "#         plt.hist(loadings[:,i])\n",
    "#         plt.savefig(save_dir+'pc_correlation_hist_{}.png'.format(i+1), transparent=None, dpi=600)\n",
    "#         plt.close()\n",
    "\n",
    "    \n",
    "#     #plot format dictionnaries\n",
    "#     titles={'text':'{}'.format(model.model_type.replace(\"_\",\" \").upper()),\n",
    "#                           'x':0.5,'xanchor':'center','yanchor':'top','font_size':40}\n",
    "#     general_fonts={'family':\"Helvetica\",'size':30,'color':\"Black\"}\n",
    "#     colorbar_fmt={'title_font_size':30,'thickness':15,'ticks':'','title_text':'Lengths',\n",
    "#                                'ticklabelposition':\"outside bottom\"}\n",
    "    \n",
    "#     fig = px.scatter(pd.DataFrame({\"PC1\":pca_batch[:,0],\"PC2\":pca_batch[:,1], \"lengths\":pep_lengths}),\n",
    "#                 symbol_sequence=['hexagon2'],x='PC1', y='PC2', color=\"lengths\",\n",
    "#                 color_continuous_scale='Jet',template='simple_white', opacity=0.9)\n",
    "#     fig.update_traces(marker=dict(size=9))\n",
    "#     fig.update_layout(title=titles,xaxis_title=\"PC1\", yaxis_title=\"PC2\",font=general_fonts)\n",
    "#     fig.update_coloraxes(colorbar=colorbar_fmt)\n",
    "#     fig.write_image(save_dir+'pca_length.png', width=900, height=600)\n",
    "\n",
    "#     fig = px.scatter(pd.DataFrame({\"PC1\":pca_batch[:,0],\"PC2\":pca_batch[:,1], \n",
    "#                                     \"Function\":list(map(lambda itm: \"AMP\" if itm==1 else \"NON-AMP\",function))}),\n",
    "#                                     x='PC1', y='PC2', color=\"Function\",symbol_sequence=['x-thin-open','circle'],\n",
    "#                                     template='simple_white',symbol='Function', opacity=0.8) \n",
    "#     fig.update_traces(marker=dict(size=9))\n",
    "#     fig.update_layout(title=titles,xaxis_title=\"PC1\",yaxis_title=\"PC2\",font=general_fonts)\n",
    "#     fig.write_image(save_dir+'pca_function.png', width=900, height=600)\n",
    "    \n",
    "#     # Plot the explained variances\n",
    "#     plt.bar(range(pca.n_components_), pca.explained_variance_ratio_*100, color='black')\n",
    "#     plt.xlabel('PCA features')\n",
    "#     plt.ylabel('variance %')\n",
    "#     plt.xticks(range(pca.n_components_))\n",
    "#     plt.savefig(save_dir+'variance_explained.png')\n",
    "\n",
    "#     fig = px.scatter_matrix(pd.DataFrame({\"PC1\":pca_batch[:,0],\"PC2\":pca_batch[:,1],\"PC3\":pca_batch[:,2],\n",
    "#                                           \"PC4\":pca_batch[:,3],\"PC5\":pca_batch[:,4],\"lengths\":pep_lengths}),\n",
    "#                                     dimensions=[\"PC1\",\"PC2\",\"PC3\",\"PC4\",\"PC5\"],\n",
    "#                                     symbol_sequence=['hexagon2'],template='simple_white',\n",
    "#                                     color=\"lengths\",color_continuous_scale='Jet', opacity=0.9)\n",
    "#     fig.update_traces(diagonal_visible=False)\n",
    "#     fig.update_layout(title=titles,font=general_fonts)\n",
    "#     fig.write_image(save_dir+'pca_matrix_length.png', width=5_000, height=2500) \n",
    "    \n",
    "#     fig = px.scatter_matrix(pd.DataFrame({\"PC1\":pca_batch[:,0],\"PC2\":pca_batch[:,1],\"PC3\":pca_batch[:,2],\n",
    "#                                           \"PC4\":pca_batch[:,3],\"PC5\":pca_batch[:,4],\n",
    "#                                    \"Function\":list(map(lambda itm: \"AMP\" if itm==1 else \"NON-AMP\",function))}),\n",
    "#                                     dimensions=[\"PC1\",\"PC2\",\"PC3\",\"PC4\",\"PC5\"],template='simple_white',\n",
    "#                                     color=\"Function\",symbol_sequence=['x-thin','circle'],\n",
    "#                                     symbol='Function', opacity=0.8) \n",
    "#     fig.update_traces(diagonal_visible=False)\n",
    "#     fig.update_layout(title=titles,font=general_fonts)\n",
    "#     fig.write_image(save_dir+'pca_matrix_function.png', width=5_000, height=2500) \n",
    "#     pearson = {} #dict to store the pearson coefficient between PCA vs AMP function or physicochem.props.\n",
    "#     pearson.update({'amp'+'_spearmanr':[(spearmanr(pca_batch[:,pc],function).correlation,\n",
    "#                                          spearmanr(pca_batch[:,pc],function).pvalue) for pc in range(5)]})\n",
    "#     if 'train' in save_dir_name:\n",
    "#         phys_props = pd.read_csv('data\\\\train_physicochem_props.csv')\n",
    "#     else:\n",
    "#         phys_props = pd.read_csv('data\\\\test_physicochem_props.csv')\n",
    "\n",
    "    \n",
    "#     for col in phys_props.columns:\n",
    "#         functions = phys_props[col][:len(mus)].values\n",
    "#         functions = functions[random_idx] #keeping track of data scrambling...\n",
    "#         pearson.update({str(col)+'_pearsonr':[pearsonr(pca_batch[:,pc],functions) for pc in range(5)]})\n",
    "#         fig = px.scatter_matrix(pd.DataFrame({\"PC1\":pca_batch[:,0],\"PC2\":pca_batch[:,1],\"PC3\":pca_batch[:,2],\n",
    "#                                                   \"PC4\":pca_batch[:,3],\"PC5\":pca_batch[:,4],\n",
    "#                                            \"Function\":functions}),\n",
    "#                                             dimensions=[\"PC1\",\"PC2\",\"PC3\",\"PC4\",\"PC5\"],template='simple_white',\n",
    "#                                             color=\"Function\",opacity=0.9) \n",
    "#         colorbar_fmt={'title_font_size':30,'thickness':15,'ticks':'','title_text':str(col),\n",
    "#                                'ticklabelposition':\"outside bottom\"}\n",
    "#         fig.update_traces(diagonal_visible=False)\n",
    "#         fig.update_layout(title=titles,font=general_fonts)\n",
    "#         fig.update_coloraxes(colorbar=colorbar_fmt, \n",
    "#                              cmax=np.mean(functions)+np.std(functions),\n",
    "#                              cmin=np.mean(functions)-np.std(functions),\n",
    "#                              cmid=np.mean(functions))\n",
    "#         fig.write_image(save_dir+col+'_PCA_matrix'+'.png', width=5_000, height=2500) \n",
    "#     df_pearson = pd.DataFrame.from_dict(pearson)\n",
    "#     df_pearson.to_csv(save_dir+'pearsonr.csv', index=False)\n",
    "\n",
    "#     #first calculate silhouette score on all latent space dims\n",
    "#     n=15\n",
    "#     latent_mem_func_subsamples = []\n",
    "#     for s in range(n):\n",
    "#         s_len = len(mus)//n #sample lengths\n",
    "#         mem_func_sil = metrics.silhouette_score(mus[s_len*s:s_len*(s+1)], function[s_len*s:s_len*(s+1)], metric='euclidean')\n",
    "#         latent_mem_func_subsamples.append(mem_func_sil)\n",
    "#     save_df = pd.concat([save_df,pd.DataFrame({'latent_mem_func_silhouette':latent_mem_func_subsamples})], axis=1)\n",
    "#     #then go over pairs of PC's from PCA and find max SS PC's\n",
    "#     pc_pairs = [[0,1],[0,2],[0,3],[0,4],[1,2],[1,3],[1,4],[2,3],[2,4],[3,4]]\n",
    "#     for pc_pair in pc_pairs:\n",
    "#         print(\"working on PC[{},{}]\".format(pc_pair[0],pc_pair[1]))\n",
    "#         pca_func_subsamples = []\n",
    "#         for s in range(n):\n",
    "#             s_len = len(mus)//n #sample lengths\n",
    "#             XY = [i for i in zip(pca_batch[s_len*s:s_len*(s+1),pc_pair[0]], pca_batch[s_len*s:s_len*(s+1),pc_pair[1]])]\n",
    "#             pca_func_sil = metrics.silhouette_score(XY, function[s_len*s:s_len*(s+1)], metric='euclidean')\n",
    "#             pca_func_subsamples.append(pca_func_sil)\n",
    "#         save_df = pd.concat([save_df,pd.DataFrame({'pca_func_silhouette[{},{}]'.format(pc_pair[0],pc_pair[1]):pca_func_subsamples})], axis=1)\n",
    "#     print( np.argmax(save_df.drop(columns=save_df.columns[0]).mean(axis=0)) )\n",
    "#     save_df.to_csv(save_dir+\"saved_info.csv\", index=False)\n",
    "    \n",
    "    \n",
    "    \n",
    "#     #Section dealing with sequence generation metrics and bootstrapping from the latent space\n",
    "#     #first randomly sample points within the latents space\n",
    "#     rnd_seq_count =1_000 \n",
    "#     rnd_latent_list=[] #generate N latent space vectors\n",
    "#     mem_min = np.min(mus)\n",
    "#     mem_max = np.max(mus)\n",
    "#     for seq in range(rnd_seq_count):\n",
    "#         rnd_latent_list.append( np.array([random.uniform(mem_min,mem_max) for i in range(model.params['d_latent'])]).astype(np.float32) )\n",
    "    \n",
    "#     model.params['BATCH_SIZE'] = 25\n",
    "#     rnd_token_list=np.empty((rnd_seq_count,model.tgt_len)) #store N decoded latent vectors now in token(0-20) form max length 125\n",
    "    \n",
    "#     #decode these points into predicted amino acid tokens (integers)\n",
    "#     for batch in range(0,rnd_seq_count,model.params['BATCH_SIZE']):\n",
    "#         rnd_token_list[batch:batch+model.params['BATCH_SIZE']] =  model.greedy_decode(torch.tensor(rnd_latent_list[batch:batch+model.params['BATCH_SIZE']]).cuda()).cpu()\n",
    "    \n",
    "#     #turn the tokens into characters\n",
    "#     decoded_rnd_seqs = decode_mols(torch.tensor(rnd_token_list), model.params['ORG_DICT'])\n",
    "#     decoded_rnd_seqs[:]=[x for x in decoded_rnd_seqs if x] #removes the empty lists\n",
    "    \n",
    "#     df_gen_scores = {} #dictionnary to store results\n",
    "#     #UNIQUENESS\n",
    "#     percent_unique, unique_conf = uniqueness(decoded_rnd_seqs)\n",
    "#     df_gen_scores.update({'percent_unique': percent_unique})\n",
    "#     df_gen_scores.update({'unique_confidence':unique_conf})\n",
    "    \n",
    "#     #NOVELTY\n",
    "#     #sample N test/train set sequences randomly and compare to those created\n",
    "#     percent_novel, novel_conf = novelty(data, np.expand_dims(np.array(decoded_rnd_seqs),1))\n",
    "#     df_gen_scores.update({'percent_novel':percent_novel})\n",
    "#     df_gen_scores.update({'novel_confidence':novel_conf})\n",
    "    \n",
    "#     #AMP SAMPLING\n",
    "#     peptides_to_probe=10\n",
    "#     sample_count=100\n",
    "#     best_pc = np.argmax([np.abs(pearsonr(pca_batch[:,pc],function)[0]) for pc in range(5)]) #find the best PCvsAMP correlation\n",
    "#     pca_min = np.min(pca_batch[:,best_pc])\n",
    "#     pca_max = np.max(pca_batch[:,best_pc])\n",
    "#     pca_scan = np.zeros((peptides_to_probe,5)) #create a reduced vector to be sent backwards to high-D\n",
    "#     pca_scan[:,best_pc]=np.linspace(start=pca_min, stop=pca_max, num=peptides_to_probe) #scan 1 dim evenly with best PC\n",
    "#     amp_sample_latents = pca.inverse_transform(pca_scan) #inverse to high-Dims for decoding\n",
    "#     all_gen_seqs = [] #stored in a text file for AMP prediction later\n",
    "#     for idx,amp in enumerate(amp_sample_latents):\n",
    "#         print(\"working on amp sample number: \",idx)\n",
    "#         mus=np.expand_dims(amp.astype(np.float32),0)\n",
    "#         nearby_samples = np.random.normal(loc=0,scale=1,size=(sample_count,1,model.params['d_latent'])).astype(np.float32)*0.3 + mus\n",
    "#         model.params['BATCH_SIZE'] = 25\n",
    "#         rnd_token_list=np.empty((sample_count,model.tgt_len)) #store N decoded latent vectors now in token(0-20) form max length 125\n",
    "#         for batch in range(0,sample_count,model.params['BATCH_SIZE']):\n",
    "#             rnd_token_list[batch:batch+model.params['BATCH_SIZE']] =  model.greedy_decode(torch.tensor(nearby_samples[batch:batch+model.params['BATCH_SIZE']]).squeeze().cuda()).cpu()\n",
    "#         decoded_rnd_seqs = decode_mols(torch.tensor(rnd_token_list), model.params['ORG_DICT'])\n",
    "        \n",
    "               \n",
    "#         for seq in decoded_rnd_seqs:\n",
    "#             if len(seq)<=50 and len(seq)>0: #save only sequences with length >=50\n",
    "#                 all_gen_seqs.append(seq) #appending to list of all generated sequences\n",
    "#         decoded_rnd_seqs = [seq for seq in decoded_rnd_seqs if len(seq)>0 and len(seq)<=50] #keep constrained length seqs\n",
    "        \n",
    "#         #SEQ SIMILARITY\n",
    "#         similarity_score = sequence_similarity(decoded_rnd_seqs)  \n",
    "#         df_gen_scores.update({'average_sequence_similarity_'+str(idx): np.average(similarity_score)})\n",
    "#         df_gen_scores.update({'std_on_similarity_score_'+str(idx): np.std(similarity_score)})\n",
    "        \n",
    "#         #AMP UNIQUENESS\n",
    "#         amp_percent_unique, amp_unique_conf = uniqueness(decoded_rnd_seqs)\n",
    "#         df_gen_scores.update({'amp_uniqueness_'+str(idx): amp_percent_unique})\n",
    "#         df_gen_scores.update({'amp_uniqueness_std_'+str(idx): amp_unique_conf})\n",
    "        \n",
    "#         #Jaccard Similarity Score\n",
    "#         jac_scores_2 = jaccard_similarity_score(decoded_rnd_seqs,2)\n",
    "#         jac_scores_3 = jaccard_similarity_score(decoded_rnd_seqs,3)\n",
    "#         df_gen_scores.update({'amp_jac_score_2_'+str(idx): np.average(jac_scores_2)})\n",
    "#         df_gen_scores.update({'amp_jac_score_std_2_'+str(idx): np.std(jac_scores_2)})\n",
    "#         df_gen_scores.update({'amp_jac_score_3_'+str(idx): np.average(jac_scores_3)})\n",
    "#         df_gen_scores.update({'amp_jac_score_std_3_'+str(idx): np.std(jac_scores_3)})\n",
    "    \n",
    "#     #Store Output\n",
    "#     with open(save_dir+'all_gen_seqs.txt','w') as f:\n",
    "#         for seq in all_gen_seqs:\n",
    "#             f.write(str(seq)+\"\\n\")\n",
    "#     f.close()\n",
    "#     with open(save_dir+'PC_minmax.txt','w') as f:\n",
    "#         f.write(str(pca_min))\n",
    "#         f.write('\\t')\n",
    "#         f.write(str(pca_max))\n",
    "#     f.close()\n",
    "#     df = pd.DataFrame.from_dict([df_gen_scores])\n",
    "#     pd.DataFrame.from_dict([df_gen_scores]).to_csv(save_dir+\"generation_metrics.csv\", index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "286ae62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transvae.tvae_util import *\n",
    "src = 'model_analyses//test//wae-128_peptide_latent128_test//all_gen_seqs.txt'\n",
    "sequence_list=[]\n",
    "with open(src, 'r') as f:\n",
    "    for line in f:\n",
    "        sequence_list.append(line.strip(\"\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "16a72d80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_list = sequence_list[0:25]\n",
    "\n",
    "#3 amino acids 2-mer\n",
    "two_variant_1 = [\"ABC\",\"ABC\"]\n",
    "two_variant_2 = [\"ABC\",\"AAA\"]\n",
    "two_variant_3 = [\"ABC\",\"BBB\"]\n",
    "two_variant_4 = [\"ABC\",\"CCC\"]\n",
    "two_variant_5 = [\"ABC\",\"AAB\"]\n",
    "two_variant_6 = [\"ABC\",\"AAC\"]\n",
    "two_variant_7 = [\"ABC\",\"ABA\"]\n",
    "two_variant_8 = [\"ABC\",\"ABB\"]\n",
    "two_variant_9 = [\"ABC\",\"ABC\"]\n",
    "two_variant_10 = [\"ABC\",\"ACA\"]\n",
    "two_variant_11 = [\"ABC\",\"ACB\"]\n",
    "two_variant_12 = [\"ABC\",\"ACC\"]\n",
    "two_variant_13 = [\"ABC\",\"BAA\"]\n",
    "two_variant_14 = [\"ABC\",\"BAB\"]\n",
    "two_variant_15 = [\"ABC\",\"BAC\"]\n",
    "two_variant_16 = [\"ABC\",\"BBA\"]\n",
    "two_variant_17 = [\"ABC\",\"BBC\"]\n",
    "two_variant_18 = [\"ABC\",\"BCA\"]\n",
    "two_variant_19 = [\"ABC\",\"BCB\"]\n",
    "two_variant_20 = [\"ABC\",\"BCC\"]\n",
    "two_variant_21 = [\"ABC\",\"CAA\"]\n",
    "two_variant_22 = [\"ABC\",\"CAB\"]\n",
    "two_variant_23 = [\"ABC\",\"CAC\"]\n",
    "two_variant_24 = [\"ABC\",\"CBA\"]\n",
    "two_variant_25 = [\"ABC\",\"CBB\"]\n",
    "two_variant_26 = [\"ABC\",\"CBC\"]\n",
    "two_variant_27 = [\"ABC\",\"CCA\"]\n",
    "two_variant_28 = [\"ABC\",\"CCB\"]\n",
    "\n",
    "np.average(jaccard_similarity_score(two_variant_9,2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8307e206",
   "metadata": {},
   "source": [
    "<H4>Since Compute Canada does not do the dimensionality reduction metrics we need to do them below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29fc2219",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current working directory:  C:\\Users\\s_renaud\\Documents\\GitHub\\MSCSAM_TBD\\main_model\n",
      "working on:  checkpointz\\to_slurm\\aae_latent128\\300_aae-128_peptide.ckpt \n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12584/3841049198.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'working on: '\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel_src\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[0mmodel_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodel_dic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodel_src\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'//'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlocals\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmodel_dic\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mload_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel_src\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#use locals to call model specific constructor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[1;31m#load the analysis file corresponding to the model from the CC outputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\MSCSAM_TBD\\main_model\\transvae\\aae_models.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, params, name, N, d_model, d_latent, dropout, tf, bypass_bottleneck, property_predictor, d_pp, depth_pp, type_pp, load_fn, discriminator_layers)\u001b[0m\n\u001b[0;32m     58\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mload_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\MSCSAM_TBD\\main_model\\transvae\\trans_models.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(self, checkpoint_path, rank)\u001b[0m\n\u001b[0;32m    123\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m         \u001b[1;31m#once we have all the parameters set we build the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 125\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m         \u001b[1;31m# Necessary code to remove additional 'module' string attached to 'dict_items' by DDP model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\MSCSAM_TBD\\main_model\\transvae\\aae_models.py\u001b[0m in \u001b[0;36mbuild_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_gpu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m'gpu'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'HARDWARE'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'CHAR_WEIGHTS'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'CHAR_WEIGHTS'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\amp21\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mcuda\u001b[1;34m(self, device)\u001b[0m\n\u001b[0;32m    678\u001b[0m             \u001b[0mModule\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    679\u001b[0m         \"\"\"\n\u001b[1;32m--> 680\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    681\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    682\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mxpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\amp21\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    568\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    569\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 570\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    571\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    572\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\amp21\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    568\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    569\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 570\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    571\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    572\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\amp21\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    187\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flat_weights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mwn\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwn\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwn\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwn\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mwn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flat_weights_names\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m         \u001b[1;31m# Flattens params (on CUDA)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 189\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten_parameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\amp21\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mflatten_parameters\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    173\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mproj_size\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m                         \u001b[0mnum_weights\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 175\u001b[1;33m                     torch._cudnn_rnn_flatten_weight(\n\u001b[0m\u001b[0;32m    176\u001b[0m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flat_weights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_weights\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_cudnn_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import coranking #coranking.readthedocs.io\n",
    "from coranking.metrics import trustworthiness, continuity, LCMC\n",
    "from transvae.snc import SNC #github.com/hj-n/steadiness-cohesiveness\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "from transvae import trans_models\n",
    "from transvae.transformer_models import TransVAE\n",
    "from transvae.rnn_models import RNN, RNNAttn\n",
    "from transvae.wae_models import WAE\n",
    "from transvae.aae_models import AAE\n",
    "from transvae.tvae_util import *\n",
    "from transvae import analysis\n",
    "import glob\n",
    "import re\n",
    "\n",
    "\n",
    "gpu = True\n",
    "\n",
    "example_data = 'data\\\\peptides\\\\datasets\\\\uniprot_v3\\\\peptide_train.txt'\n",
    "test_train='train'\n",
    "ckpt_list = glob.glob(\"\"+\"checkpointz\\\\to_slurm//**//*.ckpt\", recursive = True) #grab all checkpoints\n",
    "analyses_list = glob.glob(\"model_analyses\\\\train//**/*.csv\", recursive=True) #grab all analyses\n",
    "print('current working directory: ',os.getcwd())\n",
    "\n",
    "for i in range(len(ckpt_list)):\n",
    "    \n",
    "    #search the current directory for the model name and load that model\n",
    "    model_dic = {'trans':'TransVAE','aae':'AAE','rnnattn':'RNNAttn','rnn':'RNN','wae':'WAE'}\n",
    "    model_src = ckpt_list[i]\n",
    "    print('working on: ',model_src,'\\n')\n",
    "    model_name = list(filter(None,[key for key in model_dic.keys() if key in model_src.split('//')[-1]]))\n",
    "    model = locals()[model_dic[model_name[0]]](load_fn=model_src) #use locals to call model specific constructor\n",
    "    \n",
    "    #load the analysis file corresponding to the model from the CC outputs\n",
    "    for idx in range(len(analyses_list)):\n",
    "        if analyses_list[idx].split(\"\\\\\")[-2].find(model_src.split(\"\\\\\")[-2].split(\"_\")[0]) != -1 and analyses_list[idx].split(\"\\\\\")[-2].find(model_src.split(\"\\\\\")[-2].split(\"_\")[1]) != -1:\n",
    "            if analyses_list[idx].find(\"rnnattn\")  != -1 and model_src.find(\"rnnattn\") == -1: continue\n",
    "            save_dir = analyses_list[idx]\n",
    "            cur_analysis = pd.read_csv(save_dir)\n",
    "    print(\"analysis: \",save_dir, \"checkpoint: \",model_src)\n",
    "    save_df = cur_analysis #this will hold the number variables and save to CSV\n",
    "    \n",
    "    #load the true labels\n",
    "    data = pd.read_csv(example_data).to_numpy() \n",
    "    data_1D = data[:,0] #gets rid of extra dimension\n",
    "    \n",
    "    #moving into memory and entropy\n",
    "    if model.model_type =='aae':\n",
    "        mus, _, _ = model.calc_mems(data[:65_000], log=False, save=False) \n",
    "    elif model.model_type == 'wae':\n",
    "        mus, _, _ = model.calc_mems(data[:65_000], log=False, save=False) \n",
    "    else:\n",
    "        mems, mus, logvars = model.calc_mems(data[:65_000], log=False, save=False) #subset size 1200*35=42000 would be ok\n",
    "\n",
    "    #create random index and re-index ordered memory list creating n random sub-lists (ideally resulting in IID random lists)\n",
    "    random_idx = np.random.permutation(np.arange(stop=mus.shape[0]))\n",
    "    mus[:] = mus[random_idx]\n",
    "    data = data[random_idx]\n",
    "    mus = mus[:30_000]#limit the quantity of data to speed up\n",
    "    data = data[:30_000]\n",
    "    \n",
    "    #need to perform PCA to be able to compare dimensionality reduction quality\n",
    "    pca = PCA(n_components=2)\n",
    "    pca_batch =pca.fit_transform(X=mus) \n",
    "    \n",
    "    #now ready to calculation dimensionality reduction accuracy with metrics\n",
    "    trust_subsamples = []\n",
    "    cont_subsamples = []\n",
    "    lcmc_subsamples = []\n",
    "    steadiness_subsamples = []\n",
    "    cohesiveness_subsamples = []\n",
    "    if 'test' in test_train: #different number of bootsraps for train vs test\n",
    "        n=15\n",
    "    else:\n",
    "        n=15\n",
    "    parameter = { \"k\": 50,\"alpha\": 0.1 } #for steadiness and cohesiveness\n",
    "    for s in range(n):\n",
    "        s_len = len(mus)//n\n",
    "        Q = coranking.coranking_matrix(mus[s_len*s:s_len*(s+1)], pca_batch[s_len*s:s_len*(s+1)])\n",
    "        trust_subsamples.append( np.mean(trustworthiness(Q, min_k=1, max_k=50)) )\n",
    "        cont_subsamples.append( np.mean(continuity(Q, min_k=1, max_k=50)) )\n",
    "        lcmc_subsamples.append( np.mean(LCMC(Q, min_k=1, max_k=50)) )\n",
    "        print(s,trust_subsamples[s],cont_subsamples[s],lcmc_subsamples[s])\n",
    "\n",
    "        metrics = SNC(raw=mus[s_len*s:s_len*(s+1)], emb=pca_batch[s_len*s:s_len*(s+1)], iteration=300, dist_parameter=parameter)\n",
    "        metrics.fit() #solve for steadiness and cohesiveness\n",
    "        steadiness_subsamples.append(metrics.steadiness())\n",
    "        cohesiveness_subsamples.append(metrics.cohesiveness())\n",
    "        print(metrics.steadiness(),metrics.cohesiveness())\n",
    "        Q=0 #trying to free RAM\n",
    "        metrics=0\n",
    "        torch.cuda.empty_cache() #free allocated CUDA memory\n",
    "\n",
    "    save_df = pd.concat([save_df,pd.DataFrame({'latent_to_PCA_trustworthiness':trust_subsamples})], axis=1)\n",
    "    save_df = pd.concat([save_df,pd.DataFrame({'latent_to_PCA_continuity':cont_subsamples})], axis=1)\n",
    "    save_df = pd.concat([save_df,pd.DataFrame({'latent_to_PCA_lcmc':lcmc_subsamples})], axis=1)\n",
    "    save_df = pd.concat([save_df,pd.DataFrame({'latent_to_PCA_steadiness':steadiness_subsamples})], axis=1)\n",
    "    save_df = pd.concat([save_df,pd.DataFrame({'latent_to_PCA_cohesiveness':cohesiveness_subsamples})], axis=1)  \n",
    "    \n",
    "    save_df.to_csv(save_dir, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f455bf",
   "metadata": {},
   "source": [
    "<H3> This cell concatenates missing saved_info information (usually not necessary when ran in order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06e63ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_analyses\\test\\aae-128_peptide_latent128_test\\saved_info.csv\n",
      "model_analyses\\test\\aae-128_peptide_latent32_test\\saved_info.csv\n",
      "model_analyses\\test\\aae-128_peptide_latent64_test\\saved_info.csv\n",
      "model_analyses\\test\\rnn-128_peptide_latent128_test\\saved_info.csv\n",
      "model_analyses\\test\\rnn-128_peptide_latent32_test\\saved_info.csv\n",
      "model_analyses\\test\\rnn-128_peptide_latent64_test\\saved_info.csv\n",
      "model_analyses\\test\\rnnattn-128_peptide_latent128_test\\saved_info.csv\n",
      "model_analyses\\test\\rnnattn-128_peptide_latent32_test\\saved_info.csv\n",
      "model_analyses\\test\\rnnattn-128_peptide_latent64_test\\saved_info.csv\n",
      "model_analyses\\test\\trans1x-128_peptide_latent128_test\\saved_info.csv\n",
      "model_analyses\\test\\trans1x-128_peptide_latent32_test\\saved_info.csv\n",
      "model_analyses\\test\\trans1x-128_peptide_latent64_test\\saved_info.csv\n",
      "model_analyses\\test\\wae-128_peptide_latent128_test\\saved_info.csv\n",
      "model_analyses\\test\\wae-128_peptide_latent32_test\\saved_info.csv\n",
      "model_analyses\\test\\wae-128_peptide_latent64_test\\saved_info.csv\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "analyses_list = glob.glob(\"model_analyses\\\\test//**/*o.csv\", recursive=True) #grab all analyses\n",
    "old_analyses_list = glob.glob(\"model_analyses\\\\old\\\\test//**/*o.csv\", recursive=True)\n",
    "\n",
    "for csv,old_csv in zip(analyses_list,old_analyses_list):\n",
    "    print(csv)\n",
    "    analysis = pd.read_csv(csv)\n",
    "    old_analysis = pd.read_csv(old_csv)\n",
    "    old_analysis = old_analysis.drop(columns=old_analysis.loc[:,'mu_entropies':'latent_to_PCA_cohesiveness'].columns)\n",
    "    new_analysis = pd.concat([old_analysis,analysis], axis=1)\n",
    "    new_analysis.to_csv(csv,index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b565230",
   "metadata": {},
   "source": [
    "<H3> This cell runs the python peptides package and finds physicochemical properties of peptide sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb0c63bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import peptides\n",
    "dict_list=[]\n",
    "for seq in data:\n",
    "    pep = peptides.Peptide(seq[0])\n",
    "    dict_list.append(\n",
    "        {\"aliphatic_index\":pep.aliphatic_index(),\n",
    "     \"boman\":pep.boman(),\n",
    "     \"charge_ph3\":pep.charge(pH=3)/len(seq[0]),\n",
    "     \"charge_ph7\":pep.charge(pH=7)/len(seq[0]),\n",
    "     \"charge_ph9\":pep.charge(pH=11)/len(seq[0]),\n",
    "    \"hydrophobic_moment\":pep.hydrophobic_moment()/len(seq[0]),\n",
    "    \"hydrophobicity\":pep.hydrophobicity(),\n",
    "    \"instability_index\":pep.instability_index(),\n",
    "    \"isoelectric_point\":pep.isoelectric_point(),\n",
    "    \"molecular_weight\":pep.molecular_weight()} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf549049",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dict_list)\n",
    "df.to_csv('data/train_physicochem_props.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f50cd9",
   "metadata": {},
   "source": [
    "<H3> Special Extra section to perform particular analysis on select Peptides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4a255adb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current working directory:  C:\\Users\\s_renaud\\Documents\\GitHub\\MSCSAM_TBD\\main_model\n",
      "working on:  checkpointz\\to_slurm\\aae_latent128\\300_aae-128_peptide.ckpt \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s_renaud\\Documents\\GitHub\\MSCSAM_TBD\\main_model\\transvae\\trans_models.py:41: UserWarning:\n",
      "\n",
      "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on amp sample number:  0\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  100\n",
      "working on amp sample number:  1\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  100\n",
      "working on amp sample number:  2\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  100\n",
      "working on amp sample number:  3\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  100\n",
      "working on amp sample number:  4\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  100\n",
      "working on amp sample number:  5\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  100\n",
      "working on amp sample number:  6\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  100\n",
      "working on:  checkpointz\\to_slurm\\aae_latent32\\300_aae-128_peptide.ckpt \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s_renaud\\Documents\\GitHub\\MSCSAM_TBD\\main_model\\transvae\\trans_models.py:41: UserWarning:\n",
      "\n",
      "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on amp sample number:  0\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  100\n",
      "working on amp sample number:  1\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  100\n",
      "working on amp sample number:  2\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  100\n",
      "working on amp sample number:  3\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  100\n",
      "working on amp sample number:  4\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  100\n",
      "working on amp sample number:  5\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  100\n",
      "working on amp sample number:  6\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  100\n",
      "working on:  checkpointz\\to_slurm\\aae_latent64\\300_aae-128_peptide.ckpt \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s_renaud\\Documents\\GitHub\\MSCSAM_TBD\\main_model\\transvae\\trans_models.py:41: UserWarning:\n",
      "\n",
      "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on amp sample number:  0\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  100\n",
      "working on amp sample number:  1\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  100\n",
      "working on amp sample number:  2\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  100\n",
      "working on amp sample number:  3\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  100\n",
      "working on amp sample number:  4\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  100\n",
      "working on amp sample number:  5\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  100\n",
      "working on amp sample number:  6\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  100\n",
      "working on:  checkpointz\\to_slurm\\rnnattn_latent128\\300_rnnattn-128_peptide.ckpt \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s_renaud\\Documents\\GitHub\\MSCSAM_TBD\\main_model\\transvae\\trans_models.py:41: UserWarning:\n",
      "\n",
      "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on amp sample number:  0\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  100\n",
      "working on amp sample number:  1\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  100\n",
      "working on amp sample number:  2\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  100\n",
      "working on amp sample number:  3\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  100\n",
      "working on amp sample number:  4\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  100\n",
      "working on amp sample number:  5\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  100\n",
      "working on amp sample number:  6\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  100\n",
      "working on:  checkpointz\\to_slurm\\rnnattn_latent32\\300_rnnattn-128_peptide.ckpt \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s_renaud\\Documents\\GitHub\\MSCSAM_TBD\\main_model\\transvae\\trans_models.py:41: UserWarning:\n",
      "\n",
      "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on amp sample number:  0\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  100\n",
      "working on amp sample number:  1\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  100\n",
      "working on amp sample number:  2\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  100\n",
      "working on amp sample number:  3\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  100\n",
      "working on amp sample number:  4\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  100\n",
      "working on amp sample number:  5\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  100\n",
      "working on amp sample number:  6\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  100\n",
      "working on:  checkpointz\\to_slurm\\rnnattn_latent64\\300_rnnattn-128_peptide.ckpt \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s_renaud\\Documents\\GitHub\\MSCSAM_TBD\\main_model\\transvae\\trans_models.py:41: UserWarning:\n",
      "\n",
      "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on amp sample number:  0\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  100\n",
      "working on amp sample number:  1\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  100\n",
      "working on amp sample number:  2\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  100\n",
      "working on amp sample number:  3\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  100\n",
      "working on amp sample number:  4\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  100\n",
      "working on amp sample number:  5\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  100\n",
      "working on amp sample number:  6\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  100\n",
      "working on:  checkpointz\\to_slurm\\rnn_latent128\\300_rnn-128_peptide.ckpt \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s_renaud\\Documents\\GitHub\\MSCSAM_TBD\\main_model\\transvae\\trans_models.py:41: UserWarning:\n",
      "\n",
      "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on amp sample number:  0\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  100\n",
      "working on amp sample number:  1\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  100\n",
      "working on amp sample number:  2\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  100\n",
      "working on amp sample number:  3\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  100\n",
      "working on amp sample number:  4\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  100\n",
      "working on amp sample number:  5\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  100\n",
      "working on amp sample number:  6\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  100\n",
      "working on:  checkpointz\\to_slurm\\rnn_latent32\\300_rnn-128_peptide.ckpt \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s_renaud\\Documents\\GitHub\\MSCSAM_TBD\\main_model\\transvae\\trans_models.py:41: UserWarning:\n",
      "\n",
      "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on amp sample number:  0\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  100\n",
      "working on amp sample number:  1\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  100\n",
      "working on amp sample number:  2\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  100\n",
      "working on amp sample number:  3\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  100\n",
      "working on amp sample number:  4\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  100\n",
      "working on amp sample number:  5\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  100\n",
      "working on amp sample number:  6\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  100\n",
      "working on:  checkpointz\\to_slurm\\rnn_latent64\\300_rnn-128_peptide.ckpt \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s_renaud\\Documents\\GitHub\\MSCSAM_TBD\\main_model\\transvae\\trans_models.py:41: UserWarning:\n",
      "\n",
      "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on amp sample number:  0\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  100\n",
      "working on amp sample number:  1\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  100\n",
      "working on amp sample number:  2\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  100\n",
      "working on amp sample number:  3\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  100\n",
      "working on amp sample number:  4\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  100\n",
      "working on amp sample number:  5\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  100\n",
      "working on amp sample number:  6\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  100\n",
      "working on:  checkpointz\\to_slurm\\trans_latent128\\300_trans1x-128_peptide.ckpt \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s_renaud\\Documents\\GitHub\\MSCSAM_TBD\\main_model\\transvae\\trans_models.py:41: UserWarning:\n",
      "\n",
      "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on amp sample number:  0\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  100\n",
      "working on amp sample number:  1\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  100\n",
      "working on amp sample number:  2\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  100\n",
      "working on amp sample number:  3\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  100\n",
      "working on amp sample number:  4\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  100\n",
      "working on amp sample number:  5\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  100\n",
      "working on amp sample number:  6\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  100\n",
      "working on:  checkpointz\\to_slurm\\trans_latent32\\300_trans1x-128_peptide.ckpt \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s_renaud\\Documents\\GitHub\\MSCSAM_TBD\\main_model\\transvae\\trans_models.py:41: UserWarning:\n",
      "\n",
      "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on amp sample number:  0\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  100\n",
      "working on amp sample number:  1\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  100\n",
      "working on amp sample number:  2\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  100\n",
      "working on amp sample number:  3\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  100\n",
      "working on amp sample number:  4\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  100\n",
      "working on amp sample number:  5\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  100\n",
      "working on amp sample number:  6\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  100\n",
      "working on:  checkpointz\\to_slurm\\trans_latent64\\300_trans1x-128_peptide.ckpt \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s_renaud\\Documents\\GitHub\\MSCSAM_TBD\\main_model\\transvae\\trans_models.py:41: UserWarning:\n",
      "\n",
      "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on amp sample number:  0\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  100\n",
      "working on amp sample number:  1\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  100\n",
      "working on amp sample number:  2\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  100\n",
      "working on amp sample number:  3\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  100\n",
      "working on amp sample number:  4\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  100\n",
      "working on amp sample number:  5\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  100\n",
      "working on amp sample number:  6\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  100\n",
      "working on:  checkpointz\\to_slurm\\wae_latent128\\300_wae-128_peptide.ckpt \n",
      "\n",
      "WAE class init called /n\n",
      "WAE class build_model called /n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s_renaud\\Documents\\GitHub\\MSCSAM_TBD\\main_model\\transvae\\trans_models.py:41: UserWarning:\n",
      "\n",
      "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on amp sample number:  0\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  100\n",
      "working on amp sample number:  1\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  100\n",
      "working on amp sample number:  2\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  100\n",
      "working on amp sample number:  3\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  100\n",
      "working on amp sample number:  4\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  100\n",
      "working on amp sample number:  5\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  100\n",
      "working on amp sample number:  6\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  100\n",
      "working on:  checkpointz\\to_slurm\\wae_latent32\\300_wae-128_peptide.ckpt \n",
      "\n",
      "WAE class init called /n\n",
      "WAE class build_model called /n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s_renaud\\Documents\\GitHub\\MSCSAM_TBD\\main_model\\transvae\\trans_models.py:41: UserWarning:\n",
      "\n",
      "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on amp sample number:  0\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  100\n",
      "working on amp sample number:  1\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  100\n",
      "working on amp sample number:  2\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  100\n",
      "working on amp sample number:  3\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  100\n",
      "working on amp sample number:  4\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  100\n",
      "working on amp sample number:  5\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  100\n",
      "working on amp sample number:  6\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  100\n",
      "working on:  checkpointz\\to_slurm\\wae_latent64\\300_wae-128_peptide.ckpt \n",
      "\n",
      "WAE class init called /n\n",
      "WAE class build_model called /n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s_renaud\\Documents\\GitHub\\MSCSAM_TBD\\main_model\\transvae\\trans_models.py:41: UserWarning:\n",
      "\n",
      "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on amp sample number:  0\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  100\n",
      "working on amp sample number:  1\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  100\n",
      "working on amp sample number:  2\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  100\n",
      "working on amp sample number:  3\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  100\n",
      "working on amp sample number:  4\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  100\n",
      "working on amp sample number:  5\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  100\n",
      "working on amp sample number:  6\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  100\n"
     ]
    }
   ],
   "source": [
    "gpu = True\n",
    "\n",
    "num_sequences = 500_000\n",
    "batch_size = 200 #setting for reconstruction\n",
    "example_data = 'data\\\\peptides\\\\datasets\\\\uniprot_v3\\\\peptide_test.txt'\n",
    "save_dir_loc = 'model_analyses\\\\sample\\\\' #folder in which to save outpts\n",
    "save_dir_name = 'test' #appended to identify data: train|test|other|etc...\n",
    "\n",
    "reconstruct=True #True:reconstruct data here; False:load reconstructions from file\n",
    "recon_src = \"checkpointz//analyses_ckpts//\" #directory in which all reconstructions are stored\n",
    "true_prop_src = 'data\\\\peptides\\\\datasets\\\\uniprot_v3\\\\function_test.txt' #if property predictor load the true labels\n",
    "subset_src = \"\" #(optional) this file should have the true sequences for a subset of the \"example data\" above\n",
    "\n",
    "ckpt_list = glob.glob(\"\"+\"checkpointz\\\\to_slurm//**//*.ckpt\", recursive = True) #grab all checkpoint\n",
    "print('current working directory: ',os.getcwd())\n",
    "\n",
    "\n",
    "for i in range(len(ckpt_list)):\n",
    "    #search the current directory for the model name and load that model\n",
    "    model_dic = {'trans':'TransVAE','aae':'AAE','rnnattn':'RNNAttn','rnn':'RNN','wae':'WAE'}\n",
    "    model_src = ckpt_list[i]\n",
    "    print('working on: ',model_src,'\\n')\n",
    "    model_name = list(filter(None,[key for key in model_dic.keys() if key in model_src.split('\\\\')[-1]]))\n",
    "    model = locals()[model_dic[model_name[0]]](load_fn=model_src) #use locals to call model specific constructor\n",
    "    \n",
    "    #create save directory for the current model according to latent space size\n",
    "    latent_size = re.findall('(latent[\\d]{2,3})', model_src)\n",
    "    save_dir= save_dir_loc+model.name+\"_\"+latent_size[0]+\"_\"+save_dir_name\n",
    "    if not os.path.exists(save_dir):os.mkdir(save_dir) \n",
    "    save_dir= save_dir+\"//\" \n",
    "    \n",
    "     #load the true labels\n",
    "    data = pd.read_csv(example_data).to_numpy() \n",
    "    data_1D = data[:num_sequences,0] #gets rid of extra dimension\n",
    "    true_props_data = pd.read_csv(true_prop_src).to_numpy()\n",
    "    true_props = true_props_data[0:num_sequences,0]\n",
    "    \n",
    "    ##moving into memory and entropy\n",
    "    if model.model_type =='aae':\n",
    "        mus, _, _ = model.calc_mems(data[:65_000], log=False, save=False) #50_000\n",
    "    elif model.model_type == 'wae':\n",
    "        mus, _, _ = model.calc_mems(data[:65_000], log=False, save=False) \n",
    "    else:\n",
    "        mems, mus, logvars = model.calc_mems(data[:65_000], log=False, save=False) #subset size 1200*35=42000 would be ok\n",
    "    #create random index and re-index ordered memory list\n",
    "    random_idx = np.random.permutation(np.arange(stop=mus.shape[0]))\n",
    "    mus = mus[random_idx]\n",
    "    shuf_data = data[random_idx]\n",
    "\n",
    "    subsample_start=0\n",
    "    subsample_length=mus.shape[0] #mus shape depends on batch size!\n",
    "\n",
    "    #(for length based coloring): record all peptide lengths iterating through input\n",
    "    pep_lengths = []\n",
    "    for idx, pep in enumerate(shuf_data[subsample_start:(subsample_start+subsample_length)]):\n",
    "        pep_lengths.append( len(pep[0]) )   \n",
    "    #(for function based coloring): pull function from csv with peptide functions\n",
    "    s_to_f =pd.read_csv(true_prop_src)    \n",
    "    function = s_to_f['peptides'][subsample_start:(subsample_start+subsample_length)]\n",
    "    function = function[random_idx] #account for random permutation\n",
    "\n",
    "    pca = PCA(n_components=5,svd_solver='full')\n",
    "    pca_batch =pca.fit_transform(X=mus[:])\n",
    "    \n",
    "    #list of AMPs of interest\n",
    "    probing_amps=np.array(('GKIIKLKASLKLL','GAIIKLKASLKLL','GKIIKLAASLKLL','GKIIKLKASLALL',\n",
    "                           'GKIIKLKAALALL','GKIIALKASLKLL','IGIKLLKSKLKAL'))\n",
    "    probing_amps=np.reshape(probing_amps,(len(probing_amps),1))\n",
    "    \n",
    "    model.params['BATCH_SIZE'] = 7\n",
    "    if model.model_type =='aae' or model.model_type =='wae':\n",
    "        probed_mus,_,_=model.calc_mems(probing_amps,log=False,save=False)\n",
    "    else:\n",
    "        _,probed_mus,_=model.calc_mems(probing_amps,log=False,save=False)\n",
    "    reduced_amp_probes=pca.transform(X=probed_mus[:])\n",
    "    \n",
    "    #plotting\n",
    "    titles={'text':'{}'.format(model.model_type.replace(\"_\",\" \").upper()),\n",
    "            'x':0.5,'xanchor':'center','yanchor':'top','font_size':40}\n",
    "    general_fonts={'family':\"Helvetica\",'size':30,'color':\"Black\"}\n",
    "    colorbar_fmt={'title_font_size':30,'thickness':15,'ticks':'','title_text':'Lengths',\n",
    "                  'ticklabelposition':\"outside bottom\"}\n",
    "    \n",
    "    #need to add the probed amps to the data\n",
    "    converted_function = list(map(lambda itm: \"AMP\" if itm==1 else \"NON-AMP\",function))\n",
    "    function_w_probe_amps = np.append(converted_function, probing_amps.flatten(),axis=0)\n",
    "    pca_w_probe_amps = np.append(pca_batch,reduced_amp_probes, axis=0)\n",
    "    \n",
    "    fig = px.scatter_matrix(pd.DataFrame({\"PC1\":pca_w_probe_amps[:,0],\"PC2\":pca_w_probe_amps[:,1],\n",
    "                                            \"PC3\":pca_w_probe_amps[:,2],\"PC4\":pca_w_probe_amps[:,3],\n",
    "                                            \"PC5\":pca_w_probe_amps[:,4],\"Function\":function_w_probe_amps}),\n",
    "                                            dimensions=[\"PC1\",\"PC2\",\"PC3\",\"PC4\",\"PC5\"],template='simple_white',\n",
    "                                            color='Function',\n",
    "                                            symbol_sequence=['x-thin','circle',\n",
    "                                                             'square-dot','square-dot','square-dot',\n",
    "                                                             'square-dot','square-dot','square-dot','square-dot'],\n",
    "                                            symbol='Function', opacity=0.8)\n",
    "    fig.update_traces(diagonal_visible=False)\n",
    "    fig.update_layout(title=titles,font=general_fonts)\n",
    "    fig.write_image(save_dir+'pca_matrix_function.png', width=5_000, height=2500)\n",
    "    \n",
    "    #now we sample near the existing amps\n",
    "    sample_count=10\n",
    "    amp_sample_list=[]\n",
    "    for idx,amp in enumerate(probed_mus):\n",
    "        print(\"working on amp sample number: \",idx)\n",
    "        current_mu=np.expand_dims(amp.astype(np.float32),0)\n",
    "        nearby_samples = np.random.normal(loc=0,scale=1,size=(sample_count,1,model.params['d_latent'])).astype(np.float32)*0.3 + current_mu\n",
    "        model.params['BATCH_SIZE'] = 25\n",
    "        rnd_token_list=np.empty((sample_count,model.tgt_len)) #store N decoded latent vectors now in token(0-20) form max length 125\n",
    "        for batch in range(0,sample_count,model.params['BATCH_SIZE']):\n",
    "            rnd_token_list[batch:batch+model.params['BATCH_SIZE']] =  model.greedy_decode(torch.tensor(nearby_samples[batch:batch+model.params['BATCH_SIZE']]).squeeze().cuda()).cpu()\n",
    "            decoded_rnd_seqs = decode_mols(torch.tensor(rnd_token_list), model.params['ORG_DICT'])\n",
    "        amp_sample_list.append(decoded_rnd_seqs)\n",
    "    with open(save_dir+'amp_sample_list.txt','w') as f:\n",
    "        for amp in amp_sample_list:\n",
    "            f.write(str(amp)+'\\n')\n",
    "    f.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2cbb468",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
