{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1d136af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current working directory:  C:\\Users\\s_renaud\\Documents\\GitHub\\MSCSAM_TBD\\main_model\n",
      "working on:  checkpointz\\analyses_ckpts\\rnn_emb128_latent128_pp\\300_rnn-128_peptide.ckpt \n",
      "\n",
      "rnn-128_peptide\n",
      "cuda\n",
      "torch.Size([127, 100, 128]) torch.Size([3, 100, 128])\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  10\n",
      "decoding sequences of max length  125 current position:  20\n",
      "decoding sequences of max length  125 current position:  30\n",
      "decoding sequences of max length  125 current position:  40\n",
      "decoding sequences of max length  125 current position:  50\n",
      "decoding sequences of max length  125 current position:  60\n",
      "decoding sequences of max length  125 current position:  70\n",
      "decoding sequences of max length  125 current position:  80\n",
      "decoding sequences of max length  125 current position:  90\n",
      "decoding sequences of max length  125 current position:  100\n",
      "decoding sequences of max length  125 current position:  110\n",
      "decoding sequences of max length  125 current position:  120\n",
      "cuda\n",
      "torch.Size([127, 100, 128]) torch.Size([3, 100, 128])\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  10\n",
      "decoding sequences of max length  125 current position:  20\n",
      "decoding sequences of max length  125 current position:  30\n",
      "decoding sequences of max length  125 current position:  40\n",
      "decoding sequences of max length  125 current position:  50\n",
      "decoding sequences of max length  125 current position:  60\n",
      "decoding sequences of max length  125 current position:  70\n",
      "decoding sequences of max length  125 current position:  80\n",
      "decoding sequences of max length  125 current position:  90\n",
      "decoding sequences of max length  125 current position:  100\n",
      "decoding sequences of max length  125 current position:  110\n",
      "decoding sequences of max length  125 current position:  120\n",
      "property accuracy : 155 / 200 = 0.775\n",
      "MCC:  0.012170710888251503\n",
      "rnn-128_peptide\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s_renaud\\Documents\\GitHub\\MSCSAM_TBD\\main_model\\transvae\\tvae_util.py:221: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  position_acc.append(position_accs[0,i] / position_accs[1,i])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([127, 100, 128]) torch.Size([3, 100, 128])\n",
      "torch.Size([127, 100, 128]) torch.Size([3, 100, 128])\n",
      "torch.Size([127, 100, 128]) torch.Size([3, 100, 128])\n",
      "torch.Size([127, 100, 128]) torch.Size([3, 100, 128])\n",
      "torch.Size([127, 100, 128]) torch.Size([3, 100, 128])\n",
      "torch.Size([127, 100, 128]) torch.Size([3, 100, 128])\n",
      "torch.Size([127, 100, 128]) torch.Size([3, 100, 128])\n",
      "torch.Size([127, 100, 128]) torch.Size([3, 100, 128])\n",
      "torch.Size([127, 100, 128]) torch.Size([3, 100, 128])\n",
      "torch.Size([127, 100, 128]) torch.Size([3, 100, 128])\n",
      "torch.Size([127, 100, 128]) torch.Size([3, 100, 128])\n",
      "torch.Size([127, 100, 128]) torch.Size([3, 100, 128])\n",
      "torch.Size([127, 100, 128]) torch.Size([3, 100, 128])\n",
      "torch.Size([127, 100, 128]) torch.Size([3, 100, 128])\n",
      "torch.Size([127, 100, 128]) torch.Size([3, 100, 128])\n",
      "torch.Size([127, 100, 128]) torch.Size([3, 100, 128])\n",
      "torch.Size([127, 100, 128]) torch.Size([3, 100, 128])\n",
      "torch.Size([127, 100, 128]) torch.Size([3, 100, 128])\n",
      "torch.Size([127, 100, 128]) torch.Size([3, 100, 128])\n",
      "torch.Size([127, 100, 128]) torch.Size([3, 100, 128])\n",
      "torch.Size([127, 100, 128]) torch.Size([3, 100, 128])\n",
      "torch.Size([127, 100, 128]) torch.Size([3, 100, 128])\n",
      "torch.Size([127, 100, 128]) torch.Size([3, 100, 128])\n",
      "torch.Size([127, 100, 128]) torch.Size([3, 100, 128])\n",
      "torch.Size([127, 100, 128]) torch.Size([3, 100, 128])\n",
      "torch.Size([127, 100, 128]) torch.Size([3, 100, 128])\n",
      "torch.Size([127, 100, 128]) torch.Size([3, 100, 128])\n",
      "torch.Size([127, 100, 128]) torch.Size([3, 100, 128])\n",
      "torch.Size([127, 100, 128]) torch.Size([3, 100, 128])\n",
      "torch.Size([127, 100, 128]) torch.Size([3, 100, 128])\n",
      "torch.Size([127, 100, 128]) torch.Size([3, 100, 128])\n",
      "torch.Size([127, 100, 128]) torch.Size([3, 100, 128])\n",
      "torch.Size([127, 100, 128]) torch.Size([3, 100, 128])\n",
      "torch.Size([127, 100, 128]) torch.Size([3, 100, 128])\n",
      "torch.Size([127, 100, 128]) torch.Size([3, 100, 128])\n",
      "torch.Size([127, 100, 128]) torch.Size([3, 100, 128])\n",
      "torch.Size([127, 100, 128]) torch.Size([3, 100, 128])\n",
      "torch.Size([127, 100, 128]) torch.Size([3, 100, 128])\n",
      "torch.Size([127, 100, 128]) torch.Size([3, 100, 128])\n",
      "torch.Size([127, 100, 128]) torch.Size([3, 100, 128])\n",
      "torch.Size([127, 100, 128]) torch.Size([3, 100, 128])\n",
      "torch.Size([127, 100, 128]) torch.Size([3, 100, 128])\n",
      "torch.Size([127, 100, 128]) torch.Size([3, 100, 128])\n",
      "torch.Size([127, 100, 128]) torch.Size([3, 100, 128])\n",
      "torch.Size([127, 100, 128]) torch.Size([3, 100, 128])\n",
      "torch.Size([127, 100, 128]) torch.Size([3, 100, 128])\n",
      "torch.Size([127, 100, 128]) torch.Size([3, 100, 128])\n",
      "torch.Size([127, 100, 128]) torch.Size([3, 100, 128])\n",
      "torch.Size([127, 100, 128]) torch.Size([3, 100, 128])\n",
      "torch.Size([127, 100, 128]) torch.Size([3, 100, 128])\n",
      "torch.Size([127, 100, 128]) torch.Size([3, 100, 128])\n",
      "torch.Size([127, 100, 128]) torch.Size([3, 100, 128])\n",
      "torch.Size([127, 100, 128]) torch.Size([3, 100, 128])\n",
      "torch.Size([127, 100, 128]) torch.Size([3, 100, 128])\n",
      "torch.Size([127, 100, 128]) torch.Size([3, 100, 128])\n",
      "torch.Size([127, 100, 128]) torch.Size([3, 100, 128])\n",
      "torch.Size([127, 100, 128]) torch.Size([3, 100, 128])\n",
      "torch.Size([127, 100, 128]) torch.Size([3, 100, 128])\n",
      "torch.Size([127, 100, 128]) torch.Size([3, 100, 128])\n",
      "torch.Size([127, 100, 128]) torch.Size([3, 100, 128])\n",
      "torch.Size([127, 100, 128]) torch.Size([3, 100, 128])\n",
      "torch.Size([127, 100, 128]) torch.Size([3, 100, 128])\n",
      "torch.Size([127, 100, 128]) torch.Size([3, 100, 128])\n",
      "torch.Size([127, 100, 128]) torch.Size([3, 100, 128])\n",
      "torch.Size([127, 100, 128]) torch.Size([3, 100, 128])\n",
      "torch.Size([127, 100, 128]) torch.Size([3, 100, 128])\n",
      "torch.Size([127, 100, 128]) torch.Size([3, 100, 128])\n",
      "torch.Size([127, 100, 128]) torch.Size([3, 100, 128])\n",
      "torch.Size([127, 100, 128]) torch.Size([3, 100, 128])\n",
      "torch.Size([127, 100, 128]) torch.Size([3, 100, 128])\n",
      "torch.Size([127, 100, 128]) torch.Size([3, 100, 128])\n",
      "torch.Size([127, 100, 128]) torch.Size([3, 100, 128])\n",
      "torch.Size([127, 100, 128]) torch.Size([3, 100, 128])\n",
      "torch.Size([127, 100, 128]) torch.Size([3, 100, 128])\n",
      "torch.Size([127, 100, 128]) torch.Size([3, 100, 128])\n",
      "torch.Size([127, 100, 128]) torch.Size([3, 100, 128])\n",
      "torch.Size([127, 100, 128]) torch.Size([3, 100, 128])\n",
      "torch.Size([127, 100, 128]) torch.Size([3, 100, 128])\n",
      "torch.Size([127, 100, 128]) torch.Size([3, 100, 128])\n",
      "torch.Size([127, 100, 128]) torch.Size([3, 100, 128])\n",
      "torch.Size([127, 100, 128]) torch.Size([3, 100, 128])\n",
      "torch.Size([127, 100, 128]) torch.Size([3, 100, 128])\n",
      "torch.Size([127, 100, 128]) torch.Size([3, 100, 128])\n",
      "torch.Size([127, 100, 128]) torch.Size([3, 100, 128])\n",
      "torch.Size([127, 100, 128]) torch.Size([3, 100, 128])\n",
      "torch.Size([127, 100, 128]) torch.Size([3, 100, 128])\n",
      "torch.Size([127, 100, 128]) torch.Size([3, 100, 128])\n",
      "torch.Size([127, 100, 128]) torch.Size([3, 100, 128])\n",
      "torch.Size([127, 100, 128]) torch.Size([3, 100, 128])\n",
      "torch.Size([127, 100, 128]) torch.Size([3, 100, 128])\n",
      "torch.Size([127, 100, 128]) torch.Size([3, 100, 128])\n",
      "torch.Size([127, 100, 128]) torch.Size([3, 100, 128])\n",
      "torch.Size([127, 100, 128]) torch.Size([3, 100, 128])\n",
      "torch.Size([127, 100, 128]) torch.Size([3, 100, 128])\n",
      "torch.Size([127, 100, 128]) torch.Size([3, 100, 128])\n",
      "torch.Size([127, 100, 128]) torch.Size([3, 100, 128])\n",
      "torch.Size([127, 100, 128]) torch.Size([3, 100, 128])\n",
      "torch.Size([127, 100, 128]) torch.Size([3, 100, 128])\n",
      "torch.Size([127, 100, 128]) torch.Size([3, 100, 128])\n",
      "torch.Size([127, 100, 128]) torch.Size([3, 100, 128])\n",
      "done images\n",
      "working on:  checkpointz\\analyses_ckpts\\trans_emb128_latent128_pp\\300_trans1x-128_peptide.ckpt \n",
      "\n",
      "trans1x-128_peptide\n",
      "cuda\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  10\n",
      "decoding sequences of max length  125 current position:  20\n",
      "decoding sequences of max length  125 current position:  30\n",
      "decoding sequences of max length  125 current position:  40\n",
      "decoding sequences of max length  125 current position:  50\n",
      "decoding sequences of max length  125 current position:  60\n",
      "decoding sequences of max length  125 current position:  70\n",
      "decoding sequences of max length  125 current position:  80\n",
      "decoding sequences of max length  125 current position:  90\n",
      "decoding sequences of max length  125 current position:  100\n",
      "decoding sequences of max length  125 current position:  110\n",
      "decoding sequences of max length  125 current position:  120\n",
      "cuda\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  10\n",
      "decoding sequences of max length  125 current position:  20\n",
      "decoding sequences of max length  125 current position:  30\n",
      "decoding sequences of max length  125 current position:  40\n",
      "decoding sequences of max length  125 current position:  50\n",
      "decoding sequences of max length  125 current position:  60\n",
      "decoding sequences of max length  125 current position:  70\n",
      "decoding sequences of max length  125 current position:  80\n",
      "decoding sequences of max length  125 current position:  90\n",
      "decoding sequences of max length  125 current position:  100\n",
      "decoding sequences of max length  125 current position:  110\n",
      "decoding sequences of max length  125 current position:  120\n",
      "property accuracy : 145 / 200 = 0.725\n",
      "MCC:  -0.004855417871743739\n",
      "trans1x-128_peptide\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s_renaud\\Documents\\GitHub\\MSCSAM_TBD\\main_model\\transvae\\tvae_util.py:221: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in double_scalars\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done images\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "from transvae import trans_models\n",
    "from transvae.transformer_models import TransVAE\n",
    "from transvae.rnn_models import RNN, RNNAttn\n",
    "from transvae.wae_models import WAE\n",
    "from transvae.aae_models import AAE\n",
    "from transvae.tvae_util import *\n",
    "from transvae import analysis\n",
    "import glob\n",
    "import re\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import metrics\n",
    "from sklearn.manifold import trustworthiness\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import NullFormatter\n",
    "import plotly.express as px\n",
    "\n",
    "import coranking #coranking.readthedocs.io\n",
    "from coranking.metrics import trustworthiness, continuity, LCMC\n",
    "from transvae.snc import SNC #github.com/hj-n/steadiness-cohesiveness\n",
    "\n",
    "def loss_plots(loss_src):\n",
    "    tot_loss = analysis.plot_loss_by_type(src,loss_types=['tot_loss'])\n",
    "    plt.savefig(save_dir+'tot_loss.png')\n",
    "    recon_loss = analysis.plot_loss_by_type(src,loss_types=['recon_loss'])\n",
    "    plt.savefig(save_dir+'recon_loss.png')\n",
    "    kld_loss = analysis.plot_loss_by_type(src,loss_types=['kld_loss'])\n",
    "    plt.savefig(save_dir+'kld_loss.png')\n",
    "    prob_bce_loss = analysis.plot_loss_by_type(src,loss_types=['prop_bce_loss'])\n",
    "    plt.savefig(save_dir+'prob_bce_loss.png')\n",
    "    if 'aae' in src:\n",
    "        disc_loss = analysis.plot_loss_by_type(src,loss_types=['disc_loss'])\n",
    "        plt.savefig(save_dir+'disc_loss.png')\n",
    "    if 'wae' in src:\n",
    "        mmd_loss = analysis.plot_loss_by_type(src,loss_types=['mmd_loss'])\n",
    "        plt.savefig(save_dir+'mmd_loss.png')\n",
    "    plt.close('all')\n",
    "    \n",
    "def load_reconstructions(data,data_1D,latent_size, load_src, true_props=None,subset=None):\n",
    "    \n",
    "    recon_src = load_src+model.name+\"_\"+re.split('(\\d{2,3})',latent_size[0])[0]+\"_\"+re.split('(\\d{2,3})',latent_size[0])[1]+\"//saved_info.csv\"\n",
    "    recon_df = pd.read_csv(recon_src)\n",
    "    reconstructed_seq = recon_df['reconstructions'].to_list()[:num_sequences]\n",
    "    props = torch.Tensor(recon_df['predicted properties'][:num_sequences])\n",
    "    true_props_data = pd.read_csv(true_props).to_numpy()\n",
    "    true_props = true_props_data[0:num_sequences,0]\n",
    "    \n",
    "    if subset:\n",
    "        testing = pd.read_csv(subset).to_numpy()\n",
    "        test_idx_list = [np.where(data==testing[idx][0]) for idx in range(len(testing))]\n",
    "\n",
    "        batch_recon_len = len(reconstructed_seq)\n",
    "        reconstructed_seq = [reconstructed_seq[test_idx_list[i][0][0]] for i in range(len(test_idx_list)) if test_idx_list[i][0][0]<batch_recon_len]\n",
    "        data_1D= [data_1D[test_idx_list[i][0][0]] for i in range(len(test_idx_list)) if test_idx_list[i][0][0]<batch_recon_len]\n",
    "        props = [props[test_idx_list[i][0][0]] for i in range(len(test_idx_list)) if test_idx_list[i][0][0]<batch_recon_len]\n",
    "        props=torch.Tensor(props)\n",
    "        data = testing[:][0]\n",
    "        true_props_data = pd.read_csv(true_props).to_numpy()\n",
    "        true_props = true_props_data[0:num_sequences,0]\n",
    "        true_props= [true_props[test_idx_list[i][0][0]] for i in range(len(test_idx_list)) if test_idx_list[i][0][0]<batch_recon_len]\n",
    "\n",
    "    return data, data_1D, true_props, props, reconstructed_seq\n",
    "\n",
    "########################################################################################\n",
    "gpu = True\n",
    "\n",
    "num_sequences = 200\n",
    "batch_size = 100 #setting for reconstruction\n",
    "example_data = 'data//example_data//train_test//peptide_train.txt'\n",
    "\n",
    "reconstruct=True #True:reconstruct data here; False:load reconstructions from file\n",
    "recon_src = \"checkpointz//analyses_ckpts//\" #directory in which all reconstructions are store\n",
    "true_prop_src = \"data//example_data//function_train.txt\" #if the model has a property predictor get the true labels here\n",
    "subset_src = \"\" #(optional) this file should have the true sequences for a subset of the \"example data\" above\n",
    "\n",
    "ckpt_list = glob.glob(\"\"+\"checkpointz\\\\analyses_ckpts//**//*.ckpt\", recursive = True) #grab all checkpoint\n",
    "print('current working directory: ',os.getcwd())\n",
    "\n",
    "\n",
    "for i in range(len(ckpt_list)):\n",
    "    \n",
    "    #search the current directory for the model name and load that model\n",
    "    model_dic = {'trans':'TransVAE','aae':'AAE','rnn':'RNN','rnnattn':'RNNAttn','wae':'WAE'}\n",
    "    model_src = ckpt_list[i]\n",
    "    print('working on: ',model_src,'\\n')\n",
    "    model_name = list(filter(None,[key for key in model_dic.keys() if key in model_src.split('\\\\')[-1]]))\n",
    "    model = locals()[model_dic[model_name[0]]](load_fn=model_src) #use locals to call model specific constructor\n",
    "    \n",
    "    #create save directory for the current model according to latent space size\n",
    "    latent_size = re.findall('(latent[\\d]{2,3})', model_src)\n",
    "    save_dir= \"temp_analyses//\"+model.name+latent_size[0]+'train'\n",
    "    if not os.path.exists(save_dir):os.mkdir(save_dir) \n",
    "    save_dir= save_dir+\"//\" \n",
    "    save_df = pd.DataFrame() #this will hold the number variables and save to CSV\n",
    "    \n",
    "    #load the true labels\n",
    "    data = pd.read_csv(example_data).to_numpy() \n",
    "    data_1D = data[:num_sequences,0] #gets rid of extra dimension\n",
    "    true_props_data = pd.read_csv(true_prop_src).to_numpy()\n",
    "    true_props = true_props_data[0:num_sequences,0]\n",
    "    \n",
    "    #get the log.txt file from the ckpt and model name then plot loss curves\n",
    "    loss_src = '_'.join( (\"log\",model_src.split('\\\\')[-1].split('_')[1],model_src.split('\\\\')[-1].split('_')[2][:-4]+\"txt\") )\n",
    "    src= '\\\\'.join([str(i) for i in model_src.split('\\\\')[:-1]])+\"\\\\\"+loss_src\n",
    "    loss_plots(src)\n",
    "    \n",
    "    #set the batch size and reconstruct the data\n",
    "    model.params['BATCH_SIZE'] = batch_size\n",
    "    if reconstruct:\n",
    "        reconstructed_seq, props = model.reconstruct(data[:num_sequences], log=False, return_mems=False)\n",
    "    else:\n",
    "        data, data_1D, true_props, props, reconstructed_seq = load_reconstructions(data, data_1D,latent_size,\n",
    "                                                                                   load_src=recon_src,\n",
    "                                                                                   true_props=true_prop_src)\n",
    "    if gpu:torch.cuda.empty_cache() #free allocated CUDA memory\n",
    "    \n",
    "    #save the metrics to the dataframe\n",
    "    save_df['reconstructions'] = reconstructed_seq #placing the saves on a line separate from the ops allows for editing\n",
    "    save_df['predicted properties'] = [prop.item() for prop in props[:len(reconstructed_seq)]]\n",
    "    prop_acc, prop_conf, MCC=calc_property_accuracies(props[:len(reconstructed_seq)],true_props[:len(reconstructed_seq)], MCC=True)\n",
    "    save_df['property prediction accuracy'] = prop_acc\n",
    "    save_df['property prediction confidence'] = prop_conf\n",
    "    save_df['MCC'] = MCC\n",
    "    \n",
    "#   First we tokenize the input and reconstructed smiles\n",
    "    input_sequences = []\n",
    "    for seq in data_1D:\n",
    "        input_sequences.append(peptide_tokenizer(seq))\n",
    "    output_sequences = []\n",
    "    for seq in reconstructed_seq:\n",
    "        output_sequences.append(peptide_tokenizer(seq))\n",
    "    \n",
    "    seq_accs, tok_accs, pos_accs, seq_conf, tok_conf, pos_conf = calc_reconstruction_accuracies(input_sequences, output_sequences)\n",
    "    save_df['sequence accuracy'] = seq_accs\n",
    "    save_df['sequence confidence'] = seq_conf\n",
    "    save_df['token accuracy'] = tok_accs\n",
    "    save_df['token confidence'] = tok_conf\n",
    "    save_df = pd.concat([pd.DataFrame({'position_accs':pos_accs,'position_confidence':pos_conf }), save_df], axis=1)\n",
    "    \n",
    "    ##moving into memory and entropy\n",
    "    if model.model_type =='aae':\n",
    "        mus, _, _ = model.calc_mems(data[:], log=False, save=False) \n",
    "    elif model.model_type == 'wae':\n",
    "        mus, _, _ = model.calc_mems(data[:], log=False, save=False) \n",
    "    else:\n",
    "        mems, mus, logvars = model.calc_mems(data[:10_000], log=False, save=False) #subset size 1200*35=42000 would be ok\n",
    "    ##calculate the entropies\n",
    "    vae_entropy_mus = calc_entropy(mus)\n",
    "    save_df = pd.concat([save_df,pd.DataFrame({'mu_entropies':vae_entropy_mus})], axis=1)\n",
    "    if model.model_type != 'wae' and model.model_type!= 'aae': #these don't have a variational type bottleneck\n",
    "        vae_entropy_mems  = calc_entropy(mems)\n",
    "        save_df = pd.concat([save_df,pd.DataFrame({'mem_entropies':vae_entropy_mems})], axis=1)\n",
    "        vae_entropy_logvars = calc_entropy(logvars)\n",
    "        save_df = pd.concat([save_df,pd.DataFrame({'logvar_entropies':vae_entropy_logvars})], axis=1)\n",
    "    \n",
    "    #create random index and re-index ordered memory list creating n random sub-lists (ideally resulting in IID random lists)\n",
    "    random_idx = np.random.permutation(np.arange(stop=mus.shape[0]))\n",
    "    mus[:] = mus[random_idx]\n",
    "    data = data[random_idx]\n",
    "    #define the subset of the data to sample for PCA and silhouette/cluster metrics\n",
    "    subsample_start=0\n",
    "    subsample_length=mus.shape[0]\n",
    "\n",
    "    #(for length based coloring): record all peptide lengths iterating through input\n",
    "    pep_lengths = []\n",
    "    for idx, pep in enumerate(data[subsample_start:(subsample_start+subsample_length)]):\n",
    "        pep_lengths.append( len(pep[0]) )   \n",
    "    #(for function based coloring): pull function from csv with peptide functions\n",
    "    s_to_f =pd.read_csv(true_prop_src)    \n",
    "    function = s_to_f['peptides'][subsample_start:(subsample_start+subsample_length)]\n",
    "    function = function[random_idx] #account for random permutation\n",
    "\n",
    "    pca = PCA(n_components=3)\n",
    "    pca_batch =pca.fit_transform(X=mus[:])\n",
    "\n",
    "    fig = px.scatter_matrix(pca_batch,color= pep_lengths,dimensions=[0,1,2] ,opacity=0.7)\n",
    "    fig.update_traces(marker=dict(size=3))\n",
    "    fig.write_image(save_dir+'pca_length.png', width=1920, height=1080)\n",
    "\n",
    "    fig = px.scatter_matrix(pca_batch, color= [str(itm) for itm in function],dimensions=[0,1,2], opacity=0.7)\n",
    "    fig.update_traces(marker=dict(size=3))\n",
    "    fig.write_image(save_dir+'pca_function.png', width=1920, height=1080)\n",
    "    print('done images')\n",
    "    #create n subsamples and calculate silhouette score for each\n",
    "    latent_mem_func_subsamples = []\n",
    "    pca_func_subsamples = []\n",
    "    n=35\n",
    "    for s in range(n):\n",
    "        s_len = len(mus)//n #sample lengths\n",
    "        mem_func_sil = metrics.silhouette_score(mus[s_len*s:s_len*(s+1)], function[s_len*s:s_len*(s+1)], metric='euclidean')\n",
    "        latent_mem_func_subsamples.append(mem_func_sil)\n",
    "        XY = [i for i in zip(pca_batch[s_len*s:s_len*(s+1),0], pca_batch[s_len*s:s_len*(s+1),1])]\n",
    "        pca_func_sil = metrics.silhouette_score(XY, function[s_len*s:s_len*(s+1)], metric='euclidean')\n",
    "        pca_func_subsamples.append(pca_func_sil)\n",
    "    save_df = pd.concat([save_df,pd.DataFrame({'latent_mem_func_silhouette':latent_mem_func_subsamples})], axis=1)\n",
    "    save_df = pd.concat([save_df,pd.DataFrame({'pca_func_silhouette':pca_func_subsamples})], axis=1)\n",
    "\n",
    "    #latent space metrics when changing from high dimension to PCA representation\n",
    "#     trust_subsamples = []\n",
    "#     cont_subsamples = []\n",
    "#     lcmc_subsamples = []\n",
    "#     steadiness_subsamples = []\n",
    "#     cohesiveness_subsamples = []\n",
    "\n",
    "#     n=35\n",
    "#     parameter = { \"k\": 50,\"alpha\": 0.1 } #for steadiness and cohesiveness\n",
    "#     for s in range(n):\n",
    "#         s_len = len(mus)//n #sample lengths\n",
    "#         Q = coranking.coranking_matrix(mus[s_len*s:s_len*(s+1)], pca_batch[s_len*s:s_len*(s+1)])\n",
    "#         trust_subsamples.append( np.mean(trustworthiness(Q, min_k=1, max_k=50)) )\n",
    "#         cont_subsamples.append( np.mean(continuity(Q, min_k=1, max_k=50)) )\n",
    "#         lcmc_subsamples.append( np.mean(LCMC(Q, min_k=1, max_k=50)) )\n",
    "#         print(s,trust_subsamples[s],cont_subsamples[s],lcmc_subsamples[s])\n",
    "\n",
    "#         metrics = SNC(raw=mus[s_len*s:s_len*(s+1)], emb=pca_batch[s_len*s:s_len*(s+1)], iteration=300, dist_parameter=parameter)\n",
    "#         metrics.fit() #solve for steadiness and cohesiveness\n",
    "#         steadiness_subsamples.append(metrics.steadiness())\n",
    "#         cohesiveness_subsamples.append(metrics.cohesiveness())\n",
    "#         print(metrics.steadiness(),metrics.cohesiveness())\n",
    "#         Q=0 #trying to free RAM\n",
    "#         metrics=0\n",
    "#         torch.cuda.empty_cache() #free allocated CUDA memory\n",
    "\n",
    "#     save_df = pd.concat([save_df,pd.DataFrame({'latent_to_PCA_trustworthiness':trust_subsamples})], axis=1)\n",
    "#     save_df = pd.concat([save_df,pd.DataFrame({'latent_to_PCA_continuity':cont_subsamples})], axis=1)\n",
    "#     save_df = pd.concat([save_df,pd.DataFrame({'latent_to_PCA_lcmc':lcmc_subsamples})], axis=1)\n",
    "#     save_df = pd.concat([save_df,pd.DataFrame({'latent_to_PCA_steadiness':steadiness_subsamples})], axis=1)\n",
    "#     save_df = pd.concat([save_df,pd.DataFrame({'latent_to_PCA_cohesiveness':cohesiveness_subsamples})], axis=1)\n",
    "\n",
    "    \n",
    "    save_df.to_csv(save_dir+\"saved_info.csv\", index=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d890a541",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
