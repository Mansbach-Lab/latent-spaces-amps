{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1d136af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current working directory:  C:\\Users\\s_renaud\\Documents\\GitHub\\MSCSAM_TBD\\main_model\n",
      "working on:  checkpointz\\to_slurm\\rnn_latent128\\300_rnn-128_peptide.ckpt \n",
      "\n",
      "rnn-128_peptide\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15184/1006385876.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    264\u001b[0m     \u001b[0mrnd_latent_list\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m#generate N latent space vectors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrnd_seq_count\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 266\u001b[1;33m         \u001b[0mrnd_latent_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'd_latent'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    267\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'BATCH_SIZE'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m25\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15184/1006385876.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    264\u001b[0m     \u001b[0mrnd_latent_list\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m#generate N latent space vectors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrnd_seq_count\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 266\u001b[1;33m         \u001b[0mrnd_latent_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'd_latent'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    267\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'BATCH_SIZE'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m25\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mamax\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\amp21\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36mamax\u001b[1;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m   2752\u001b[0m     \u001b[1;36m5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2753\u001b[0m     \"\"\"\n\u001b[1;32m-> 2754\u001b[1;33m     return _wrapreduction(a, np.maximum, 'max', axis, None, out,\n\u001b[0m\u001b[0;32m   2755\u001b[0m                           keepdims=keepdims, initial=initial, where=where)\n\u001b[0;32m   2756\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\amp21\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[1;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[0;32m     84\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "from transvae import trans_models\n",
    "from transvae.transformer_models import TransVAE\n",
    "from transvae.rnn_models import RNN, RNNAttn\n",
    "from transvae.wae_models import WAE\n",
    "from transvae.aae_models import AAE\n",
    "from transvae.tvae_util import *\n",
    "from transvae import analysis\n",
    "import glob\n",
    "import re\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import Isomap\n",
    "from sklearn import metrics\n",
    "from sklearn.manifold import trustworthiness\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import NullFormatter\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import coranking #coranking.readthedocs.io\n",
    "from coranking.metrics import trustworthiness, continuity, LCMC\n",
    "from transvae.snc import SNC #github.com/hj-n/steadiness-cohesiveness\n",
    "\n",
    "import Bio\n",
    "from Bio import pairwise2\n",
    "from Bio.Align import substitution_matrices\n",
    "\n",
    "def loss_plots(loss_src):\n",
    "    tot_loss = analysis.plot_loss_by_type(src,loss_types=['tot_loss'])\n",
    "    plt.savefig(save_dir+'tot_loss.png')\n",
    "    recon_loss = analysis.plot_loss_by_type(src,loss_types=['recon_loss'])\n",
    "    plt.savefig(save_dir+'recon_loss.png')\n",
    "    kld_loss = analysis.plot_loss_by_type(src,loss_types=['kld_loss'])\n",
    "    plt.savefig(save_dir+'kld_loss.png')\n",
    "    prob_bce_loss = analysis.plot_loss_by_type(src,loss_types=['prop_bce_loss'])\n",
    "    plt.savefig(save_dir+'prob_bce_loss.png')\n",
    "    if 'aae' in src:\n",
    "        disc_loss = analysis.plot_loss_by_type(src,loss_types=['disc_loss'])\n",
    "        plt.savefig(save_dir+'disc_loss.png')\n",
    "    if 'wae' in src:\n",
    "        mmd_loss = analysis.plot_loss_by_type(src,loss_types=['mmd_loss'])\n",
    "        plt.savefig(save_dir+'mmd_loss.png')\n",
    "    plt.close('all')\n",
    "    \n",
    "def load_reconstructions(data,data_1D,latent_size, load_src, true_props=None,subset=None):\n",
    "    \n",
    "    recon_src = load_src+model.name+\"_\"+re.split('(\\d{2,3})',latent_size[0])[0]+\"_\"+re.split('(\\d{2,3})',latent_size[0])[1]+\"//saved_info.csv\"\n",
    "    recon_df = pd.read_csv(recon_src)\n",
    "    reconstructed_seq = recon_df['reconstructions'].to_list()[:num_sequences]\n",
    "    props = torch.Tensor(recon_df['predicted properties'][:num_sequences])\n",
    "    true_props_data = pd.read_csv(true_props).to_numpy()\n",
    "    true_props = true_props_data[0:num_sequences,0]\n",
    "    \n",
    "    if subset:\n",
    "        testing = pd.read_csv(subset).to_numpy()\n",
    "        test_idx_list = [np.where(data==testing[idx][0]) for idx in range(len(testing))]\n",
    "\n",
    "\n",
    "        batch_recon_len = len(reconstructed_seq)\n",
    "        reconstructed_seq = [reconstructed_seq[test_idx_list[i][0][0]] for i in range(len(test_idx_list)) if test_idx_list[i][0][0]<batch_recon_len]\n",
    "        data_1D= [data_1D[test_idx_list[i][0][0]] for i in range(len(test_idx_list)) if test_idx_list[i][0][0]<batch_recon_len]\n",
    "        props = [props[test_idx_list[i][0][0]] for i in range(len(test_idx_list)) if test_idx_list[i][0][0]<batch_recon_len]\n",
    "        props=torch.Tensor(props)\n",
    "        data = testing[:][0]\n",
    "        true_props_data = pd.read_csv(true_props).to_numpy()\n",
    "        true_props = true_props_data[0:num_sequences,0]\n",
    "        true_props= [true_props[test_idx_list[i][0][0]] for i in range(len(test_idx_list)) if test_idx_list[i][0][0]<batch_recon_len]\n",
    "\n",
    "    return data, data_1D, true_props, props, reconstructed_seq\n",
    "\n",
    "########################################################################################\n",
    "gpu = True\n",
    "\n",
    "num_sequences = 500#_000\n",
    "batch_size = 200 #setting for reconstruction\n",
    "example_data = 'data\\\\peptides\\\\datasets\\\\uniprot_v2\\\\peptide_train.txt'\n",
    "save_dir_loc = 'model_analyses\\\\train\\\\' #folder in which to save outpts\n",
    "save_dir_name = 'train' #appended to identify data: train|test|other|etc...\n",
    "\n",
    "reconstruct=True #True:reconstruct data here; False:load reconstructions from file\n",
    "recon_src = \"checkpointz//analyses_ckpts//\" #directory in which all reconstructions are stored\n",
    "true_prop_src = 'data\\\\peptides\\\\datasets\\\\uniprot_v2\\\\function_train.txt' #if property predictor load the true labels\n",
    "subset_src = \"\" #(optional) this file should have the true sequences for a subset of the \"example data\" above\n",
    "\n",
    "ckpt_list = glob.glob(\"\"+\"checkpointz\\\\to_slurm//**//*.ckpt\", recursive = True) #grab all checkpoint\n",
    "print('current working directory: ',os.getcwd())\n",
    "\n",
    "\n",
    "for i in range(len(ckpt_list)):\n",
    "    \n",
    "    #search the current directory for the model name and load that model\n",
    "    model_dic = {'trans':'TransVAE','aae':'AAE','rnnattn':'RNNAttn','rnn':'RNN','wae':'WAE'}\n",
    "    model_src = ckpt_list[i]\n",
    "    print('working on: ',model_src,'\\n')\n",
    "    model_name = list(filter(None,[key for key in model_dic.keys() if key in model_src.split('\\\\')[-1]]))\n",
    "    model = locals()[model_dic[model_name[0]]](load_fn=model_src) #use locals to call model specific constructor\n",
    "    \n",
    "    #create save directory for the current model according to latent space size\n",
    "    latent_size = re.findall('(latent[\\d]{2,3})', model_src)\n",
    "    save_dir= save_dir_loc+model.name+\"_\"+latent_size[0]+\"_\"+save_dir_name\n",
    "    if not os.path.exists(save_dir):os.mkdir(save_dir) \n",
    "    save_dir= save_dir+\"//\" \n",
    "#     save_df = pd.DataFrame() #this will hold the number variables and save to CSV\n",
    "    \n",
    "    #load the true labels\n",
    "    data = pd.read_csv(example_data).to_numpy() \n",
    "    data_1D = data[:num_sequences,0] #gets rid of extra dimension\n",
    "    true_props_data = pd.read_csv(true_prop_src).to_numpy()\n",
    "    true_props = true_props_data[0:num_sequences,0]\n",
    "\n",
    "    \n",
    "#     #get the log.txt file from the ckpt and model name then plot loss curves\n",
    "#     loss_src = '_'.join( (\"log\",model_src.split('\\\\')[-1].split('_')[1],model_src.split('\\\\')[-1].split('_')[2][:-4]+\"txt\") )\n",
    "#     src= '\\\\'.join([str(i) for i in model_src.split('\\\\')[:-1]])+\"\\\\\"+loss_src\n",
    "#     print(loss_src, src)\n",
    "#     loss_plots(src)\n",
    "    \n",
    "#     #set the batch size and reconstruct the data\n",
    "#     model.params['BATCH_SIZE'] = batch_size\n",
    "#     if reconstruct:\n",
    "#         reconstructed_seq, props = model.reconstruct(data[:num_sequences], log=False, return_mems=False)\n",
    "#     else:\n",
    "#         data, data_1D, true_props, props, reconstructed_seq = load_reconstructions(data, data_1D,latent_size,\n",
    "#                                                                                    load_src=recon_src,\n",
    "#                                                                                    true_props=true_prop_src)\n",
    "#     if gpu:torch.cuda.empty_cache() #free allocated CUDA memory\n",
    "    \n",
    "#     #save the metrics to the dataframe\n",
    "#     save_df['reconstructions'] = reconstructed_seq #placing the saves on a line separate from the ops allows for editing\n",
    "#     save_df['predicted properties'] = [prop.item() for prop in props[:len(reconstructed_seq)]]\n",
    "#     prop_acc, prop_conf, MCC=calc_property_accuracies(props[:len(reconstructed_seq)],true_props[:len(reconstructed_seq)], MCC=True)\n",
    "#     save_df['property prediction accuracy'] = prop_acc\n",
    "#     save_df['property prediction confidence'] = prop_conf\n",
    "#     save_df['MCC'] = MCC\n",
    "    \n",
    "\n",
    "# #   First we tokenize the input and reconstructed smiles\n",
    "#     input_sequences = []\n",
    "#     for seq in data_1D:\n",
    "#         input_sequences.append(peptide_tokenizer(seq))\n",
    "#     output_sequences = []\n",
    "#     for seq in reconstructed_seq:\n",
    "#         output_sequences.append(peptide_tokenizer(seq))\n",
    "    \n",
    "#     seq_accs, tok_accs, pos_accs, seq_conf, tok_conf, pos_conf = calc_reconstruction_accuracies(input_sequences, output_sequences)\n",
    "#     save_df['sequence accuracy'] = seq_accs\n",
    "#     save_df['sequence confidence'] = seq_conf\n",
    "#     save_df['token accuracy'] = tok_accs\n",
    "#     save_df['token confidence'] = tok_conf\n",
    "#     save_df = pd.concat([pd.DataFrame({'position_accs':pos_accs,'position_confidence':pos_conf }), save_df], axis=1)\n",
    "    \n",
    "    ##moving into memory and entropy\n",
    "    if model.model_type =='aae':\n",
    "        mus, _, _ = model.calc_mems(data[:50_000], log=False, save=False) \n",
    "    elif model.model_type == 'wae':\n",
    "        mus, _, _ = model.calc_mems(data[:50_000], log=False, save=False) \n",
    "    else:\n",
    "        mems, mus, logvars = model.calc_mems(data[:50_000], log=False, save=False) #subset size 1200*35=42000 would be ok\n",
    "\n",
    "#     ##calculate the entropies\n",
    "#     vae_entropy_mus = calc_entropy(mus)\n",
    "#     save_df = pd.concat([save_df,pd.DataFrame({'mu_entropies':vae_entropy_mus})], axis=1)\n",
    "#     if model.model_type != 'wae' and model.model_type!= 'aae': #these don't have a variational type bottleneck\n",
    "#         vae_entropy_mems  = calc_entropy(mems)\n",
    "#         save_df = pd.concat([save_df,pd.DataFrame({'mem_entropies':vae_entropy_mems})], axis=1)\n",
    "#         vae_entropy_logvars = calc_entropy(logvars)\n",
    "#         save_df = pd.concat([save_df,pd.DataFrame({'logvar_entropies':vae_entropy_logvars})], axis=1)\n",
    "    \n",
    "\n",
    "\n",
    "#     #create random index and re-index ordered memory list creating n random sub-lists (ideally resulting in IID random lists)\n",
    "#     random_idx = np.random.permutation(np.arange(stop=mus.shape[0]))\n",
    "#     mus = mus[random_idx]\n",
    "#     data = data[random_idx]\n",
    "\n",
    "#     subsample_start=0\n",
    "#     subsample_length=mus.shape[0] #this may change depending on batch size\n",
    "\n",
    "#     #(for length based coloring): record all peptide lengths iterating through input\n",
    "#     pep_lengths = []\n",
    "#     for idx, pep in enumerate(data[subsample_start:(subsample_start+subsample_length)]):\n",
    "#         pep_lengths.append( len(pep[0]) )   \n",
    "#     #(for function based coloring): pull function from csv with peptide functions\n",
    "#     s_to_f =pd.read_csv(true_prop_src)    \n",
    "#     function = s_to_f['peptides'][subsample_start:(subsample_start+subsample_length)]\n",
    "#     function = function[random_idx] #account for random permutation\n",
    "\n",
    "#     pca = PCA(n_components=5)\n",
    "#     pca_batch =pca.fit_transform(X=mus[:])\n",
    "\n",
    "#     #plot format dictionnaries\n",
    "#     titles={'text':'{}'.format(model.model_type.replace(\"_\",\" \").upper()),\n",
    "#                           'x':0.5,'xanchor':'center','yanchor':'top','font_size':40}\n",
    "#     general_fonts={'family':\"Helvetica\",'size':30,'color':\"Black\"}\n",
    "#     colorbar_fmt={'title_font_size':30,'thickness':15,'ticks':'','title_text':'Lengths',\n",
    "#                                'ticklabelposition':\"outside bottom\"}\n",
    "    \n",
    "#     fig = px.scatter(pd.DataFrame({\"PC1\":pca_batch[:,0],\"PC2\":pca_batch[:,1], \"lengths\":pep_lengths}),\n",
    "#                 symbol_sequence=['hexagon2'],x='PC1', y='PC2', color=\"lengths\",\n",
    "#                 color_continuous_scale='Jet',template='simple_white', opacity=0.9)\n",
    "#     fig.update_traces(marker=dict(size=9))\n",
    "#     fig.update_layout(title=titles,xaxis_title=\"PC1\", yaxis_title=\"PC2\",font=general_fonts)\n",
    "#     fig.update_coloraxes(colorbar=colorbar_fmt)\n",
    "#     fig.write_image(save_dir+'pca_length.png', width=900, height=600)\n",
    "\n",
    "#     fig = px.scatter(pd.DataFrame({\"PC1\":pca_batch[:,0],\"PC2\":pca_batch[:,1], \n",
    "#                                     \"Function\":list(map(lambda itm: \"AMP\" if itm==1 else \"NON-AMP\",function))}),\n",
    "#                                     x='PC1', y='PC2', color=\"Function\",symbol_sequence=['x-thin-open','circle'],\n",
    "#                                     template='simple_white',symbol='Function', opacity=0.8) \n",
    "#     fig.update_traces(marker=dict(size=9))\n",
    "#     fig.update_layout(title=titles,xaxis_title=\"PC1\",yaxis_title=\"PC2\",font=general_fonts)\n",
    "#     fig.write_image(save_dir+'pca_function.png', width=900, height=600)\n",
    "    \n",
    "#     # Plot the explained variances\n",
    "#     plt.bar(range(pca.n_components_), pca.explained_variance_ratio_*100, color='black')\n",
    "#     plt.xlabel('PCA features')\n",
    "#     plt.ylabel('variance %')\n",
    "#     plt.xticks(features)\n",
    "#     plt.savefig(save_dir+'variance_explained.png')\n",
    "\n",
    "#     fig = px.scatter_matrix(pd.DataFrame({\"PC1\":pca_batch[:,0],\"PC2\":pca_batch[:,1],\"PC3\":pca_batch[:,2],\n",
    "#                                     \"lengths\":pep_lengths}), dimensions=[\"PC1\",\"PC2\",\"PC3\"],\n",
    "#                                     symbol_sequence=['hexagon2'],template='simple_white',\n",
    "#                                     color=\"lengths\",color_continuous_scale='Jet', opacity=0.9)\n",
    "#     fig.update_traces(diagonal_visible=False)\n",
    "#     fig.update_layout(title=titles,xaxis_title=\"PC1\", yaxis_title=\"PC2\",font=general_fonts)\n",
    "#     fig.write_image(save_dir+'pca_matrix_length.png', width=1920, height=1080) \n",
    "    \n",
    "#     fig = px.scatter_matrix(pd.DataFrame({\"PC1\":pca_batch[:,0],\"PC2\":pca_batch[:,1],\"PC3\":pca_batch[:,2], \n",
    "#                                    \"Function\":list(map(lambda itm: \"AMP\" if itm==1 else \"NON-AMP\",function))}),\n",
    "#                                     dimensions=[\"PC1\",\"PC2\",\"PC3\"],template='simple_white',\n",
    "#                                     color=\"Function\",symbol_sequence=['x-thin','circle'],\n",
    "#                                     symbol='Function', opacity=0.8) \n",
    "#     fig.update_traces(diagonal_visible=False)\n",
    "#     fig.update_layout(title=titles,xaxis_title=\"PC1\", yaxis_title=\"PC2\",font=general_fonts)\n",
    "#     fig.write_image(save_dir+'pca_matrix_function.png', width=1920, height=1080) \n",
    "\n",
    "#     #create n subsamples and calculate silhouette score for each\n",
    "#     latent_mem_func_subsamples = []\n",
    "#     pca_func_subsamples = []\n",
    "#     n=250\n",
    "#     for s in range(n):\n",
    "#         s_len = len(mus)//n #sample lengths\n",
    "#         mem_func_sil = metrics.silhouette_score(mus[s_len*s:s_len*(s+1)], function[s_len*s:s_len*(s+1)], metric='euclidean')\n",
    "#         latent_mem_func_subsamples.append(mem_func_sil)\n",
    "#         XY = [i for i in zip(pca_batch[s_len*s:s_len*(s+1),0], pca_batch[s_len*s:s_len*(s+1),1])]\n",
    "#         pca_func_sil = metrics.silhouette_score(XY, function[s_len*s:s_len*(s+1)], metric='euclidean')\n",
    "#         pca_func_subsamples.append(pca_func_sil)\n",
    "#     save_df = pd.concat([save_df,pd.DataFrame({'latent_mem_func_silhouette':latent_mem_func_subsamples})], axis=1)\n",
    "#     save_df = pd.concat([save_df,pd.DataFrame({'pca_func_silhouette':pca_func_subsamples})], axis=1)\n",
    "\n",
    "#     save_df.to_csv(save_dir+\"saved_info.csv\", index=False)\n",
    "    \n",
    "#New section dealing with sequence generation metrics and bootstrapping from the latent space\n",
    "    rnd_seq_count = 1_000\n",
    "    rnd_latent_list=[] #generate N latent space vectors\n",
    "    for seq in range(rnd_seq_count):\n",
    "        rnd_latent_list.append( np.array([random.uniform(np.min(mus),np.max(mus)) for i in range(model.params['d_latent'])]).astype(np.float32) )\n",
    "    \n",
    "    model.params['BATCH_SIZE'] = 25\n",
    "    rnd_token_list=np.empty((rnd_seq_count,model.tgt_len)) #store N decoded latent vectors now in token(0-20) form max length 125\n",
    "    print('pass_check')\n",
    "    for batch in range(0,rnd_seq_count,model.params['BATCH_SIZE']):\n",
    "        rnd_token_list[batch:batch+model.params['BATCH_SIZE']] =  model.greedy_decode(torch.tensor(rnd_latent_list[batch:batch+model.params['BATCH_SIZE']]).cuda()).cpu()\n",
    "    \n",
    "    decoded_rnd_seqs = decode_mols(torch.tensor(rnd_token_list), model.params['ORG_DICT'])\n",
    "    decoded_rnd_seqs[:]=[x for x in decoded_rnd_seqs if x] #removes the empty lists\n",
    "    z=1.96 #95% confidence interval\n",
    "    percent_unique = len(set(decoded_rnd_seqs)) / rnd_seq_count\n",
    "    unique_conf = z*math.sqrt(percent_unique*(1-percent_unique)/rnd_seq_count)\n",
    "    percent_unique, unique_conf\n",
    "    \n",
    "    df_gen_scores = {}\n",
    "    df_gen_scores.update({'percent_unique': percent_unique})\n",
    "    df_gen_scores.update({'unique_confidence':unique_conf})\n",
    "    \n",
    "    #sample N test set sequences randomly\n",
    "    shuffled_test = random.sample(data.tolist(),len(data))\n",
    "    shuffled_test = np.array(shuffled_test[:len(decoded_rnd_seqs)])\n",
    "    combined = np.concatenate(( shuffled_test,np.array(decoded_rnd_seqs).reshape(len(decoded_rnd_seqs),1)) )\n",
    "    percent_novel = len(set(combined.flatten().tolist()))/(2*len(decoded_rnd_seqs))\n",
    "    novel_conf =  z*math.sqrt(percent_novel*(1-percent_novel)/(2*len(decoded_rnd_seqs)))\n",
    "    percent_novel, novel_conf\n",
    "    df_gen_scores.update({'percent_novel':percent_novel})\n",
    "    df_gen_scores.update({'novel_confidence':novel_conf})\n",
    "    \n",
    "    shuffled_test = shuffled_test.flatten().tolist()\n",
    "    similarity_score=[]\n",
    "    matrix = substitution_matrices.load(\"BLOSUM62\")\n",
    "    for seq in shuffled_test[:10_000:100]: #grab 100 test set peptides\n",
    "        for seq2 in decoded_rnd_seqs[::10]: #grab 100 of the 1000 random latent peptides\n",
    "            similarity_score.append( pairwise2.align.globaldx(seq,seq2, matrix, score_only=True)/(len(seq)+len(seq2)) )\n",
    "            \n",
    "  \n",
    "    df_gen_scores.update({'average_sequence_similarity': np.average(similarity_score)})\n",
    "    df_gen_scores.update({'std_on_similarity_score': np.std(similarity_score)})\n",
    "    \n",
    "    #GLFDIWKKWRWRR is an AMP in the test set. Use it as a reference point in the latent space\n",
    "    model.params['BATCH_SIZE'] = 1\n",
    "    if model.model_type =='aae':\n",
    "        mus, _, _ = model.calc_mems(np.array([['GLIDTVKNMAINAAKSAGMSVLKTLSCKLSKEC']], dtype='O'), log=False, save=False) \n",
    "    elif model.model_type == 'wae':\n",
    "        mus, _, _ = model.calc_mems(np.array([['GLIDTVKNMAINAAKSAGMSVLKTLSCKLSKEC']], dtype='O'), log=False, save=False) \n",
    "    else:\n",
    "        mems, mus, logvars = model.calc_mems(np.array([['GLIDTVKNMAINAAKSAGMSVLKTLSCKLSKEC']], dtype='O'), log=False, save=False) #subset size 1200*35=42000 would be ok\n",
    "    \n",
    "    if model.model_type =='aae' or model.model_type =='wae':\n",
    "        nearby_samples = np.random.normal(loc=0,scale=1,size=(500,1,model.params['d_latent'])).astype(np.float32)*0.4 + mus\n",
    "    else:\n",
    "        nearby_samples = np.random.normal(loc=0,scale=1,size=(500,1,model.params['d_latent'])).astype(np.float32)*np.exp(0.5*logvars) + mus\n",
    "    \n",
    "    rnd_seq_count=500\n",
    "    model.params['BATCH_SIZE'] = 50\n",
    "    rnd_token_list=np.empty((rnd_seq_count,model.tgt_len)) #store N decoded latent vectors now in token(0-20) form max length 125\n",
    "    for batch in range(0,rnd_seq_count,model.params['BATCH_SIZE']):\n",
    "        rnd_token_list[batch:batch+model.params['BATCH_SIZE']] =  model.greedy_decode(torch.tensor(nearby_samples[batch:batch+model.params['BATCH_SIZE']]).squeeze().cuda()).cpu()\n",
    "    \n",
    "    decoded_rnd_seqs = decode_mols(torch.tensor(rnd_token_list), model.params['ORG_DICT'])                                             \n",
    "             \n",
    "    z=1.96 #95% confidence interval\n",
    "    amp_percent_unique = len(set(decoded_rnd_seqs))/len(decoded_rnd_seqs)\n",
    "    amp_unique_conf = z*math.sqrt(amp_percent_unique*(1-amp_percent_unique)/rnd_seq_count)\n",
    "    df_gen_scores.update({'amp_uniqueness': amp_percent_unique})\n",
    "    df_gen_scores.update({'amp_uniqueness_std': amp_unique_conf})\n",
    "                                                \n",
    "    k=2\n",
    "    jac_scores = np.empty((len(list(set(decoded_rnd_seqs))),1))\n",
    "    jac_scores.shape\n",
    "    for i,decoded_seq in enumerate(list(set(decoded_rnd_seqs))):\n",
    "        for j,seq in enumerate(np.array(['GLIDTVKNMAINAAKSAGMSVLKTLSCKLSKEC'], dtype='O')):\n",
    "            jac_scores[i,j] = (jaccard_similarity(build_kmers(seq,k), build_kmers(decoded_seq,k)))\n",
    "    df_gen_scores.update({'amp_jac_score': np.average(jac_scores)})\n",
    "    df_gen_scores.update({'amp_jac_score_std': np.std(jac_scores)})\n",
    "    np.average(jac_scores), np.std(jac_scores) \n",
    "\n",
    "    model.params['BATCH_SIZE'] = 2\n",
    "    reconstructed_seq, props = model.reconstruct(np.array([[seq] for seq in list(set(decoded_rnd_seqs))]), log=False, return_mems=False)\n",
    "    props[props>1]=1 #sometimes the model outputs a probability >1 for a class so threshold\n",
    "    amp_percent_amp = sum(props.round()).item()/len(props)\n",
    "    amp_amp_conf = z*math.sqrt(amp_percent_amp*(1-amp_percent_amp)/len(props))\n",
    "    df_gen_scores.update({'predicted_amps': amp_percent_amp})\n",
    "    df_gen_scores.update({'predicted_amps_conf': amp_amp_conf})    \n",
    "    df = pd.DataFrame.from_dict([df_gen_scores])\n",
    "    pd.DataFrame.from_dict([df_gen_scores]).to_csv(save_dir+\"generation_metrics.csv\", index=False)\n",
    "    \n",
    "    #GLFDIWKKWRWRR is an AMP in the test set. Use it as a reference point in the latent space\n",
    "    #GLIDTVKNMAINAAKSAGMSVLKTLSCKLSKEC is an amp in the training set also can be used as a reference pt in latent space\n",
    "    model.params['BATCH_SIZE'] = 200\n",
    "    if model.model_type =='aae':\n",
    "        mus, _, _ = model.calc_mems(data[:20_000], log=False, save=False) \n",
    "    elif model.model_type == 'wae':\n",
    "        mus, _, _ = model.calc_mems(data[:20_000], log=False, save=False) \n",
    "    else:\n",
    "        mems, mus, logvars = model.calc_mems(data[:20_000], log=False, save=False)\n",
    "        \n",
    "    \n",
    "    true_prop_src = 'data\\\\peptides\\\\datasets\\\\uniprot_v2\\\\function_test.txt'\n",
    "    model.params['BATCH_SIZE'] = 1\n",
    "    subsample_start=0\n",
    "    subsample_length=mus.shape[0] #this may change depending on batch size\n",
    "\n",
    "    #(for length based coloring): record all peptide lengths iterating through input\n",
    "    pep_lengths = []\n",
    "    for idx, pep in enumerate(data[subsample_start:(subsample_start+subsample_length)]):\n",
    "        pep_lengths.append( len(pep[0]) )   \n",
    "    #(for function based coloring): pull function from csv with peptide functions\n",
    "    s_to_f =pd.read_csv(true_prop_src)    \n",
    "    function = s_to_f['peptides'][subsample_start:(subsample_start+subsample_length)]\n",
    "\n",
    "    pca = PCA(n_components=3)\n",
    "    pca_batch =pca.fit_transform(X=mus[:])\n",
    "    pca_generated = pca.transform(nearby_samples.squeeze())\n",
    "\n",
    "    if model.model_type =='aae' or model.model_type =='wae':\n",
    "        amp_mus, _, _ = model.calc_mems(np.array([['GLIDTVKNMAINAAKSAGMSVLKTLSCKLSKEC']], dtype='O'), log=False, save=False)\n",
    "    else:\n",
    "        amp_mems, amp_mus, amp_logvars = model.calc_mems(np.array([['GLIDTVKNMAINAAKSAGMSVLKTLSCKLSKEC']], dtype='O'), log=False, save=False)\n",
    "    pca_amp = pca.transform(amp_mus)\n",
    "\n",
    "    #plot format dictionnaries\n",
    "    titles={'text':'{}'.format(model.model_type.replace(\"_\",\" \").upper()),\n",
    "                          'x':0.5,'xanchor':'center','yanchor':'top','font_size':40}\n",
    "    general_fonts={'family':\"Helvetica\",'size':30,'color':\"Black\"}\n",
    "    colorbar_fmt={'title_font_size':30,'thickness':15,'ticks':'','title_text':'Lengths',\n",
    "                               'ticklabelposition':\"outside bottom\", 'showscale':'False'}\n",
    "\n",
    "    fig1 = px.scatter(pd.DataFrame({\"PC1\":pca_batch[:,0],\"PC2\":pca_batch[:,1], \n",
    "                                    \"Function\":list(map(lambda itm: \"AMP\" if itm==1 else \"NON-AMP\",function))}),\n",
    "                                    x='PC1', y='PC2', color=\"Function\",symbol_sequence=['x-thin-open','circle'],\n",
    "                                    template='simple_white',symbol='Function', opacity=0.3) \n",
    "    fig2 = px.scatter(pd.DataFrame({\"PC1\":pca_generated[:,0],\"PC2\":pca_generated[:,1],\n",
    "                                    'color':[\"Generated\" for i in pca_generated[:,0]],}),\n",
    "                                    x='PC1', y='PC2',color='color',labels='label',symbol_sequence=['asterisk-open'],\n",
    "                                    template='simple_white', opacity=0.9)\n",
    "    fig2.update_traces(marker=dict(color='red'))\n",
    "    fig3 = px.scatter(pd.DataFrame({\"PC1\":pca_amp[:,0],\"PC2\":pca_amp[:,1], 'color':['source AMP' for i in pca_amp[:,0]]}),\n",
    "                                    x='PC1', y='PC2',color='color',symbol_sequence=['cross'],\n",
    "                                    template='simple_white', opacity=1)\n",
    "    fig3.update_traces(marker=dict(size=12, color='black'))\n",
    "    fig1.update_traces(marker=dict(size=9))\n",
    "    fig2.update_traces(marker=dict(size=9))\n",
    "    fig = go.Figure(data= fig1.data+fig2.data+fig3.data)\n",
    "    fig.update_coloraxes(showscale=False)\n",
    "    fig.update_layout(title=titles,xaxis_title=\"PC1\",yaxis_title=\"PC2\",font=general_fonts)\n",
    "    fig.write_image(save_dir+'amp_sample.png', width=1_000, height=700)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8307e206",
   "metadata": {},
   "source": [
    "<H4>Since Compute Canada does not do the dimensionality reduction metrics we need to do them below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fc2219",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current working directory:  C:\\Users\\s_renaud\\Documents\\GitHub\\MSCSAM_TBD\\main_model\n",
      "working on:  checkpointz\\to_slurm\\rnn_latent128\\300_rnn-128_peptide.ckpt \n",
      "\n",
      "analysis:  model_analyses\\test\\rnn-128_peptide_latent128_test\\saved_info.csv checkpoint:  checkpointz\\to_slurm\\rnn_latent128\\300_rnn-128_peptide.ckpt\n",
      "rnn-128_peptide\n",
      "0 0.7263278906062263 0.8013199558729199 0.0733011277176656\n",
      "0.7152629414570874 0.6150958533741794\n",
      "1 0.7172063902859634 0.7914899201190343 0.0715827182435673\n",
      "0.7058813278944015 0.6286144357018227\n",
      "2 0.7155813759984018 0.7916761788451923 0.06936732373353666\n",
      "0.7051496672660817 0.6336141042480539\n",
      "3 0.7216968580864563 0.7970960963455368 0.06890977742513325\n",
      "0.7165728671426881 0.6510426816726227\n",
      "4 0.7273828882274378 0.8011807891448378 0.06981649325874331\n",
      "0.7102687106527645 0.6370959883035602\n",
      "5 0.724585498669805 0.7987928943939604 0.07576132908707749\n",
      "0.7010568947270404 0.6579795792161849\n",
      "6 0.7189782410502227 0.7998008640811263 0.07084000794804672\n",
      "0.7221717835460162 0.636167830283304\n",
      "7 0.7176991455014009 0.7958293705552745 0.07157554775563339\n",
      "0.7066581457762533 0.6564316469718086\n",
      "8 0.71205263779221 0.7884952426697387 0.06890429263006151\n",
      "0.7072851081750504 0.6328576686956029\n",
      "9 0.7200353297727224 0.794680569333982 0.06967660780900795\n",
      "0.7131667396758378 0.6307346970608995\n",
      "10 0.7275861168769898 0.7988148629038303 0.07199956546674782\n",
      "0.7164932357743923 0.6260320924223415\n",
      "11 0.7223850459387979 0.7997598083605316 0.07212995041326764\n",
      "0.7048092095584633 0.6502356057663363\n",
      "12 0.7162237939098574 0.7947964167741584 0.07222539091557573\n",
      "0.7443194662684479 0.6202940248653486\n",
      "13 0.7161377092465653 0.7927748817001279 0.06895485390615458\n",
      "0.7370715369368208 0.6111129298607885\n",
      "14 0.7213669196822301 0.7935577297538732 0.06833970819655515\n",
      "0.6949010422792874 0.6460519213974848\n",
      "working on:  checkpointz\\to_slurm\\rnn_latent32\\300_rnn-128_peptide.ckpt \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s_renaud\\Documents\\GitHub\\MSCSAM_TBD\\main_model\\transvae\\trans_models.py:54: UserWarning:\n",
      "\n",
      "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analysis:  model_analyses\\test\\rnn-128_peptide_latent32_test\\saved_info.csv checkpoint:  checkpointz\\to_slurm\\rnn_latent32\\300_rnn-128_peptide.ckpt\n",
      "rnn-128_peptide\n",
      "0 0.6880380225019455 0.7798709712605748 0.06627739182811522\n",
      "0.6135066966994913 0.7198686646522403\n",
      "1 0.6857518235161038 0.7781790646514453 0.06219396814358677\n",
      "0.6192396065750676 0.710346533661266\n",
      "2 0.6989112341332542 0.7855606921847275 0.07296879467552649\n",
      "0.6200832655942752 0.7294500587626398\n",
      "3 0.6842890203442478 0.7741448419083942 0.0643408282248118\n",
      "0.6162378511973263 0.717832203344827\n",
      "4 0.6900583866749541 0.7771103011006475 0.06411888073001792\n",
      "0.6106633344002548 0.725982461720017\n",
      "5 0.6841661794522836 0.773414452418139 0.06857221431855808\n",
      "0.6255874404546424 0.7011656968114361\n",
      "6 0.6923723821967731 0.7804090504574986 0.06731805134180202\n",
      "0.6072037878208381 0.7140680598341926\n",
      "7 0.693119596561149 0.7833630706516134 0.06734564269736318\n",
      "0.6200257261986079 0.71552681374486\n",
      "8 0.6916022070316151 0.7785283598537547 0.07061099922748273\n",
      "0.6194419760076038 0.7187650914417907\n",
      "9 0.6884189014112976 0.777687490590734 0.06593181206175104\n",
      "0.6253298586321687 0.7148146656111374\n",
      "10 0.691961353247303 0.7776709167230244 0.06811170117682251\n",
      "0.6143281545467989 0.7175548650148568\n",
      "11 0.6928259791466017 0.78234862104259 0.0693538852541955\n",
      "0.6213309583471529 0.7205204672745913\n",
      "12 0.6927177128715228 0.7795601861901712 0.06686639678096147\n",
      "0.6201628686199072 0.7190079392981787\n",
      "13 0.6961074948821405 0.7805011315967362 0.06660441553142629\n",
      "0.6112057240696214 0.7418304351010615\n",
      "14 0.6962479059076281 0.7812485532289677 0.06958052874660474\n",
      "0.6205033719802464 0.7396150377297117\n",
      "working on:  checkpointz\\to_slurm\\rnn_latent64\\300_rnn-128_peptide.ckpt \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s_renaud\\Documents\\GitHub\\MSCSAM_TBD\\main_model\\transvae\\trans_models.py:54: UserWarning:\n",
      "\n",
      "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analysis:  model_analyses\\test\\rnn-128_peptide_latent64_test\\saved_info.csv checkpoint:  checkpointz\\to_slurm\\rnn_latent64\\300_rnn-128_peptide.ckpt\n",
      "rnn-128_peptide\n",
      "0 0.7300701591465508 0.8122864720256364 0.0749737024369066\n",
      "0.7287871389634732 0.632182260405256\n",
      "1 0.7319761107778988 0.8151575407746705 0.07841880388113903\n",
      "0.7245793580074413 0.6258209668856803\n",
      "2 0.7277143835908246 0.8117680071329 0.07541178340561856\n",
      "0.7103801072166633 0.6238262977356805\n",
      "3 0.7353295093172799 0.8194648627803142 0.07997016108468606\n",
      "0.7328077185743048 0.617868866934086\n",
      "4 0.7358159752004113 0.8142707857529893 0.07735419422574104\n",
      "0.7174752841676452 0.6050244436865764\n",
      "5 0.7298872849712784 0.8125320855531135 0.0730988239076381\n",
      "0.7302507784883007 0.6087263573487184\n",
      "6 0.7388238971789775 0.8202667355751466 0.08355252246671999\n",
      "0.736200297330778 0.6132691948561331\n",
      "7 0.7348557144671303 0.817013228621897 0.08115438636154078\n",
      "0.724666156946523 0.6017808961286154\n",
      "8 0.727138583320619 0.8074262516189208 0.07614840047692537\n",
      "0.7011956322103499 0.6413370206121229\n",
      "9 0.7362715037286375 0.8220927136393774 0.08149355653888862\n",
      "0.7427459514999182 0.6068836966340221\n",
      "10 0.723575680437846 0.8103030841840629 0.07744117385521723\n",
      "0.7053330075951896 0.6337699208581788\n",
      "11 0.7328462005931021 0.8136219776953575 0.07911469649584325\n",
      "0.7112086783023829 0.6376348579310456\n",
      "12 0.7290149614288832 0.8119772840449979 0.07584095729222222\n",
      "0.7151648048841994 0.6285375198075387\n",
      "13 0.733111058084197 0.8157136287128048 0.077446994362823\n",
      "0.721804858186831 0.6121392504624414\n",
      "14 0.7330585229439569 0.8149021395198659 0.07491968864549793\n"
     ]
    }
   ],
   "source": [
    "import coranking #coranking.readthedocs.io\n",
    "from coranking.metrics import trustworthiness, continuity, LCMC\n",
    "from transvae.snc import SNC #github.com/hj-n/steadiness-cohesiveness\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "from transvae import trans_models\n",
    "from transvae.transformer_models import TransVAE\n",
    "from transvae.rnn_models import RNN, RNNAttn\n",
    "from transvae.wae_models import WAE\n",
    "from transvae.aae_models import AAE\n",
    "from transvae.tvae_util import *\n",
    "from transvae import analysis\n",
    "import glob\n",
    "import re\n",
    "\n",
    "\n",
    "gpu = True\n",
    "\n",
    "example_data = 'data\\\\peptides\\\\datasets\\\\uniprot_v2\\\\peptide_test.txt'\n",
    "test_train='test'\n",
    "ckpt_list = glob.glob(\"\"+\"checkpointz\\\\to_slurm//**//*.ckpt\", recursive = True) #grab all checkpoints\n",
    "analyses_list = glob.glob(\"model_analyses\\\\test//**/*.csv\", recursive=True) #grab all analyses\n",
    "print('current working directory: ',os.getcwd())\n",
    "\n",
    "for i in range(len(ckpt_list)):\n",
    "    \n",
    "    #search the current directory for the model name and load that model\n",
    "    model_dic = {'trans':'TransVAE','aae':'AAE','rnnattn':'RNNAttn','rnn':'RNN','wae':'WAE'}\n",
    "    model_src = ckpt_list[i]\n",
    "    print('working on: ',model_src,'\\n')\n",
    "    model_name = list(filter(None,[key for key in model_dic.keys() if key in model_src.split('//')[-1]]))\n",
    "    model = locals()[model_dic[model_name[0]]](load_fn=model_src) #use locals to call model specific constructor\n",
    "    \n",
    "    #load the analysis file corresponding to the model from the CC outputs\n",
    "    for idx in range(len(analyses_list)):\n",
    "        if analyses_list[idx].split(\"\\\\\")[-2].find(model_src.split(\"\\\\\")[-2].split(\"_\")[0]) != -1 and analyses_list[idx].split(\"\\\\\")[-2].find(model_src.split(\"\\\\\")[-2].split(\"_\")[1]) != -1:\n",
    "            if analyses_list[idx].find(\"rnnattn\")  != -1 and model_src.find(\"rnnattn\") == -1: continue\n",
    "            save_dir = analyses_list[idx]\n",
    "            cur_analysis = pd.read_csv(save_dir)\n",
    "    print(\"analysis: \",save_dir, \"checkpoint: \",model_src)\n",
    "    save_df = cur_analysis #this will hold the number variables and save to CSV\n",
    "    \n",
    "    #load the true labels\n",
    "    data = pd.read_csv(example_data).to_numpy() \n",
    "    data_1D = data[:,0] #gets rid of extra dimension\n",
    "    \n",
    "    #moving into memory and entropy\n",
    "    if model.model_type =='aae':\n",
    "        mus, _, _ = model.calc_mems(data[:60_000], log=False, save=False) \n",
    "    elif model.model_type == 'wae':\n",
    "        mus, _, _ = model.calc_mems(data[:60_000], log=False, save=False) \n",
    "    else:\n",
    "        mems, mus, logvars = model.calc_mems(data[:60_000], log=False, save=False) #subset size 1200*35=42000 would be ok\n",
    "\n",
    "    #create random index and re-index ordered memory list creating n random sub-lists (ideally resulting in IID random lists)\n",
    "    random_idx = np.random.permutation(np.arange(stop=mus.shape[0]))\n",
    "    mus[:] = mus[random_idx]\n",
    "    data = data[random_idx]\n",
    "    \n",
    "    #need to perform PCA to be able to compare dimensionality reduction quality\n",
    "    pca = PCA(n_components=2)\n",
    "    pca_batch =pca.fit_transform(X=mus[:])\n",
    "    \n",
    "    #now ready to calculation dimensionality reduction accuracy with metrics\n",
    "    trust_subsamples = []\n",
    "    cont_subsamples = []\n",
    "    lcmc_subsamples = []\n",
    "    steadiness_subsamples = []\n",
    "    cohesiveness_subsamples = []\n",
    "    if 'test' in test_train: #different number of bootsraps for train vs test\n",
    "        n=15\n",
    "    else:\n",
    "        n=60\n",
    "    parameter = { \"k\": 50,\"alpha\": 0.1 } #for steadiness and cohesiveness\n",
    "    for s in range(n):\n",
    "        s_len = len(mus)//n\n",
    "        Q = coranking.coranking_matrix(mus[s_len*s:s_len*(s+1)], pca_batch[s_len*s:s_len*(s+1)])\n",
    "        trust_subsamples.append( np.mean(trustworthiness(Q, min_k=1, max_k=50)) )\n",
    "        cont_subsamples.append( np.mean(continuity(Q, min_k=1, max_k=50)) )\n",
    "        lcmc_subsamples.append( np.mean(LCMC(Q, min_k=1, max_k=50)) )\n",
    "        print(s,trust_subsamples[s],cont_subsamples[s],lcmc_subsamples[s])\n",
    "\n",
    "        metrics = SNC(raw=mus[s_len*s:s_len*(s+1)], emb=pca_batch[s_len*s:s_len*(s+1)], iteration=300, dist_parameter=parameter)\n",
    "        metrics.fit() #solve for steadiness and cohesiveness\n",
    "        steadiness_subsamples.append(metrics.steadiness())\n",
    "        cohesiveness_subsamples.append(metrics.cohesiveness())\n",
    "        print(metrics.steadiness(),metrics.cohesiveness())\n",
    "        Q=0 #trying to free RAM\n",
    "        metrics=0\n",
    "        torch.cuda.empty_cache() #free allocated CUDA memory\n",
    "\n",
    "    save_df = pd.concat([save_df,pd.DataFrame({'latent_to_PCA_trustworthiness':trust_subsamples})], axis=1)\n",
    "    save_df = pd.concat([save_df,pd.DataFrame({'latent_to_PCA_continuity':cont_subsamples})], axis=1)\n",
    "    save_df = pd.concat([save_df,pd.DataFrame({'latent_to_PCA_lcmc':lcmc_subsamples})], axis=1)\n",
    "    save_df = pd.concat([save_df,pd.DataFrame({'latent_to_PCA_steadiness':steadiness_subsamples})], axis=1)\n",
    "    save_df = pd.concat([save_df,pd.DataFrame({'latent_to_PCA_cohesiveness':cohesiveness_subsamples})], axis=1)  \n",
    "    \n",
    "    save_df.to_csv(save_dir, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5c006d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
