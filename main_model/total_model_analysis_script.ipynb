{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1d136af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current working directory:  C:\\Users\\s_renaud\\Documents\\GitHub\\MSCSAM_TBD\\main_model\n",
      "working on:  checkpointz\\to_slurm\\aae_latent128\\300_aae-128_peptide.ckpt \n",
      "\n",
      "aae-128_peptide\n",
      "working on:  checkpointz\\to_slurm\\aae_latent32\\300_aae-128_peptide.ckpt \n",
      "\n",
      "aae-128_peptide\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s_renaud\\Documents\\GitHub\\MSCSAM_TBD\\main_model\\transvae\\trans_models.py:54: UserWarning:\n",
      "\n",
      "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on:  checkpointz\\to_slurm\\aae_latent64\\300_aae-128_peptide.ckpt \n",
      "\n",
      "aae-128_peptide\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s_renaud\\Documents\\GitHub\\MSCSAM_TBD\\main_model\\transvae\\trans_models.py:54: UserWarning:\n",
      "\n",
      "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on:  checkpointz\\to_slurm\\rnnattn_latent128\\300_rnnattn-128_peptide.ckpt \n",
      "\n",
      "rnnattn-128_peptide\n",
      "working on:  checkpointz\\to_slurm\\rnnattn_latent32\\300_rnnattn-128_peptide.ckpt \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s_renaud\\Documents\\GitHub\\MSCSAM_TBD\\main_model\\transvae\\trans_models.py:54: UserWarning:\n",
      "\n",
      "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rnnattn-128_peptide\n",
      "working on:  checkpointz\\to_slurm\\rnnattn_latent64\\300_rnnattn-128_peptide.ckpt \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s_renaud\\Documents\\GitHub\\MSCSAM_TBD\\main_model\\transvae\\trans_models.py:54: UserWarning:\n",
      "\n",
      "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rnnattn-128_peptide\n",
      "working on:  checkpointz\\to_slurm\\rnn_latent128\\300_rnn-128_peptide.ckpt \n",
      "\n",
      "rnn-128_peptide\n",
      "working on:  checkpointz\\to_slurm\\rnn_latent32\\300_rnn-128_peptide.ckpt \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s_renaud\\Documents\\GitHub\\MSCSAM_TBD\\main_model\\transvae\\trans_models.py:54: UserWarning:\n",
      "\n",
      "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rnn-128_peptide\n",
      "working on:  checkpointz\\to_slurm\\rnn_latent64\\300_rnn-128_peptide.ckpt \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s_renaud\\Documents\\GitHub\\MSCSAM_TBD\\main_model\\transvae\\trans_models.py:54: UserWarning:\n",
      "\n",
      "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rnn-128_peptide\n",
      "working on:  checkpointz\\to_slurm\\trans_latent128\\300_trans1x-128_peptide.ckpt \n",
      "\n",
      "trans1x-128_peptide\n",
      "working on:  checkpointz\\to_slurm\\trans_latent32\\300_trans1x-128_peptide.ckpt \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s_renaud\\Documents\\GitHub\\MSCSAM_TBD\\main_model\\transvae\\trans_models.py:54: UserWarning:\n",
      "\n",
      "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trans1x-128_peptide\n",
      "working on:  checkpointz\\to_slurm\\trans_latent64\\300_trans1x-128_peptide.ckpt \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s_renaud\\Documents\\GitHub\\MSCSAM_TBD\\main_model\\transvae\\trans_models.py:54: UserWarning:\n",
      "\n",
      "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trans1x-128_peptide\n",
      "working on:  checkpointz\\to_slurm\\wae_latent128\\300_wae-128_peptide.ckpt \n",
      "\n",
      "WAE class init called /n\n",
      "WAE class build_model called /n\n",
      "wae-128_peptide\n",
      "working on:  checkpointz\\to_slurm\\wae_latent32\\300_wae-128_peptide.ckpt \n",
      "\n",
      "WAE class init called /n\n",
      "WAE class build_model called /n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s_renaud\\Documents\\GitHub\\MSCSAM_TBD\\main_model\\transvae\\trans_models.py:54: UserWarning:\n",
      "\n",
      "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wae-128_peptide\n",
      "working on:  checkpointz\\to_slurm\\wae_latent64\\300_wae-128_peptide.ckpt \n",
      "\n",
      "WAE class init called /n\n",
      "WAE class build_model called /n\n",
      "wae-128_peptide\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s_renaud\\Documents\\GitHub\\MSCSAM_TBD\\main_model\\transvae\\trans_models.py:54: UserWarning:\n",
      "\n",
      "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "from transvae import trans_models\n",
    "from transvae.transformer_models import TransVAE\n",
    "from transvae.rnn_models import RNN, RNNAttn\n",
    "from transvae.wae_models import WAE\n",
    "from transvae.aae_models import AAE\n",
    "from transvae.tvae_util import *\n",
    "from transvae import analysis\n",
    "import glob\n",
    "import re\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import Isomap\n",
    "from sklearn import metrics\n",
    "from sklearn.manifold import trustworthiness\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import NullFormatter\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import coranking #coranking.readthedocs.io\n",
    "from coranking.metrics import trustworthiness, continuity, LCMC\n",
    "from transvae.snc import SNC #github.com/hj-n/steadiness-cohesiveness\n",
    "\n",
    "import Bio\n",
    "from Bio import pairwise2\n",
    "from Bio.Align import substitution_matrices\n",
    "\n",
    "def loss_plots(loss_src):\n",
    "    tot_loss = analysis.plot_loss_by_type(src,loss_types=['tot_loss'])\n",
    "    plt.savefig(save_dir+'tot_loss.png')\n",
    "    recon_loss = analysis.plot_loss_by_type(src,loss_types=['recon_loss'])\n",
    "    plt.savefig(save_dir+'recon_loss.png')\n",
    "    kld_loss = analysis.plot_loss_by_type(src,loss_types=['kld_loss'])\n",
    "    plt.savefig(save_dir+'kld_loss.png')\n",
    "    prob_bce_loss = analysis.plot_loss_by_type(src,loss_types=['prop_bce_loss'])\n",
    "    plt.savefig(save_dir+'prob_bce_loss.png')\n",
    "    if 'aae' in src:\n",
    "        disc_loss = analysis.plot_loss_by_type(src,loss_types=['disc_loss'])\n",
    "        plt.savefig(save_dir+'disc_loss.png')\n",
    "    if 'wae' in src:\n",
    "        mmd_loss = analysis.plot_loss_by_type(src,loss_types=['mmd_loss'])\n",
    "        plt.savefig(save_dir+'mmd_loss.png')\n",
    "    plt.close('all')\n",
    "    \n",
    "def load_reconstructions(data,data_1D,latent_size, load_src, true_props=None,subset=None):\n",
    "    \n",
    "    recon_src = load_src+model.name+\"_\"+re.split('(\\d{2,3})',latent_size[0])[0]+\"_\"+re.split('(\\d{2,3})',latent_size[0])[1]+\"//saved_info.csv\"\n",
    "    recon_df = pd.read_csv(recon_src)\n",
    "    reconstructed_seq = recon_df['reconstructions'].to_list()[:num_sequences]\n",
    "    props = torch.Tensor(recon_df['predicted properties'][:num_sequences])\n",
    "    true_props_data = pd.read_csv(true_props).to_numpy()\n",
    "    true_props = true_props_data[0:num_sequences,0]\n",
    "    \n",
    "    if subset:\n",
    "        testing = pd.read_csv(subset).to_numpy()\n",
    "        test_idx_list = [np.where(data==testing[idx][0]) for idx in range(len(testing))]\n",
    "\n",
    "\n",
    "        batch_recon_len = len(reconstructed_seq)\n",
    "        reconstructed_seq = [reconstructed_seq[test_idx_list[i][0][0]] for i in range(len(test_idx_list)) if test_idx_list[i][0][0]<batch_recon_len]\n",
    "        data_1D= [data_1D[test_idx_list[i][0][0]] for i in range(len(test_idx_list)) if test_idx_list[i][0][0]<batch_recon_len]\n",
    "        props = [props[test_idx_list[i][0][0]] for i in range(len(test_idx_list)) if test_idx_list[i][0][0]<batch_recon_len]\n",
    "        props=torch.Tensor(props)\n",
    "        data = testing[:][0]\n",
    "        true_props_data = pd.read_csv(true_props).to_numpy()\n",
    "        true_props = true_props_data[0:num_sequences,0]\n",
    "        true_props= [true_props[test_idx_list[i][0][0]] for i in range(len(test_idx_list)) if test_idx_list[i][0][0]<batch_recon_len]\n",
    "\n",
    "    return data, data_1D, true_props, props, reconstructed_seq\n",
    "\n",
    "########################################################################################\n",
    "gpu = True\n",
    "\n",
    "num_sequences = 500#_000\n",
    "batch_size = 200 #setting for reconstruction\n",
    "example_data = 'data\\\\peptides\\\\datasets\\\\uniprot_v2\\\\peptide_train.txt'\n",
    "save_dir_loc = 'model_analyses\\\\train\\\\' #folder in which to save outpts\n",
    "save_dir_name = 'train' #appended to identify data: train|test|other|etc...\n",
    "\n",
    "reconstruct=True #True:reconstruct data here; False:load reconstructions from file\n",
    "recon_src = \"checkpointz//analyses_ckpts//\" #directory in which all reconstructions are stored\n",
    "true_prop_src = 'data\\\\peptides\\\\datasets\\\\uniprot_v2\\\\function_train.txt' #if property predictor load the true labels\n",
    "subset_src = \"\" #(optional) this file should have the true sequences for a subset of the \"example data\" above\n",
    "\n",
    "ckpt_list = glob.glob(\"\"+\"checkpointz\\\\to_slurm//**//*.ckpt\", recursive = True) #grab all checkpoint\n",
    "print('current working directory: ',os.getcwd())\n",
    "\n",
    "\n",
    "for i in range(len(ckpt_list)):\n",
    "    \n",
    "    #search the current directory for the model name and load that model\n",
    "    model_dic = {'trans':'TransVAE','aae':'AAE','rnnattn':'RNNAttn','rnn':'RNN','wae':'WAE'}\n",
    "    model_src = ckpt_list[i]\n",
    "    print('working on: ',model_src,'\\n')\n",
    "    model_name = list(filter(None,[key for key in model_dic.keys() if key in model_src.split('\\\\')[-1]]))\n",
    "    model = locals()[model_dic[model_name[0]]](load_fn=model_src) #use locals to call model specific constructor\n",
    "    \n",
    "    #create save directory for the current model according to latent space size\n",
    "    latent_size = re.findall('(latent[\\d]{2,3})', model_src)\n",
    "    save_dir= save_dir_loc+model.name+\"_\"+latent_size[0]+\"_\"+save_dir_name\n",
    "    if not os.path.exists(save_dir):os.mkdir(save_dir) \n",
    "    save_dir= save_dir+\"//\" \n",
    "    save_df = pd.DataFrame() #this will hold the number variables and save to CSV\n",
    "    \n",
    "    #load the true labels\n",
    "    data = pd.read_csv(example_data).to_numpy() \n",
    "    data_1D = data[:num_sequences,0] #gets rid of extra dimension\n",
    "    true_props_data = pd.read_csv(true_prop_src).to_numpy()\n",
    "    true_props = true_props_data[0:num_sequences,0]\n",
    "\n",
    "    \n",
    "#     #get the log.txt file from the ckpt and model name then plot loss curves\n",
    "#     loss_src = '_'.join( (\"log\",model_src.split('\\\\')[-1].split('_')[1],model_src.split('\\\\')[-1].split('_')[2][:-4]+\"txt\") )\n",
    "#     src= '\\\\'.join([str(i) for i in model_src.split('\\\\')[:-1]])+\"\\\\\"+loss_src\n",
    "#     print(loss_src, src)\n",
    "#     loss_plots(src)\n",
    "    \n",
    "#     #set the batch size and reconstruct the data\n",
    "#     model.params['BATCH_SIZE'] = batch_size\n",
    "#     if reconstruct:\n",
    "#         reconstructed_seq, props = model.reconstruct(data[:num_sequences], log=False, return_mems=False)\n",
    "#     else:\n",
    "#         data, data_1D, true_props, props, reconstructed_seq = load_reconstructions(data, data_1D,latent_size,\n",
    "#                                                                                    load_src=recon_src,\n",
    "#                                                                                    true_props=true_prop_src)\n",
    "#     if gpu:torch.cuda.empty_cache() #free allocated CUDA memory\n",
    "    \n",
    "#     #save the metrics to the dataframe\n",
    "#     save_df['reconstructions'] = reconstructed_seq #placing the saves on a line separate from the ops allows for editing\n",
    "#     save_df['predicted properties'] = [prop.item() for prop in props[:len(reconstructed_seq)]]\n",
    "#     prop_acc, prop_conf, MCC=calc_property_accuracies(props[:len(reconstructed_seq)],true_props[:len(reconstructed_seq)], MCC=True)\n",
    "#     save_df['property prediction accuracy'] = prop_acc\n",
    "#     save_df['property prediction confidence'] = prop_conf\n",
    "#     save_df['MCC'] = MCC\n",
    "    \n",
    "\n",
    "# #   First we tokenize the input and reconstructed smiles\n",
    "#     input_sequences = []\n",
    "#     for seq in data_1D:\n",
    "#         input_sequences.append(peptide_tokenizer(seq))\n",
    "#     output_sequences = []\n",
    "#     for seq in reconstructed_seq:\n",
    "#         output_sequences.append(peptide_tokenizer(seq))\n",
    "    \n",
    "#     seq_accs, tok_accs, pos_accs, seq_conf, tok_conf, pos_conf = calc_reconstruction_accuracies(input_sequences, output_sequences)\n",
    "#     save_df['sequence accuracy'] = seq_accs\n",
    "#     save_df['sequence confidence'] = seq_conf\n",
    "#     save_df['token accuracy'] = tok_accs\n",
    "#     save_df['token confidence'] = tok_conf\n",
    "#     save_df = pd.concat([pd.DataFrame({'position_accs':pos_accs,'position_confidence':pos_conf }), save_df], axis=1)\n",
    "    \n",
    "    ##moving into memory and entropy\n",
    "    if model.model_type =='aae':\n",
    "        mus, _, _ = model.calc_mems(data[:50_000], log=False, save=False) \n",
    "    elif model.model_type == 'wae':\n",
    "        mus, _, _ = model.calc_mems(data[:50_000], log=False, save=False) \n",
    "    else:\n",
    "        mems, mus, logvars = model.calc_mems(data[:50_000], log=False, save=False) #subset size 1200*35=42000 would be ok\n",
    "\n",
    "#     ##calculate the entropies\n",
    "#     vae_entropy_mus = calc_entropy(mus)\n",
    "#     save_df = pd.concat([save_df,pd.DataFrame({'mu_entropies':vae_entropy_mus})], axis=1)\n",
    "#     if model.model_type != 'wae' and model.model_type!= 'aae': #these don't have a variational type bottleneck\n",
    "#         vae_entropy_mems  = calc_entropy(mems)\n",
    "#         save_df = pd.concat([save_df,pd.DataFrame({'mem_entropies':vae_entropy_mems})], axis=1)\n",
    "#         vae_entropy_logvars = calc_entropy(logvars)\n",
    "#         save_df = pd.concat([save_df,pd.DataFrame({'logvar_entropies':vae_entropy_logvars})], axis=1)\n",
    "    \n",
    "\n",
    "\n",
    "    #create random index and re-index ordered memory list creating n random sub-lists (ideally resulting in IID random lists)\n",
    "    random_idx = np.random.permutation(np.arange(stop=mus.shape[0]))\n",
    "    mus = mus[random_idx]\n",
    "    data = data[random_idx]\n",
    "\n",
    "    subsample_start=0\n",
    "    subsample_length=mus.shape[0] #this may change depending on batch size\n",
    "\n",
    "    #(for length based coloring): record all peptide lengths iterating through input\n",
    "    pep_lengths = []\n",
    "    for idx, pep in enumerate(data[subsample_start:(subsample_start+subsample_length)]):\n",
    "        pep_lengths.append( len(pep[0]) )   \n",
    "    #(for function based coloring): pull function from csv with peptide functions\n",
    "    s_to_f =pd.read_csv(true_prop_src)    \n",
    "    function = s_to_f['peptides'][subsample_start:(subsample_start+subsample_length)]\n",
    "    function = function[random_idx] #account for random permutation\n",
    "\n",
    "    pca = PCA(n_components=5)\n",
    "    pca_batch =pca.fit_transform(X=mus[:])\n",
    "\n",
    "    #plot format dictionnaries\n",
    "    titles={'text':'{}'.format(model.model_type.replace(\"_\",\" \").upper()),\n",
    "                          'x':0.5,'xanchor':'center','yanchor':'top','font_size':40}\n",
    "    general_fonts={'family':\"Helvetica\",'size':30,'color':\"Black\"}\n",
    "    colorbar_fmt={'title_font_size':30,'thickness':15,'ticks':'','title_text':'Lengths',\n",
    "                               'ticklabelposition':\"outside bottom\"}\n",
    "    \n",
    "    fig = px.scatter(pd.DataFrame({\"PC1\":pca_batch[:,0],\"PC2\":pca_batch[:,1], \"lengths\":pep_lengths}),\n",
    "                symbol_sequence=['hexagon2'],x='PC1', y='PC2', color=\"lengths\",\n",
    "                color_continuous_scale='Jet',template='simple_white', opacity=0.9)\n",
    "    fig.update_traces(marker=dict(size=9))\n",
    "    fig.update_layout(title=titles,xaxis_title=\"PC1\", yaxis_title=\"PC2\",font=general_fonts)\n",
    "    fig.update_coloraxes(colorbar=colorbar_fmt)\n",
    "    fig.write_image(save_dir+'pca_length.png', width=900, height=600)\n",
    "\n",
    "    fig = px.scatter(pd.DataFrame({\"PC1\":pca_batch[:,0],\"PC2\":pca_batch[:,1], \n",
    "                                    \"Function\":list(map(lambda itm: \"AMP\" if itm==1 else \"NON-AMP\",function))}),\n",
    "                                    x='PC1', y='PC2', color=\"Function\",symbol_sequence=['x-thin-open','circle'],\n",
    "                                    template='simple_white',symbol='Function', opacity=0.8) \n",
    "    fig.update_traces(marker=dict(size=9))\n",
    "    fig.update_layout(title=titles,xaxis_title=\"PC1\",yaxis_title=\"PC2\",font=general_fonts)\n",
    "    fig.write_image(save_dir+'pca_function.png', width=900, height=600)\n",
    "    \n",
    "    # Plot the explained variances\n",
    "    plt.bar(range(pca.n_components_), pca.explained_variance_ratio_*100, color='black')\n",
    "    plt.xlabel('PCA features')\n",
    "    plt.ylabel('variance %')\n",
    "    plt.xticks(range(pca.n_components_))\n",
    "    plt.savefig(save_dir+'variance_explained.png')\n",
    "\n",
    "    fig = px.scatter_matrix(pd.DataFrame({\"PC1\":pca_batch[:,0],\"PC2\":pca_batch[:,1],\"PC3\":pca_batch[:,2],\n",
    "                                    \"lengths\":pep_lengths}), dimensions=[\"PC1\",\"PC2\",\"PC3\"],\n",
    "                                    symbol_sequence=['hexagon2'],template='simple_white',\n",
    "                                    color=\"lengths\",color_continuous_scale='Jet', opacity=0.9)\n",
    "    fig.update_traces(diagonal_visible=False)\n",
    "    fig.update_layout(title=titles,xaxis_title=\"PC1\", yaxis_title=\"PC2\",font=general_fonts)\n",
    "    fig.write_image(save_dir+'pca_matrix_length.png', width=1920, height=1080) \n",
    "    \n",
    "    fig = px.scatter_matrix(pd.DataFrame({\"PC1\":pca_batch[:,0],\"PC2\":pca_batch[:,1],\"PC3\":pca_batch[:,2], \n",
    "                                   \"Function\":list(map(lambda itm: \"AMP\" if itm==1 else \"NON-AMP\",function))}),\n",
    "                                    dimensions=[\"PC1\",\"PC2\",\"PC3\"],template='simple_white',\n",
    "                                    color=\"Function\",symbol_sequence=['x-thin','circle'],\n",
    "                                    symbol='Function', opacity=0.8) \n",
    "    fig.update_traces(diagonal_visible=False)\n",
    "    fig.update_layout(title=titles,xaxis_title=\"PC1\", yaxis_title=\"PC2\",font=general_fonts)\n",
    "    fig.write_image(save_dir+'pca_matrix_function.png', width=1920, height=1080) \n",
    "\n",
    "    #create n subsamples and calculate silhouette score for each\n",
    "    latent_mem_func_subsamples = []\n",
    "    pca_func_subsamples = []\n",
    "    n=15\n",
    "    for s in range(n):\n",
    "        s_len = len(mus)//n #sample lengths\n",
    "        mem_func_sil = metrics.silhouette_score(mus[s_len*s:s_len*(s+1)], function[s_len*s:s_len*(s+1)], metric='euclidean')\n",
    "        latent_mem_func_subsamples.append(mem_func_sil)\n",
    "        XY = [i for i in zip(pca_batch[s_len*s:s_len*(s+1),0], pca_batch[s_len*s:s_len*(s+1),1])]\n",
    "        pca_func_sil = metrics.silhouette_score(XY, function[s_len*s:s_len*(s+1)], metric='euclidean')\n",
    "        pca_func_subsamples.append(pca_func_sil)\n",
    "    save_df = pd.concat([save_df,pd.DataFrame({'latent_mem_func_silhouette':latent_mem_func_subsamples})], axis=1)\n",
    "    save_df = pd.concat([save_df,pd.DataFrame({'pca_func_silhouette':pca_func_subsamples})], axis=1)\n",
    "\n",
    "#     save_df.to_csv(save_dir+\"saved_info.csv\", index=False)\n",
    "    \n",
    "# #New section dealing with sequence generation metrics and bootstrapping from the latent space\n",
    "#     #first randomly sample points within the latents space\n",
    "#     rnd_seq_count = 1_000\n",
    "#     rnd_latent_list=[] #generate N latent space vectors\n",
    "#     mem_min = np.min(mus)\n",
    "#     mem_max = np.max(mus)\n",
    "#     for seq in range(rnd_seq_count):\n",
    "#         rnd_latent_list.append( np.array([random.uniform(mem_min,mem_max) for i in range(model.params['d_latent'])]).astype(np.float32) )\n",
    "    \n",
    "#     model.params['BATCH_SIZE'] = 25\n",
    "#     rnd_token_list=np.empty((rnd_seq_count,model.tgt_len)) #store N decoded latent vectors now in token(0-20) form max length 125\n",
    "    \n",
    "#     #decode these points into predicted amino acid tokens (integers)\n",
    "#     for batch in range(0,rnd_seq_count,model.params['BATCH_SIZE']):\n",
    "#         rnd_token_list[batch:batch+model.params['BATCH_SIZE']] =  model.greedy_decode(torch.tensor(rnd_latent_list[batch:batch+model.params['BATCH_SIZE']]).cuda()).cpu()\n",
    "    \n",
    "#     #turn the tokens into characters\n",
    "#     decoded_rnd_seqs = decode_mols(torch.tensor(rnd_token_list), model.params['ORG_DICT'])\n",
    "#     decoded_rnd_seqs[:]=[x for x in decoded_rnd_seqs if x] #removes the empty lists\n",
    "#     z=1.96 #95% confidence interval\n",
    "#     percent_unique = len(set(decoded_rnd_seqs)) / rnd_seq_count\n",
    "#     unique_conf = z*math.sqrt(percent_unique*(1-percent_unique)/rnd_seq_count)\n",
    "#     percent_unique, unique_conf\n",
    "    \n",
    "#     df_gen_scores = {}\n",
    "#     df_gen_scores.update({'percent_unique': percent_unique})\n",
    "#     df_gen_scores.update({'unique_confidence':unique_conf})\n",
    "    \n",
    "#     #sample N test/train set sequences randomly and compare to those created\n",
    "#     shuffled_test = random.sample(data.tolist(),len(data))\n",
    "#     shuffled_test = np.array(shuffled_test[:len(decoded_rnd_seqs)])\n",
    "#     combined = np.concatenate(( shuffled_test,np.array(decoded_rnd_seqs).reshape(len(decoded_rnd_seqs),1)) )\n",
    "#     percent_novel = (len(set(combined.flatten().tolist()))-len(shuffled_test))/(rnd_seq_count)\n",
    "#     novel_conf =  z*math.sqrt(percent_novel*(1-percent_novel)/(2*len(decoded_rnd_seqs)))\n",
    "#     percent_novel, novel_conf\n",
    "#     df_gen_scores.update({'percent_novel':percent_novel})\n",
    "#     df_gen_scores.update({'novel_confidence':novel_conf})\n",
    "    \n",
    "#     shuffled_test = shuffled_test.flatten().tolist()\n",
    "#     similarity_score=[]\n",
    "#     matrix = substitution_matrices.load(\"BLOSUM62\")\n",
    "#     for seq in shuffled_test[:10_000:100]: #grab 100 test set peptides\n",
    "#         for seq2 in decoded_rnd_seqs[::10]: #grab 100 of the 1000 random latent peptides\n",
    "#             similarity_score.append( pairwise2.align.globaldx(seq,seq2, matrix, score_only=True)/(len(seq)+len(seq2)) )\n",
    "            \n",
    "  \n",
    "#     df_gen_scores.update({'average_sequence_similarity': np.average(similarity_score)})\n",
    "#     df_gen_scores.update({'std_on_similarity_score': np.std(similarity_score)})\n",
    "    \n",
    "#     #GLFDIWKKWRWRR is an AMP in the test set. Use it as a reference point in the latent space\n",
    "#     #GLIDTVKNMAINAAKSAGMSVLKTLSCKLSKEC is an amp in the training set also can be used as a reference pt in latent space\n",
    "#     model.params['BATCH_SIZE'] = 1\n",
    "#     if model.model_type =='aae':\n",
    "#         mus, _, _ = model.calc_mems(np.array([['GLIDTVKNMAINAAKSAGMSVLKTLSCKLSKEC']], dtype='O'), log=False, save=False) \n",
    "#     elif model.model_type == 'wae':\n",
    "#         mus, _, _ = model.calc_mems(np.array([['GLIDTVKNMAINAAKSAGMSVLKTLSCKLSKEC']], dtype='O'), log=False, save=False) \n",
    "#     else:\n",
    "#         mems, mus, logvars = model.calc_mems(np.array([['GLIDTVKNMAINAAKSAGMSVLKTLSCKLSKEC']], dtype='O'), log=False, save=False) #subset size 1200*35=42000 would be ok\n",
    "    \n",
    "    if model.model_type =='aae' or model.model_type =='wae':\n",
    "        nearby_samples = np.random.normal(loc=0,scale=1,size=(500,1,model.params['d_latent'])).astype(np.float32)*0.4 + mus\n",
    "    else:\n",
    "        nearby_samples = np.random.normal(loc=0,scale=1,size=(500,1,model.params['d_latent'])).astype(np.float32)*np.exp(0.5*logvars) + mus\n",
    "    \n",
    "#     rnd_seq_count=500\n",
    "#     model.params['BATCH_SIZE'] = 50\n",
    "#     rnd_token_list=np.empty((rnd_seq_count,model.tgt_len)) #store N decoded latent vectors now in token(0-20) form max length 125\n",
    "#     for batch in range(0,rnd_seq_count,model.params['BATCH_SIZE']):\n",
    "#         rnd_token_list[batch:batch+model.params['BATCH_SIZE']] =  model.greedy_decode(torch.tensor(nearby_samples[batch:batch+model.params['BATCH_SIZE']]).squeeze().cuda()).cpu()\n",
    "    \n",
    "#     decoded_rnd_seqs = decode_mols(torch.tensor(rnd_token_list), model.params['ORG_DICT'])                                             \n",
    "             \n",
    "#     z=1.96 #95% confidence interval\n",
    "#     amp_percent_unique = len(set(decoded_rnd_seqs))/len(decoded_rnd_seqs)\n",
    "#     amp_unique_conf = z*math.sqrt(amp_percent_unique*(1-amp_percent_unique)/rnd_seq_count)\n",
    "#     df_gen_scores.update({'amp_uniqueness': amp_percent_unique})\n",
    "#     df_gen_scores.update({'amp_uniqueness_std': amp_unique_conf})\n",
    "                                                \n",
    "#     k=2\n",
    "#     jac_scores = np.empty((len(list(set(decoded_rnd_seqs))),1))\n",
    "#     jac_scores.shape\n",
    "#     for i,decoded_seq in enumerate(list(set(decoded_rnd_seqs))):\n",
    "#         for j,seq in enumerate(np.array(['GLIDTVKNMAINAAKSAGMSVLKTLSCKLSKEC'], dtype='O')):\n",
    "#             jac_scores[i,j] = (jaccard_similarity(build_kmers(seq,k), build_kmers(decoded_seq,k)))\n",
    "#     df_gen_scores.update({'amp_jac_score': np.average(jac_scores)})\n",
    "#     df_gen_scores.update({'amp_jac_score_std': np.std(jac_scores)})\n",
    "#     np.average(jac_scores), np.std(jac_scores) \n",
    "\n",
    "#     model.params['BATCH_SIZE'] = 2\n",
    "#     reconstructed_seq, props = model.reconstruct(np.array([[seq] for seq in list(set(decoded_rnd_seqs))]), log=False, return_mems=False)\n",
    "#     props[props>1]=1 #sometimes the model outputs a probability >1 for a class so threshold\n",
    "#     amp_percent_amp = sum(props.round()).item()/len(props)\n",
    "#     amp_amp_conf = z*math.sqrt(amp_percent_amp*(1-amp_percent_amp)/len(props))\n",
    "#     df_gen_scores.update({'predicted_amps': amp_percent_amp})\n",
    "#     df_gen_scores.update({'predicted_amps_conf': amp_amp_conf})    \n",
    "#     df = pd.DataFrame.from_dict([df_gen_scores])\n",
    "#     pd.DataFrame.from_dict([df_gen_scores]).to_csv(save_dir+\"generation_metrics.csv\", index=False)\n",
    "    \n",
    "#     #GLFDIWKKWRWRR is an AMP in the test set. Use it as a reference point in the latent space\n",
    "#     #GLIDTVKNMAINAAKSAGMSVLKTLSCKLSKEC is an amp in the training set also can be used as a reference pt in latent space\n",
    "#     model.params['BATCH_SIZE'] = 200\n",
    "#     if model.model_type =='aae':\n",
    "#         mus, _, _ = model.calc_mems(data[:20_000], log=False, save=False) \n",
    "#     elif model.model_type == 'wae':\n",
    "#         mus, _, _ = model.calc_mems(data[:20_000], log=False, save=False) \n",
    "#     else:\n",
    "#         mems, mus, logvars = model.calc_mems(data[:20_000], log=False, save=False)\n",
    "        \n",
    "    \n",
    "#     model.params['BATCH_SIZE'] = 1\n",
    "#     subsample_start=0\n",
    "#     subsample_length=mus.shape[0] #this may change depending on batch size\n",
    "\n",
    "#     #(for length based coloring): record all peptide lengths iterating through input\n",
    "#     pep_lengths = []\n",
    "#     for idx, pep in enumerate(data[subsample_start:(subsample_start+subsample_length)]):\n",
    "#         pep_lengths.append( len(pep[0]) )   \n",
    "#     #(for function based coloring): pull function from csv with peptide functions\n",
    "#     s_to_f =pd.read_csv(true_prop_src)    \n",
    "#     function = s_to_f['peptides'][subsample_start:(subsample_start+subsample_length)]\n",
    "\n",
    "#     pca = PCA(n_components=3)\n",
    "#     pca_batch =pca.fit_transform(X=mus[:])\n",
    "#     pca_generated = pca.transform(nearby_samples.squeeze())\n",
    "\n",
    "#     if model.model_type =='aae' or model.model_type =='wae':\n",
    "#         amp_mus, _, _ = model.calc_mems(np.array([['GLIDTVKNMAINAAKSAGMSVLKTLSCKLSKEC']], dtype='O'), log=False, save=False)\n",
    "#     else:\n",
    "#         amp_mems, amp_mus, amp_logvars = model.calc_mems(np.array([['GLIDTVKNMAINAAKSAGMSVLKTLSCKLSKEC']], dtype='O'), log=False, save=False)\n",
    "#     pca_amp = pca.transform(amp_mus)\n",
    "\n",
    "#     #plot format dictionnaries\n",
    "#     titles={'text':'{}'.format(model.model_type.replace(\"_\",\" \").upper()),\n",
    "#                           'x':0.5,'xanchor':'center','yanchor':'top','font_size':40}\n",
    "#     general_fonts={'family':\"Helvetica\",'size':30,'color':\"Black\"}\n",
    "#     colorbar_fmt={'title_font_size':30,'thickness':15,'ticks':'','title_text':'Lengths',\n",
    "#                                'ticklabelposition':\"outside bottom\", 'showscale':'False'}\n",
    "\n",
    "#     fig1 = px.scatter(pd.DataFrame({\"PC1\":pca_batch[:,0],\"PC2\":pca_batch[:,1], \n",
    "#                                     \"Function\":list(map(lambda itm: \"AMP\" if itm==1 else \"NON-AMP\",function))}),\n",
    "#                                     x='PC1', y='PC2', color=\"Function\",symbol_sequence=['x-thin-open','circle'],\n",
    "#                                     template='simple_white',symbol='Function', opacity=0.3) \n",
    "#     fig2 = px.scatter(pd.DataFrame({\"PC1\":pca_generated[:,0],\"PC2\":pca_generated[:,1],\n",
    "#                                     'color':[\"Generated\" for i in pca_generated[:,0]],}),\n",
    "#                                     x='PC1', y='PC2',color='color',labels='label',symbol_sequence=['asterisk-open'],\n",
    "#                                     template='simple_white', opacity=0.9)\n",
    "#     fig2.update_traces(marker=dict(color='red'))\n",
    "#     fig3 = px.scatter(pd.DataFrame({\"PC1\":pca_amp[:,0],\"PC2\":pca_amp[:,1], 'color':['source AMP' for i in pca_amp[:,0]]}),\n",
    "#                                     x='PC1', y='PC2',color='color',symbol_sequence=['cross'],\n",
    "#                                     template='simple_white', opacity=1)\n",
    "#     fig3.update_traces(marker=dict(size=12, color='black'))\n",
    "#     fig1.update_traces(marker=dict(size=9))\n",
    "#     fig2.update_traces(marker=dict(size=9))\n",
    "#     fig = go.Figure(data= fig1.data+fig2.data+fig3.data)\n",
    "#     fig.update_coloraxes(showscale=False)\n",
    "#     fig.update_layout(title=titles,xaxis_title=\"PC1\",yaxis_title=\"PC2\",font=general_fonts)\n",
    "#     fig.write_image(save_dir+'amp_sample.png', width=1_000, height=700)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8307e206",
   "metadata": {},
   "source": [
    "<H4>Since Compute Canada does not do the dimensionality reduction metrics we need to do them below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29fc2219",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current working directory:  C:\\Users\\s_renaud\\Documents\\GitHub\\MSCSAM_TBD\\main_model\n",
      "working on:  checkpointz\\to_slurm\\aae_latent128\\300_aae-128_peptide.ckpt \n",
      "\n",
      "analysis:  model_analyses\\train\\aae-128_peptide_latent128_train\\saved_info.csv checkpoint:  checkpointz\\to_slurm\\aae_latent128\\300_aae-128_peptide.ckpt\n",
      "aae-128_peptide\n",
      "0 0.8491426682235053 0.9013040873151008 0.13193220339775047\n",
      "0.8041849918528825 0.6861762311715094\n",
      "1 0.8412760446459416 0.8953758977141204 0.12758644117565618\n",
      "0.7949895979171204 0.6899147477142924\n",
      "2 0.8424101471733513 0.8974639774021076 0.13402334500019236\n",
      "0.8042051599397544 0.69024676797904\n",
      "3 0.8436442620784003 0.8975999753935946 0.13223303294038688\n",
      "0.8098959933994025 0.6871484248418172\n",
      "4 0.845727503861393 0.8976939135690032 0.13454679988254445\n",
      "0.7956154223447032 0.6772885963970339\n",
      "5 0.835827257547975 0.8915914329364764 0.12413643138245187\n",
      "0.7990545229109078 0.6658793601302204\n",
      "6 0.8495625307136195 0.8993038673376365 0.1310893121463485\n",
      "0.8046380511643147 0.680481357224492\n",
      "7 0.8437597854765683 0.8978268569027956 0.12782252489895501\n",
      "0.8205478316562341 0.6545547017165055\n",
      "8 0.8418960448462897 0.8957521811880031 0.13131630612375272\n",
      "0.7954082671628387 0.6761155140694484\n",
      "9 0.8436130529070018 0.8985806137752466 0.12835467117014637\n",
      "0.8011104503673355 0.6821796843993722\n",
      "10 0.8426408147823237 0.8969797241290706 0.12569266615174923\n",
      "0.8039519985328016 0.6667210155992649\n",
      "11 0.8446179897439368 0.8968950653369403 0.1309551406210652\n",
      "0.7919620101028806 0.6922482674996028\n",
      "12 0.840254554994526 0.8918541632249236 0.12790963077552212\n",
      "0.8068272647541793 0.660519316589121\n",
      "13 0.839078994035907 0.893958697773572 0.13119204008486193\n",
      "0.8151797963707871 0.6558702745741165\n",
      "14 0.8484970543465479 0.8998215478027556 0.13404999253962468\n",
      "0.8025146176376734 0.6954150347216366\n",
      "15 0.8439428966373931 0.8950760290664593 0.1291448158768509\n",
      "0.8011504767567214 0.6883986697538745\n",
      "16 0.8451602913530386 0.8991925221035076 0.13447681607617995\n",
      "0.8076851193778768 0.6955327069097894\n",
      "17 0.8432413288392381 0.8967683177301575 0.13564004162992777\n",
      "0.8009672961017686 0.6702642092650846\n",
      "18 0.8452873923690197 0.8971370022764026 0.13133930257856105\n",
      "0.8121277849511368 0.6679273434432089\n",
      "19 0.8382017873597477 0.8949289700606993 0.12497774909255503\n",
      "0.832956419523206 0.6297948243795761\n",
      "20 0.8456352687904265 0.8967540919538711 0.12851962686686227\n",
      "0.8064812934293246 0.6712753898503623\n",
      "21 0.8384052120286516 0.8952876870100843 0.13001735954669671\n",
      "0.7975049160162528 0.6740733633816576\n",
      "22 0.844391213215027 0.8991361109797658 0.13477043853759016\n",
      "0.8067397724948732 0.6726690740327637\n",
      "23 0.8437326544772866 0.8962778038255345 0.12949425465685563\n",
      "0.7975270038662771 0.6913153758707\n",
      "24 0.8337756195636339 0.8878859234173054 0.12395218093248117\n",
      "0.7845489089372736 0.6818398886016057\n",
      "25 0.8465137006986324 0.899746631573067 0.13425391852569282\n",
      "0.8064368193266747 0.673814972765413\n",
      "26 0.8418775373547907 0.8961484028744375 0.132305438518415\n",
      "0.810634982785225 0.6657842232877825\n",
      "27 0.8364286016365046 0.8934489589918579 0.12807944192838308\n",
      "0.7975064465236341 0.6707019259404199\n",
      "28 0.8421154990983031 0.896019507725669 0.13334347930855292\n",
      "0.8029330451827416 0.6675112073156834\n",
      "29 0.846737078727677 0.8991460164380286 0.1335292490790919\n",
      "0.8073584125683675 0.697607569651447\n",
      "30 0.8405561931201676 0.8952067211244186 0.12490054602826427\n",
      "0.8093329671739478 0.6456204631852577\n",
      "31 0.8392879158529097 0.8942884115032391 0.11931855491575638\n",
      "0.8110033815364555 0.6583344910516733\n",
      "32 0.845654163125488 0.8979299260045843 0.13581523291527842\n",
      "0.8047670134365066 0.6757700943556519\n",
      "33 0.8320031695448434 0.88803912172398 0.11754875167864598\n",
      "0.8055987936742638 0.6458947653572544\n",
      "34 0.8412821802953863 0.8953937210005649 0.126661461270698\n",
      "0.8105991941021764 0.6660241682776384\n",
      "35 0.8462049834124039 0.9003185726282046 0.13442004956261688\n",
      "0.798420925221367 0.6866663933280511\n",
      "36 0.8416959573437809 0.8962987899508267 0.13137590131016097\n",
      "0.810317248114247 0.6557393835684502\n",
      "37 0.8417600907733995 0.8956383191425725 0.13377829860689316\n",
      "0.796780569276974 0.6826706170502554\n",
      "38 0.8423297782213083 0.8962862094267697 0.12627231047711557\n",
      "0.8056697341658727 0.6741318055052161\n",
      "39 0.841051927744228 0.8950698746677322 0.12394125325997495\n",
      "0.797025673459171 0.6649488155716259\n",
      "40 0.8378920837267047 0.8924881852256825 0.12387597998420738\n",
      "0.810586604208089 0.6574406352416611\n",
      "41 0.8447519324912152 0.8955791414452419 0.1272081979653839\n",
      "0.8093799377459467 0.6585691223928501\n",
      "42 0.8360272436432415 0.891573150119155 0.12407536695026983\n",
      "0.8165062609393605 0.652147616961468\n",
      "43 0.8397516979096955 0.8933716954223964 0.12466736040302209\n",
      "0.7986196090563744 0.6833743652422916\n",
      "44 0.8404021737971332 0.8959477449496013 0.12900600786714694\n",
      "0.7969949757282657 0.6911741505291948\n",
      "45 0.8422665873192607 0.8941422439517016 0.1304441181447777\n",
      "0.7989820940491079 0.6855681526343896\n",
      "46 0.8491381227231084 0.8999756307442824 0.13273933262982984\n",
      "0.8279067395315624 0.6587236163739216\n",
      "47 0.846398740969598 0.8944429749418864 0.12658837226413217\n",
      "0.8229174069494256 0.6503276817732841\n",
      "48 0.8425123800821132 0.8984616128589262 0.12669413015425882\n",
      "0.8175056821411312 0.6529814770618501\n",
      "49 0.840754824095467 0.8969385333788646 0.1316080606992112\n",
      "0.8184905015458709 0.6661014636918023\n",
      "50 0.8469562953928234 0.8986252495740635 0.1324352132278283\n",
      "0.8227410149905138 0.6525860135360453\n",
      "51 0.8433288341370657 0.8982849113728326 0.1264204311991956\n",
      "0.8092678099925219 0.6583778761716237\n",
      "52 0.8422748539032877 0.8970351270496859 0.12904532074610217\n",
      "0.8091793437114161 0.6693560968007806\n",
      "53 0.8459626094147491 0.8998673140778162 0.12876051643812117\n",
      "0.8210933048615672 0.6650445025157323\n",
      "54 0.8422239186355294 0.8947707577716113 0.12830352524723393\n",
      "0.8061323425762877 0.6664600927480755\n",
      "55 0.8435348766731171 0.8948689763222701 0.1265908046802757\n",
      "0.8170962246043902 0.6726787243980747\n",
      "56 0.8420394535272853 0.896381840738683 0.12566145874101795\n",
      "0.8144251724690237 0.6678001609541818\n",
      "57 0.8436990572387524 0.8966882749950875 0.12478865075093366\n",
      "0.8162611046568069 0.6569403471546863\n",
      "58 0.8436759527399932 0.8961690671788607 0.1280617670440612\n",
      "0.8226868247255795 0.6318470870780855\n",
      "59 0.8431603720086718 0.8964083349201183 0.126634886873026\n",
      "0.7898210350257631 0.7090533822146949\n",
      "working on:  checkpointz\\to_slurm\\aae_latent32\\300_aae-128_peptide.ckpt \n",
      "\n",
      "analysis:  model_analyses\\train\\aae-128_peptide_latent32_train\\saved_info.csv checkpoint:  checkpointz\\to_slurm\\aae_latent32\\300_aae-128_peptide.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s_renaud\\Documents\\GitHub\\MSCSAM_TBD\\main_model\\transvae\\trans_models.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.params['CHAR_WEIGHTS'] = torch.tensor(self.params['CHAR_WEIGHTS'], dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aae-128_peptide\n",
      "0 0.7739448568041946 0.8554904604917709 0.10462115711733572\n",
      "0.7102195780349501 0.7000090440621665\n",
      "1 0.7804774439614277 0.8625428165127881 0.1031477330183998\n",
      "0.7341246521616254 0.6862074658593078\n",
      "2 0.7670967970057474 0.8530382017487 0.10087597932389525\n",
      "0.7200712892406417 0.6964792830510876\n",
      "3 0.7714408113510282 0.8546135405404744 0.10212324929319704\n",
      "0.7199940415496953 0.7030185611528258\n",
      "4 0.7746987452179214 0.8588298020018059 0.1045004371459804\n",
      "0.7240380222495194 0.683614760978804\n",
      "5 0.7683534259948832 0.8518512898720801 0.09813941415573921\n",
      "0.7268007455271566 0.662241469169762\n",
      "6 0.7668915430958654 0.8528940827878035 0.09675200665492538\n",
      "0.7351563384290901 0.6707861852025183\n",
      "7 0.7653218372272482 0.8483662106101691 0.09447848984693391\n",
      "0.7138569957029539 0.6998121292069781\n",
      "8 0.7698836492090545 0.8555777701112759 0.09796863885752666\n",
      "0.7309041949828067 0.6697626666428046\n",
      "9 0.7693899549289174 0.856695949381585 0.10292441830727853\n",
      "0.7229517474817528 0.678483431156947\n",
      "10 0.7701118810109576 0.8551126902852517 0.09539140707865763\n",
      "0.7321316591232278 0.6554142283347936\n",
      "11 0.7666970309507316 0.8517749360545548 0.09944512791849032\n",
      "0.7239497188561854 0.6799998154111722\n",
      "12 0.7653560114596243 0.850765224745595 0.09651731124608764\n",
      "0.7081056318430685 0.6917308222513237\n",
      "13 0.7729387608608921 0.8543060122424214 0.10060423975983246\n",
      "0.7256982806348191 0.6742011631941379\n",
      "14 0.7685929121356461 0.8536569056035805 0.09909294245477385\n",
      "0.734502181817044 0.6778955123179052\n",
      "15 0.7727950021826758 0.8542626024016049 0.10452685799859025\n",
      "0.7181029963481287 0.6855000188830969\n",
      "16 0.7636200723143137 0.8498314447773468 0.0938663907256974\n",
      "0.7329127789546048 0.6659572412676565\n",
      "17 0.765304777991489 0.8492760182576035 0.09741058657703572\n",
      "0.7109120464550361 0.6740095805900397\n",
      "18 0.7667798634692246 0.8511145198288516 0.0973090835904843\n",
      "0.7217891617285896 0.6747606644400819\n",
      "19 0.7633753816426014 0.8466524844465133 0.09490430655362053\n",
      "0.7336907410483957 0.6704237044292229\n",
      "20 0.7775786547186239 0.854832857269578 0.10602730472663488\n",
      "0.7176263501912545 0.6956732466868483\n",
      "21 0.7679770807468216 0.8508623668017746 0.1018540334146961\n",
      "0.7183009336442991 0.6644427473246501\n",
      "22 0.7768217833409554 0.8582609839473127 0.10445201494949727\n",
      "0.7411317077663047 0.6597279816766637\n",
      "23 0.7696045947550756 0.8535018423501788 0.10327894564486843\n",
      "0.7232284554505481 0.6849942855069435\n",
      "24 0.7633162674017995 0.848979895983981 0.10016055217323354\n",
      "0.7192150728397506 0.6702728434411345\n",
      "25 0.7670784225937852 0.8533016247824274 0.09655714351367577\n",
      "0.7331731003093529 0.6864156785547704\n",
      "26 0.7706421633998238 0.85554428209439 0.09926907538761903\n",
      "0.7428800784259213 0.6637429366880873\n",
      "27 0.765902524443267 0.8515545636284378 0.0991958859234566\n",
      "0.7226046860218245 0.6894665804128819\n",
      "28 0.7688244616222311 0.8542902198643744 0.10253917784692408\n",
      "0.7234604516987339 0.6796121074572821\n",
      "29 0.7699221836912591 0.8528280388308276 0.09760999259902363\n",
      "0.7267797760705632 0.677865600410821\n",
      "30 0.7779837264259712 0.8610950826430454 0.10149940195085662\n",
      "0.7413262057976291 0.6709263572838511\n",
      "31 0.7633981469158971 0.8492278883139802 0.0985431195658598\n",
      "0.7162599581478188 0.6812493108684434\n",
      "32 0.7567301581467853 0.8484469803842251 0.093829993537742\n",
      "0.7218675061787053 0.6807303996397689\n",
      "33 0.764526600618722 0.8513480759407923 0.09546190532331794\n",
      "0.7377126245291636 0.6677622700271262\n",
      "34 0.7571802429902984 0.8444693543470292 0.09260610759211775\n",
      "0.7165845055548612 0.6712721563952799\n",
      "35 0.7641036102865353 0.8512410581922291 0.09959716768905445\n",
      "0.7353461363176453 0.6663821776328641\n",
      "36 0.765122564785915 0.8489241260606429 0.09457727976479365\n",
      "0.7292579470503686 0.6871277261396372\n",
      "37 0.7628106626067848 0.8511011788786684 0.09710280655656726\n",
      "0.7222209826924246 0.6695929291866575\n",
      "38 0.766084004927519 0.8518229701333474 0.09871065895178925\n",
      "0.7360919715310338 0.660879357746063\n",
      "39 0.7795469885949117 0.8594757232336846 0.10153762377325114\n",
      "0.746636033164155 0.6672017138505477\n",
      "40 0.752067751114965 0.8421710659960558 0.09666445743477384\n",
      "0.7108785813990028 0.6834172321226453\n",
      "41 0.768801231986769 0.8539311338466352 0.10089916458247737\n",
      "0.7310506052208186 0.6839232222047733\n",
      "42 0.7630120442594814 0.851478805385978 0.09497904883398138\n",
      "0.7309451705610469 0.6811146606651778\n",
      "43 0.7755546657273101 0.8592750699422629 0.0989722600834788\n",
      "0.7397462674952783 0.6679641030708778\n",
      "44 0.7613788836875873 0.8473270448518297 0.09684561051627998\n",
      "0.7161312430374979 0.6987677142456377\n",
      "45 0.7717460007281405 0.8547206080057849 0.0991451471380379\n",
      "0.7259028107017469 0.6728719190647918\n",
      "46 0.7675124745051471 0.8515624485894118 0.09732985474614253\n",
      "0.7220220806655512 0.6672988318841154\n",
      "47 0.7638327865325195 0.8501811040726889 0.10189175365466177\n",
      "0.720983038803326 0.6793258946228355\n",
      "48 0.7621372531744434 0.8494416806516029 0.09651144721075482\n",
      "0.7225516372811149 0.682243405098262\n",
      "49 0.7755370358034834 0.8583744553491338 0.09977843956864627\n",
      "0.7393370272685098 0.6916904165858169\n",
      "50 0.7622318113536173 0.8510030873309731 0.09541841867843721\n",
      "0.7371843356529644 0.6685354077698556\n",
      "51 0.7688173837038311 0.853614133437695 0.10117205309597684\n",
      "0.7321992962609944 0.6584228901211837\n",
      "52 0.7852462658160807 0.862021010173467 0.10390411814518406\n",
      "0.7357388160399977 0.6984696865195431\n",
      "53 0.7662857299599851 0.8529384406664614 0.09485104378295539\n",
      "0.7125310805306577 0.6967414583019527\n",
      "54 0.7661318038703426 0.8527394446447994 0.09575407378675856\n",
      "0.717538047019044 0.6928634095132675\n",
      "55 0.7612214808559224 0.8450063798538228 0.09653110360643535\n",
      "0.6919889942266817 0.7081724463768828\n",
      "56 0.7741683423749949 0.8581563792175121 0.10346290301330788\n",
      "0.7328420515025671 0.6736098576844985\n",
      "57 0.7705477960671367 0.8541208659291702 0.0985294456156388\n",
      "0.7287375447949771 0.6829326478723958\n",
      "58 0.7713406334178685 0.8546942381467549 0.10062475763194265\n",
      "0.722646843729231 0.6837750396547756\n",
      "59 0.764550858863958 0.8510904130188086 0.09593275862913468\n",
      "0.7256623230695027 0.6708826873450182\n",
      "working on:  checkpointz\\to_slurm\\aae_latent64\\300_aae-128_peptide.ckpt \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s_renaud\\Documents\\GitHub\\MSCSAM_TBD\\main_model\\transvae\\trans_models.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.params['CHAR_WEIGHTS'] = torch.tensor(self.params['CHAR_WEIGHTS'], dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analysis:  model_analyses\\train\\aae-128_peptide_latent64_train\\saved_info.csv checkpoint:  checkpointz\\to_slurm\\aae_latent64\\300_aae-128_peptide.ckpt\n",
      "aae-128_peptide\n",
      "0 0.822203487942284 0.8858475033436685 0.1242824001670698\n",
      "0.8021975635657836 0.6581197190573773\n",
      "1 0.8190927973738567 0.881861912375724 0.12364098805450555\n",
      "0.7777062603669556 0.6738649601913931\n",
      "2 0.8215969940510878 0.8833844200826828 0.1204594326197814\n",
      "0.7845883704360648 0.680993025869268\n",
      "3 0.8190258524114408 0.880913337245452 0.11833806032103322\n",
      "0.7938925061593494 0.6513335737851972\n",
      "4 0.8240800677390111 0.885280305737875 0.12065644259609465\n",
      "0.7995590021951173 0.6531563802115337\n",
      "5 0.8252717705646239 0.884845262126699 0.12107359456687868\n",
      "0.7916200578231718 0.6595278189141602\n",
      "6 0.8196240132277314 0.880046882022359 0.11761256903197309\n",
      "0.7832295763962875 0.6547123424977767\n",
      "7 0.8182486149099745 0.8794844036393324 0.11865760940219795\n",
      "0.7698572789172125 0.6719551125893761\n",
      "8 0.8139705297883771 0.8770055445637 0.11865342923105687\n",
      "0.7908481345306696 0.6491282088201062\n",
      "9 0.8175285132901646 0.8796619780122309 0.1170228766216434\n",
      "0.7999352354913515 0.6511933573540036\n",
      "10 0.8165103338089106 0.879287937771516 0.11934847964232424\n",
      "0.7756939782045016 0.6834476996042069\n",
      "11 0.8307356230173204 0.8895018152588393 0.12186993011257133\n",
      "0.7925276791745802 0.6619343345171711\n",
      "12 0.8158651027604559 0.878300040486456 0.11295967584122013\n",
      "0.7618788610501973 0.6830971655202904\n",
      "13 0.8221988315597094 0.8812354022165466 0.11872847099457975\n",
      "0.7881232183249608 0.6606213408845685\n",
      "14 0.8225147988702419 0.8844378230567511 0.11550750668002152\n",
      "0.7734182096156503 0.6914206465809\n",
      "15 0.8146253259169668 0.8793819802030173 0.11734500290132531\n",
      "0.7782324231489919 0.6738069113912737\n",
      "16 0.822728302776566 0.8830447793689089 0.12148048922609546\n",
      "0.7753074528277845 0.6805924953071194\n",
      "17 0.8234098873912737 0.884375907948774 0.12243207257147574\n",
      "0.7767551445448531 0.6784053770905683\n",
      "18 0.8128134868969276 0.8771762758969336 0.11405785286252666\n",
      "0.7723823481406629 0.6738238681354963\n",
      "19 0.8161002954060947 0.8797665695378591 0.11579422312817814\n",
      "0.7807138474305231 0.6636471793151657\n",
      "20 0.8193286758706062 0.8801159736517618 0.11385365495971664\n",
      "0.8079962396320877 0.6332745659534841\n",
      "21 0.824551629609414 0.8837186892349117 0.11636414743843973\n",
      "0.7913622547101393 0.6666149258583594\n",
      "22 0.8231285818421624 0.8816289244725098 0.11857578746913279\n",
      "0.7764699344306956 0.67932082170722\n",
      "23 0.8196432873627928 0.8803479712102334 0.11710875571226302\n",
      "0.7713100939125495 0.6854538880140919\n",
      "24 0.8233070799564526 0.881339821485129 0.11614260459846389\n",
      "0.7894029330825165 0.6551410951943306\n",
      "25 0.820428332397191 0.880167417011705 0.11970793627946481\n",
      "0.7875119798576795 0.664718285685901\n",
      "26 0.8179053199960279 0.8806834937300667 0.11524880523799719\n",
      "0.7745855218317357 0.7034518393214193\n",
      "27 0.816422987673714 0.8778788622349849 0.11811187077641724\n",
      "0.788350117675094 0.6513125885593358\n",
      "28 0.8168144510264748 0.880229461962172 0.11794237289122797\n",
      "0.769244868957018 0.6822607516971801\n",
      "29 0.829237104718493 0.8865129268354137 0.12170697443823392\n",
      "0.7956015131945724 0.6706305811950855\n",
      "30 0.8282242186188274 0.8869486882067552 0.12190275489372719\n",
      "0.7991938204623528 0.6690079029596387\n",
      "31 0.828716783951876 0.8863288519776087 0.12878056373285965\n",
      "0.7845118288894228 0.6731295645274519\n",
      "32 0.8184702924905775 0.8797527575408481 0.11506132589779387\n",
      "0.7913239231200369 0.6652211872836844\n",
      "33 0.8163912062148146 0.8804802670094324 0.11446865124517475\n",
      "0.7991654256837487 0.6493890207901327\n",
      "34 0.8195982111839366 0.8831144149990081 0.11598872699458102\n",
      "0.811273746519571 0.6342849785327391\n",
      "35 0.8175115723606392 0.8805780623063603 0.1186934761948138\n",
      "0.7776586599796074 0.6572784310340982\n",
      "36 0.8221666590720449 0.8826228285474917 0.12399181025886866\n",
      "0.7894770412670179 0.6539989434780045\n",
      "37 0.8216722624040179 0.882205373874498 0.11875045053941752\n",
      "0.7892384450232619 0.6666981898214934\n",
      "38 0.8236614777816034 0.8863511269596662 0.12454369818252847\n",
      "0.802783758928447 0.6671443569752821\n",
      "39 0.8217689441467775 0.8815175059825806 0.12205800476817466\n",
      "0.7864844583308264 0.6685387439136512\n",
      "40 0.8244973214251662 0.8847575954040258 0.11482660024107538\n",
      "0.7862236681279224 0.6691573485730924\n",
      "41 0.8226942625771415 0.8821850833819268 0.11871061419363524\n",
      "0.7824926454636745 0.6610662708732087\n",
      "42 0.8191863415005053 0.8804612281473391 0.12140957414860343\n",
      "0.7936755709245209 0.6559538720328058\n",
      "43 0.8148012251363227 0.8763921775521449 0.1146253897364816\n",
      "0.7827556265448952 0.655878400006328\n",
      "44 0.8217447283385326 0.8827749464349596 0.1187061300564448\n",
      "0.7852260919129979 0.6789683597830565\n",
      "45 0.8248353142125954 0.8818277321222705 0.12192533519613895\n",
      "0.7808284518689875 0.6693571573308099\n",
      "46 0.8250931802007988 0.8823670813190948 0.11987483043622162\n",
      "0.8021788074262286 0.6521145037747477\n",
      "47 0.815902343877662 0.8760154948709045 0.11458994316449242\n",
      "0.7725969826068187 0.6594721174863568\n",
      "48 0.8239910726000094 0.8842844692241927 0.12180998771998906\n",
      "0.8007242608474422 0.648755669907588\n",
      "49 0.8279133220260901 0.8869272615823968 0.11970526911921202\n",
      "0.7837136917762797 0.685405093469251\n",
      "50 0.8166878941700627 0.8793939731868582 0.11366432018759898\n",
      "0.7936849044030048 0.651008384885201\n",
      "51 0.8191217955332274 0.8775988049021664 0.11704822741571745\n",
      "0.7797440707443151 0.6750100308668981\n",
      "52 0.824065982199549 0.8841611835281354 0.11878792688143798\n",
      "0.7944038256350012 0.6555670161746198\n",
      "53 0.8223300483433513 0.8828507923534796 0.11860037056503808\n",
      "0.7986020462526657 0.6529723252780157\n",
      "54 0.8226282359735082 0.8866888043587602 0.12298962488607204\n",
      "0.7950704513710887 0.6667478979425536\n",
      "55 0.813184453304808 0.8785293230848125 0.11434069716649078\n",
      "0.7757652989876802 0.6551045175701729\n",
      "56 0.8208977033068569 0.8809164690193776 0.11552234319905502\n",
      "0.7761048322474386 0.6785191367594925\n",
      "57 0.8139523289121169 0.880269696151574 0.11770096025139712\n",
      "0.7773095543576137 0.6594920905296298\n",
      "58 0.8153764396284672 0.8775631988204313 0.11925311525529941\n",
      "0.7948281235035929 0.6557045951451297\n",
      "59 0.8255891319361082 0.8828303056165898 0.12531244435956407\n",
      "0.7977828968211493 0.6544699359135352\n",
      "working on:  checkpointz\\to_slurm\\rnnattn_latent128\\300_rnnattn-128_peptide.ckpt \n",
      "\n",
      "analysis:  model_analyses\\train\\rnnattn-128_peptide_latent128_train\\saved_info.csv checkpoint:  checkpointz\\to_slurm\\rnnattn_latent128\\300_rnnattn-128_peptide.ckpt\n",
      "rnnattn-128_peptide\n",
      "0 0.6353308604171564 0.7583802813779119 0.04454913121532766\n",
      "0.6971005257822698 0.4037748933353855\n",
      "1 0.6313764211645666 0.759344344084711 0.04200283236744863\n",
      "0.6887400445781504 0.41804052967627836\n",
      "2 0.6441158050354351 0.7719659049182519 0.048377824402574696\n",
      "0.7001851417030469 0.42565976295277475\n",
      "3 0.639110939329942 0.7681394811844883 0.046320674702908744\n",
      "0.6865930642930491 0.44502796006866585\n",
      "4 0.637485419504073 0.7713691899184898 0.04944572105289525\n",
      "0.7109793196634357 0.4017400559139276\n",
      "5 0.6384627588186488 0.7635960551321745 0.04667272305438541\n",
      "0.6971693405223947 0.42437843101504336\n",
      "6 0.6335107239986691 0.7566431868039795 0.042726659428300784\n",
      "0.6844397675361817 0.43654861571591475\n",
      "7 0.636168162752646 0.7604779329866866 0.04744539426841948\n",
      "0.6882498677280515 0.45779359628455396\n",
      "8 0.6342530151059536 0.7600505306751999 0.04375236717112351\n",
      "0.6777576868583306 0.4465502032475168\n",
      "9 0.6366265540662784 0.7698098551615612 0.046718366238414334\n",
      "0.6811531859701219 0.4249433365495642\n",
      "10 0.6400406387652607 0.7737209428090138 0.04635693377819443\n",
      "0.6971729983096231 0.41900736551203366\n",
      "11 0.6387662529628954 0.764870961697533 0.04673200574974193\n",
      "0.69881055548044 0.44228392004207007\n",
      "12 0.6359832668184331 0.7682537998113129 0.04691690692173821\n",
      "0.6789264318233412 0.45962475904416333\n",
      "13 0.645014524784162 0.7697918456315255 0.05121199605104518\n",
      "0.6983232432058311 0.4351813186778414\n",
      "14 0.6407428915330433 0.7612114526868248 0.0482612174663117\n",
      "0.7170512073171932 0.4009787511179044\n",
      "15 0.6391369665322852 0.7630526454084754 0.04540656548359253\n",
      "0.6963458338406083 0.4384926247572629\n",
      "16 0.6407371598333211 0.770279700126682 0.04972356570007934\n",
      "0.685415624371245 0.44841969702523143\n",
      "17 0.6376887317505856 0.7703604413319431 0.0472271139571799\n",
      "0.6973852240144015 0.4282692183941895\n",
      "18 0.6369445259330538 0.762661362818799 0.04590043133918335\n",
      "0.6976039049074643 0.4293715982638826\n",
      "19 0.6401222546801906 0.7687020839990044 0.047482636352481615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6874494942626816 0.4399529387416573\n",
      "20 0.6483628301855098 0.7716200197145096 0.05098626257975673\n",
      "0.6980836120050216 0.433861031138309\n",
      "21 0.6361401458418119 0.7680618585482493 0.04806220515390253\n",
      "0.677516861178342 0.43756355710042694\n",
      "22 0.6404838609940016 0.759469568055874 0.046809673760490056\n",
      "0.677925540274932 0.45077789945108393\n",
      "23 0.633045155975729 0.7633088978667749 0.0472795902739773\n",
      "0.6730411872044307 0.4469309638455279\n",
      "24 0.6341100211943218 0.7589201528935439 0.04597413878051065\n",
      "0.6803818892328517 0.42978055554240446\n",
      "25 0.647251619080084 0.7725873627102662 0.04967100737030794\n",
      "0.6801131604833263 0.46590254005768283\n",
      "26 0.6411745240796257 0.7684816104059101 0.048060692071615584\n",
      "0.6925871035047948 0.43884447730264464\n",
      "27 0.6369297059032387 0.7673715543753487 0.04801668563153102\n",
      "0.7074648277581941 0.41361576625160135\n",
      "28 0.6404495734212382 0.7660245188681168 0.04861535222138651\n",
      "0.6940990671553426 0.44264263366332457\n",
      "29 0.6466609115487498 0.7755535950743336 0.05049246289550305\n",
      "0.6856431792707861 0.46138755568211576\n",
      "30 0.6360614127709054 0.7661413788042499 0.044522872324063524\n",
      "0.6807566073533979 0.44944425832380364\n",
      "31 0.6460466801696132 0.7763709344336348 0.050547648889849105\n",
      "0.7119089197670839 0.43871167524637844\n",
      "32 0.6407083242251731 0.7682659663963929 0.044010807570832776\n",
      "0.6777793950952578 0.4388178606937275\n",
      "33 0.6434311041016275 0.7671812528852526 0.051007795949359266\n",
      "0.6923699928132279 0.44618470335546656\n",
      "34 0.6426789276548243 0.7737011604866726 0.05068794084739373\n",
      "0.6850228678707562 0.43802075477399216\n",
      "35 0.6320434558182457 0.7625041257058331 0.04308603437935604\n",
      "0.6761577839831232 0.4642020899668019\n",
      "36 0.6371504003164667 0.7658694236523724 0.04633188262157526\n",
      "0.6791410462876273 0.4526105055102786\n",
      "37 0.6376251215021058 0.7689530298834333 0.048604243167745795\n",
      "0.6708305591481665 0.4504240508753764\n",
      "38 0.6364444217076896 0.7542597038942512 0.044990307239783645\n",
      "0.6945044534255627 0.42671006906149567\n",
      "39 0.6380070082289369 0.7713629341749538 0.04624262349802567\n",
      "0.6824929575827744 0.45238586413794757\n",
      "40 0.6445674603444456 0.7649159829441539 0.04785077306710522\n",
      "0.6822709072305082 0.4654997912019615\n",
      "41 0.643925266862743 0.7746205444174341 0.051318131854179584\n",
      "0.6986044982599093 0.42892921628338443\n",
      "42 0.6470159445317959 0.7781896756133236 0.04935444254421677\n",
      "0.7006961076344282 0.43795766113873946\n",
      "43 0.6320925550969834 0.7641611751770601 0.047771103535128985\n",
      "0.6828069030865938 0.43825216087673535\n",
      "44 0.6386430529651156 0.7690583959273799 0.049636937603503184\n",
      "0.7022069143980189 0.43800674373852433\n",
      "45 0.6403738678757727 0.7598393653721178 0.04640486779043448\n",
      "0.6993064737697805 0.41737793267914824\n",
      "46 0.6389413507041921 0.7650785882699823 0.04691835913703003\n",
      "0.6926132637779594 0.43591872249889907\n",
      "47 0.6406429146368693 0.7675688852534218 0.0473730493225725\n",
      "0.694025307574514 0.4038107201261556\n",
      "48 0.6407948477682973 0.7744707031154628 0.047522814241480416\n",
      "0.6935917588111055 0.4368020622934863\n",
      "49 0.6392257762007277 0.7615598516786771 0.04676456713098969\n",
      "0.6895221231184252 0.4248845816013086\n",
      "50 0.6398122891966479 0.7708471164239613 0.04726177494703455\n",
      "0.6915483106437683 0.4261150925555437\n",
      "51 0.6396298144461271 0.7694577225204079 0.050708004467667205\n",
      "0.6835163902287404 0.46684439402067346\n",
      "52 0.6340476342694155 0.7638664809708477 0.04232102875087831\n",
      "0.7085289521013618 0.40048705490193115\n",
      "53 0.6348857540535223 0.7634146077478776 0.04509049335126429\n",
      "0.6835709489674178 0.45468210283447197\n",
      "54 0.6333871911772962 0.7564591390056175 0.04553709885598437\n",
      "0.672746933084823 0.4465343825716368\n",
      "55 0.6402107198137639 0.7639646120717682 0.04816950215320338\n",
      "0.6717222654773398 0.4589304374873264\n",
      "56 0.6369835123928027 0.7621899839299241 0.0438467373224042\n",
      "0.6777014628468989 0.4490044743013093\n",
      "57 0.6390880921561302 0.7670272284969584 0.050300568795560864\n",
      "0.684930298310952 0.45344874489794273\n",
      "58 0.6420469390873722 0.7672405890322891 0.049638888916456125\n",
      "0.6994799368579703 0.4280947388004832\n",
      "59 0.6387911698907366 0.765931785805302 0.046426011281229584\n",
      "0.7008887398281471 0.4373414491966632\n",
      "working on:  checkpointz\\to_slurm\\rnnattn_latent32\\300_rnnattn-128_peptide.ckpt \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s_renaud\\Documents\\GitHub\\MSCSAM_TBD\\main_model\\transvae\\trans_models.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.params['CHAR_WEIGHTS'] = torch.tensor(self.params['CHAR_WEIGHTS'], dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analysis:  model_analyses\\train\\rnnattn-128_peptide_latent32_train\\saved_info.csv checkpoint:  checkpointz\\to_slurm\\rnnattn_latent32\\300_rnnattn-128_peptide.ckpt\n",
      "rnnattn-128_peptide\n",
      "0 0.8411458165822864 0.9240312602088098 0.16078537884068125\n",
      "0.8254672708054736 0.7268605702661624\n",
      "1 0.843797931780024 0.9237988666490848 0.15698984708785096\n",
      "0.8266625917259635 0.6969756361939483\n",
      "2 0.8410318217346663 0.92344741209883 0.16108898400101357\n",
      "0.8311656054822147 0.7057517517107109\n",
      "3 0.8381546715066426 0.9238797910675747 0.15594719840002913\n",
      "0.8347886378744218 0.698386577182037\n",
      "4 0.831230862810444 0.9212061618083046 0.1521931427051043\n",
      "0.8350408628345631 0.6968464168963633\n",
      "5 0.84164119886802 0.9246295184170625 0.15974516967639446\n",
      "0.8237300478185805 0.6969752114051029\n",
      "6 0.8339183127934675 0.9202380461197044 0.15141383456571147\n",
      "0.8248433914050173 0.7129839487051656\n",
      "7 0.8423227399202687 0.9240867211363465 0.16170064932285\n",
      "0.8298230856209494 0.7158670715419313\n",
      "8 0.8403651822432502 0.9231265032853226 0.15848043516568558\n",
      "0.8222668459978384 0.7091221282169652\n",
      "9 0.8453912641909744 0.9248750031503131 0.16253747223809573\n",
      "0.833637508587909 0.6911953983458985\n",
      "10 0.8322278606265344 0.9227281061168845 0.15767196045270415\n",
      "0.8339543204982582 0.6767684122351942\n",
      "11 0.8396181632223539 0.922559533797318 0.15429857008089343\n",
      "0.8277987175320503 0.687159472304216\n",
      "12 0.8426350394620368 0.9221998499099403 0.15609800425822365\n",
      "0.8210036633479523 0.7316566347155156\n",
      "13 0.8390132803929399 0.9241133182796232 0.16306214486830173\n",
      "0.8293762836415987 0.6958159922377416\n",
      "14 0.8437397924641427 0.9233254373899171 0.1589427182714841\n",
      "0.8177607963958773 0.711010620100708\n",
      "15 0.8347048535810904 0.9207944660784282 0.15055821507059367\n",
      "0.8291174749262774 0.6926378991642712\n",
      "16 0.8340128045255661 0.9216481612817351 0.1550834577623126\n",
      "0.8282371521031238 0.713004228498648\n",
      "17 0.8430095875665079 0.9247438715622192 0.15644510023075073\n",
      "0.8309496117934356 0.7004066823832427\n",
      "18 0.8423715820019357 0.9231169113043061 0.15675609121130357\n",
      "0.8241918972424074 0.7045444775483962\n",
      "19 0.8481639358219671 0.925921175046148 0.16476721698741334\n",
      "0.830206777105106 0.7075575539021024\n",
      "20 0.8415201251815178 0.9226629416783932 0.1601277115582987\n",
      "0.8278887622141641 0.6861384144917648\n",
      "21 0.835502953534259 0.9210381892274783 0.155770816793529\n",
      "0.8295938329792254 0.675298344830674\n",
      "22 0.8355765336038629 0.9217967222278408 0.1551617931468927\n",
      "0.8265588801172936 0.716174000523724\n",
      "23 0.8392743159911576 0.922545975851569 0.15623447909365965\n",
      "0.8315067461251275 0.7030637521020604\n",
      "24 0.8412149617961555 0.9253433632734998 0.1579623271166905\n",
      "0.8353234818004465 0.7117521541414089\n",
      "25 0.8471853732067371 0.9252904285388732 0.16301104064450184\n",
      "0.837820678309638 0.680437982387812\n",
      "26 0.8407372018791178 0.9236732957091333 0.1619173640558477\n",
      "0.830490200278178 0.6983731767029073\n",
      "27 0.836900498629685 0.9214678749708659 0.1537474950150257\n",
      "0.8153886242038654 0.6809972600442645\n",
      "28 0.8444188438247826 0.9242255605092363 0.15754984138161648\n",
      "0.834784455926874 0.7249375344323638\n",
      "29 0.845072012916085 0.9259985610509939 0.16013748276027104\n",
      "0.8418319903541374 0.7029079077027041\n",
      "30 0.838489096730721 0.9221506117060203 0.15860286439133253\n",
      "0.8282285441808879 0.6954142658697653\n",
      "31 0.8408571001228835 0.9233247140265523 0.16071524715559357\n",
      "0.8277620555810354 0.6985675046676335\n",
      "32 0.8392788979898643 0.9238604488213369 0.15514900734266293\n",
      "0.8328633637644227 0.6817268976953073\n",
      "33 0.8361744262951428 0.9225819341072333 0.15338643564319376\n",
      "0.8259992420264174 0.7072452218845473\n",
      "34 0.8427761283420461 0.9236417610327716 0.1589472489141825\n",
      "0.8207823554629079 0.7286663608975142\n",
      "35 0.8394590615947204 0.9216654418137157 0.15448565375371703\n",
      "0.828282056373463 0.7100008032563045\n",
      "36 0.8359055906434798 0.9230029880537387 0.1649059833822478\n",
      "0.8183057917297163 0.6991159816187915\n",
      "37 0.8386254041047688 0.9229495225473183 0.15892488994054965\n",
      "0.8286749544903184 0.6904586086414652\n",
      "38 0.8400297224465186 0.9235118302153986 0.1584003928468129\n",
      "0.8292402383135098 0.7022265903056372\n",
      "39 0.8431787466634687 0.9238206956200219 0.1598242216819046\n",
      "0.8229399882955726 0.7146194458653148\n",
      "40 0.8413888037106642 0.924795714491296 0.16081945160726102\n",
      "0.8221638822061861 0.720445232562656\n",
      "41 0.8374910362264537 0.9224256556646713 0.15620000148713797\n",
      "0.8325171539752623 0.7120795962924116\n",
      "42 0.8411697962462468 0.9222267657050304 0.15932874023273333\n",
      "0.8330006650312096 0.7079125203870913\n",
      "43 0.8405219268807034 0.9222114950008964 0.15780237498962904\n",
      "0.8305857731608237 0.709578352847853\n",
      "44 0.8412166946502181 0.9230261939296773 0.1505815598826375\n",
      "0.8331333202253961 0.697943245284377\n",
      "45 0.8394803076314877 0.9220232718766018 0.16037841303126998\n",
      "0.8219088557247253 0.7054500766540163\n",
      "46 0.8418229984020262 0.9214232295259617 0.15778779548322594\n",
      "0.8167676700007215 0.6966114382621068\n",
      "47 0.836678858292995 0.9220874560979979 0.15640224938722136\n",
      "0.8322929987647856 0.6749988464982404\n",
      "48 0.8437544128850457 0.925034704080892 0.159761133134514\n",
      "0.8398379775467262 0.6857010741248117\n",
      "49 0.8436869330650312 0.9237523756987277 0.15964663138216922\n",
      "0.835865893393415 0.7005427213466612\n",
      "50 0.8463199876918205 0.9245879783513159 0.1613235839706597\n",
      "0.8298438520839001 0.7111282116316084\n",
      "51 0.8445674785784082 0.9231215142741097 0.15733077654897262\n",
      "0.84643864295074 0.6844368985983534\n",
      "52 0.8415546306605349 0.9236854376633304 0.158601610774923\n",
      "0.8440064305851587 0.689935356090069\n",
      "53 0.8421275075849698 0.9224098134362909 0.16138965782780024\n",
      "0.8200900255396011 0.6923464251441428\n",
      "54 0.8457264216233398 0.9254632338662959 0.16323932455531717\n",
      "0.8290875520999672 0.715071536713471\n",
      "55 0.8417221654987352 0.9249949057186256 0.1645798715446497\n",
      "0.8345258331130126 0.703477077108151\n",
      "56 0.840104059626298 0.922064307557903 0.15406117555062626\n",
      "0.8303501958800908 0.6938132694362704\n",
      "57 0.842421736585093 0.9231485966348398 0.1592461448197907\n",
      "0.8199365133722439 0.7222940583932331\n",
      "58 0.8429144604538965 0.9248642670846937 0.15703187414372896\n",
      "0.835109334146662 0.6812147505987216\n",
      "59 0.8383588165913101 0.9228579494329858 0.15859114566482216\n",
      "0.8359499898058023 0.696199922669343\n",
      "working on:  checkpointz\\to_slurm\\rnnattn_latent64\\300_rnnattn-128_peptide.ckpt \n",
      "\n",
      "analysis:  model_analyses\\train\\rnnattn-128_peptide_latent64_train\\saved_info.csv checkpoint:  checkpointz\\to_slurm\\rnnattn_latent64\\300_rnnattn-128_peptide.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s_renaud\\Documents\\GitHub\\MSCSAM_TBD\\main_model\\transvae\\trans_models.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.params['CHAR_WEIGHTS'] = torch.tensor(self.params['CHAR_WEIGHTS'], dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rnnattn-128_peptide\n",
      "0 0.6784659468174653 0.7880437994816769 0.07144512795368324\n",
      "0.6570222058505719 0.5741605420033014\n",
      "1 0.6706165608207498 0.7842838158428274 0.07113808619347702\n",
      "0.6725471199951865 0.5611468485979082\n",
      "2 0.6744940984678336 0.7856100301521914 0.07315731375618922\n",
      "0.6645727528510994 0.6081781830181188\n",
      "3 0.6765381787032141 0.7903419555634738 0.07150906755315824\n",
      "0.6796090483409821 0.5582776737758084\n",
      "4 0.6809641162082679 0.7932534097497996 0.07572072391851802\n",
      "0.6780243263244226 0.5826062425681307\n",
      "5 0.6771917218531235 0.7895214400874068 0.07498894343796654\n",
      "0.6612943670545787 0.5718467529847067\n",
      "6 0.6761487029788871 0.7857270039178567 0.07537428492953487\n",
      "0.6819862290081189 0.5583836096707548\n",
      "7 0.6770867274773342 0.7905151866751302 0.06987553011789292\n",
      "0.6714350719777153 0.5930202766413308\n",
      "8 0.6850880643181411 0.7946265192305768 0.07628924448112995\n",
      "0.6777626164411525 0.5758754489503317\n",
      "9 0.6802023855489114 0.7912616560909556 0.07109190319830024\n",
      "0.6662549304896868 0.5857855945814229\n",
      "10 0.6774749065407989 0.7863993847435882 0.0701154098858055\n",
      "0.6701583726622222 0.5851286067488962\n",
      "11 0.6792985780454682 0.7937187357465589 0.07101773076240785\n",
      "0.6913176357249582 0.5639465262325849\n",
      "12 0.6672232113427374 0.7795461392541474 0.07209209332323646\n",
      "0.6654281456948969 0.5756881804328442\n",
      "13 0.6763793598028522 0.7932444829576942 0.0716158220576795\n",
      "0.6736670076003373 0.5609389423112813\n",
      "14 0.6772420474050407 0.7906246454027339 0.06825147410460311\n",
      "0.6693943844580206 0.5857097343999449\n",
      "15 0.6936431845959631 0.797748113131737 0.0786034952963929\n",
      "0.6933397711275362 0.5665595769123746\n",
      "16 0.6787342179833877 0.7927371343136606 0.07455458931232087\n",
      "0.6832484483831438 0.550875638370965\n",
      "17 0.6756671861549318 0.7897421998856059 0.07086922956236122\n",
      "0.6752948198920599 0.5852129059076034\n",
      "18 0.678532106941706 0.7896060384748603 0.0742621478575499\n",
      "0.6832159108659146 0.583223667491291\n",
      "19 0.6796828964145517 0.7913722088169518 0.06869182304439184\n",
      "0.688566102752503 0.5616138378375478\n",
      "20 0.6795038680857214 0.7861825094969138 0.07312856196050425\n",
      "0.6728051227421229 0.5931015362421856\n",
      "21 0.6779530181606538 0.7864245916573148 0.07272925416017945\n",
      "0.6554299740083075 0.5816520055097796\n",
      "22 0.6774798423919237 0.7859649137669359 0.07156878572694152\n",
      "0.6783620111509234 0.554665210048791\n",
      "23 0.6722104894412473 0.7847960773060584 0.06772803135811922\n",
      "0.6741688808606286 0.5631141516569393\n",
      "24 0.6735808635331992 0.785368813000109 0.07279571703032718\n",
      "0.6612313046301004 0.5717386088939551\n",
      "25 0.6793526353545346 0.7897188771347577 0.07045805906827621\n",
      "0.6780035612512627 0.5339069547229559\n",
      "26 0.6758333213134218 0.7871599583039226 0.06954765793670424\n",
      "0.6562493067429571 0.5846974041596885\n",
      "27 0.6715636936394455 0.7836809386227596 0.06929672029141194\n",
      "0.6734769154706948 0.5819014388607189\n",
      "28 0.672956956846826 0.7873030278505515 0.06914126199003935\n",
      "0.6774869980518649 0.562682728275493\n",
      "29 0.6713101484895143 0.77963429976667 0.06617315514401578\n",
      "0.674097104932118 0.5867545867732455\n",
      "30 0.6681641458841113 0.7857916775131839 0.06802304077256902\n",
      "0.6741923180654144 0.5667280427175256\n",
      "31 0.6778559719132274 0.7885672699210331 0.07375138060471888\n",
      "0.6561537666348953 0.5939441859054821\n",
      "32 0.6776941539885963 0.7891558703042109 0.07354070486485306\n",
      "0.6671504417333739 0.5677647751792341\n",
      "33 0.6777393953908019 0.7939582680278405 0.07344288901872491\n",
      "0.6807823303506483 0.5831477662120982\n",
      "34 0.6787327280488991 0.7908796224284976 0.07251750396733574\n",
      "0.6610824002370148 0.59820805541521\n",
      "35 0.6732470280234872 0.7796841057356364 0.06495416121186977\n",
      "0.6901294259950302 0.5489495393848091\n",
      "36 0.6755472871731145 0.789797901847137 0.0721436759922158\n",
      "0.6643072174829521 0.5983314293390374\n",
      "37 0.6845806489577989 0.7958854520049027 0.07238903494068233\n",
      "0.6688139293180548 0.5681521147058117\n",
      "38 0.6774486589971686 0.7886004143561203 0.07346932792832236\n",
      "0.6769115412412028 0.5857840523650761\n",
      "39 0.6728001221190834 0.7799978800345881 0.0690798053029991\n",
      "0.6737812612722623 0.5642839836754612\n",
      "40 0.6711086591764921 0.7871355885417102 0.06940996156838096\n",
      "0.673055073376385 0.5491423922974608\n",
      "41 0.6892301574637467 0.7940614504589132 0.07702352175201169\n",
      "0.6820089820000752 0.5869850301447511\n",
      "42 0.6707776706407274 0.7839689920683692 0.06815526967304784\n",
      "0.684443882752289 0.5583998237108254\n",
      "43 0.6776401283055083 0.7873278645723508 0.07342805807529626\n",
      "0.6842523004790337 0.5623094833993132\n",
      "44 0.6744318581851383 0.7835813173758012 0.0680204208024183\n",
      "0.6615783410238077 0.5754199359202661\n",
      "45 0.6704692121856528 0.7920870050326081 0.07455666539170937\n",
      "0.6645101614564692 0.6003199878382297\n",
      "46 0.6797318531046802 0.78895837283516 0.07056069127182563\n",
      "0.6559979949835126 0.6076590010316542\n",
      "47 0.6803179460787291 0.7956630976838355 0.07316169477222567\n",
      "0.6917357598455167 0.5884053597699299\n",
      "48 0.6837528944276666 0.792051117306788 0.07509576470564544\n",
      "0.6768949571894853 0.5750895182028516\n",
      "49 0.6845928796041411 0.7928871133039805 0.07003159485946084\n",
      "0.6808806524428159 0.5510559355617088\n",
      "50 0.680339332047184 0.7822085453095079 0.07117305755746467\n",
      "0.6613053386775962 0.5725115016217204\n",
      "51 0.6702598953241442 0.7862883714550754 0.06989819639375529\n",
      "0.6550125708199204 0.5565859199125138\n",
      "52 0.6753662090601315 0.7872333315544936 0.06947025213011564\n",
      "0.6556945827660365 0.5784130419470485\n",
      "53 0.6830199389961995 0.7914818741852626 0.07423800134820617\n",
      "0.6842005085584204 0.5674247227399543\n",
      "54 0.6766024012706292 0.7846636108947608 0.06939083606129852\n",
      "0.6527253532812436 0.582817129103464\n",
      "55 0.6773073579747273 0.7892191744405257 0.07331859351767034\n",
      "0.6693282486597372 0.5840175778141254\n",
      "56 0.6756564876505147 0.7893976181418395 0.07261955506317935\n",
      "0.6733563374723317 0.580187798681743\n",
      "57 0.6757325149546181 0.7907635672378973 0.07477632188077467\n",
      "0.6673989871502244 0.599295977402013\n",
      "58 0.6830473468967309 0.7927652082998093 0.07190315883367461\n",
      "0.6924788095901003 0.5470662137496658\n",
      "59 0.67445730692104 0.7908670526314788 0.070709680501007\n",
      "0.6921019509044458 0.5486779810953025\n",
      "working on:  checkpointz\\to_slurm\\rnn_latent128\\300_rnn-128_peptide.ckpt \n",
      "\n",
      "analysis:  model_analyses\\train\\rnn-128_peptide_latent128_train\\saved_info.csv checkpoint:  checkpointz\\to_slurm\\rnn_latent128\\300_rnn-128_peptide.ckpt\n",
      "rnn-128_peptide\n",
      "0 0.7170463124766149 0.7897659100970492 0.06949795914726524\n",
      "0.7037608724301856 0.6602847562893401\n",
      "1 0.7190965854351803 0.7953086207325951 0.07133745893251042\n",
      "0.7095189583422801 0.6272379642768477\n",
      "2 0.7181987949169658 0.7913269495940833 0.07036050735162394\n",
      "0.7131492619562061 0.6432196182323962\n",
      "3 0.7220322354768292 0.7970482189898086 0.06913233651445702\n",
      "0.7191614092961034 0.62702680491944\n",
      "4 0.7195427734180263 0.7943946381661179 0.0712019529276419\n",
      "0.7216830377221355 0.6466359848909635\n",
      "5 0.7225848412403896 0.7965545936989115 0.074709118921247\n",
      "0.7199780362069743 0.6276243866726585\n",
      "6 0.7201900063167029 0.7981394657882769 0.0729164929442621\n",
      "0.7105361962877922 0.6393253403246202\n",
      "7 0.7190625867562441 0.7967890614478226 0.06933793921264958\n",
      "0.6967317490780041 0.6433200641376972\n",
      "8 0.7222484945433897 0.7983457147450576 0.07006525027658644\n",
      "0.7043658880422119 0.6476085932557012\n",
      "9 0.7198128225768992 0.7998052056083599 0.07305121430739847\n",
      "0.7229810250589535 0.6456859796460641\n",
      "10 0.7159815607909679 0.7965888100584412 0.07162940424892994\n",
      "0.715942805609677 0.6144208569200502\n",
      "11 0.7252426503955849 0.8028881410810799 0.07587093552952756\n",
      "0.7086607561216933 0.6544863980452136\n",
      "12 0.7230343448970938 0.7979692450014559 0.06883154433000667\n",
      "0.7174632871252975 0.629850097198823\n",
      "13 0.7144677454312693 0.7918178540685437 0.07090982181406509\n",
      "0.7114909190085761 0.6089456379213362\n",
      "14 0.7091504145085874 0.785139648335216 0.06688573471489975\n",
      "0.7003034473120635 0.6317024863466503\n",
      "15 0.7145250548110775 0.7894610922052087 0.0723399638643281\n",
      "0.6991656632475922 0.6622598181916303\n",
      "16 0.718632007161841 0.7903683633377939 0.07127725294933288\n",
      "0.7092398384748082 0.6223308916536686\n",
      "17 0.7267490714328155 0.8040876725020407 0.07166777345851787\n",
      "0.7201154711574735 0.6394296709103406\n",
      "18 0.7173717944191911 0.7936644130237126 0.07176140803552825\n",
      "0.7211613969910882 0.6149165056048924\n",
      "19 0.7161880168842137 0.7900326001076156 0.07028171460759487\n",
      "0.7034997418409261 0.612630662338671\n",
      "20 0.7118106127501507 0.7948008386857894 0.07092606260445466\n",
      "0.7269673427425782 0.6253167863537821\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 0.7157836104352369 0.7932578938644621 0.0675671071078069\n",
      "0.7017240221688761 0.623545293052123\n",
      "22 0.7198228693922046 0.7984629995538555 0.07290750767297044\n",
      "0.7072444761465411 0.6236361463876595\n",
      "23 0.7181157454352742 0.7952269009839292 0.07116534108602726\n",
      "0.7304471350295243 0.6133819730171627\n",
      "24 0.720995216406226 0.7963407058442433 0.072056738885602\n",
      "0.7253331437395014 0.6351098566750173\n",
      "25 0.7180811111996518 0.7991794052288024 0.06924063456847498\n",
      "0.6998824668259859 0.6541241732953771\n",
      "26 0.719307652687122 0.798141790609136 0.0703171718382556\n",
      "0.715052706248106 0.6383625634582812\n",
      "27 0.7205939214016582 0.8021807006274684 0.07447767071450338\n",
      "0.7075187245814634 0.6492485163020585\n",
      "28 0.7247382858393497 0.803160947485803 0.07337383104551093\n",
      "0.71731764929364 0.6439015504111923\n",
      "29 0.7197534151440779 0.7954519929883359 0.0715859231647706\n",
      "0.7202419101980182 0.6169553404020512\n",
      "30 0.7214275460194748 0.79862974278644 0.07163260495138231\n",
      "0.701881210540239 0.6524826838747619\n",
      "31 0.7250008613664255 0.8005833434045967 0.07416455450802807\n",
      "0.7188181375760699 0.6307225575739793\n",
      "32 0.7252615192910193 0.7984639730182741 0.07330811796785297\n",
      "0.7119036701662466 0.6300714897627417\n",
      "33 0.7164188940485678 0.7904173366474997 0.07118224651912596\n",
      "0.7105077367641115 0.6291691793649199\n",
      "34 0.726464735014267 0.7994194318484569 0.07138830949672073\n",
      "0.7071664393377601 0.6448147036769494\n",
      "35 0.7167592127695834 0.7925641322548708 0.07107244230405982\n",
      "0.7243462068146989 0.6236188346809103\n",
      "36 0.7177303620879437 0.7884620321515711 0.07191825316052577\n",
      "0.7065756878572597 0.6167042280674425\n",
      "37 0.7141137682854336 0.7940281474275163 0.0668923019258168\n",
      "0.7045961431182963 0.6438514065065788\n",
      "38 0.7183008085163398 0.794406997378738 0.06963556720953065\n",
      "0.7154455629315799 0.6172279004958543\n",
      "39 0.7305975200389496 0.8051556063083244 0.07494533166687398\n",
      "0.7108160214187399 0.6617340713385991\n",
      "40 0.7180483914014728 0.7947661849675011 0.07162578157426461\n",
      "0.7027469903215842 0.6248720590336214\n",
      "41 0.7245169363461315 0.7997481371572613 0.07253944731528994\n",
      "0.7123042472691437 0.6325570036600519\n",
      "42 0.7175227123104425 0.7939939243767242 0.06959379636055783\n",
      "0.7025236660177416 0.646990568113589\n",
      "43 0.719720288825692 0.7982126796275105 0.0738732486394507\n",
      "0.7276250194191943 0.6004508087412497\n",
      "44 0.7359261148695321 0.8060535357881571 0.0761443903243459\n",
      "0.7333359146729108 0.6222506249797124\n",
      "45 0.716165256738283 0.7916281338510051 0.07166552889656962\n",
      "0.691223985398064 0.6359618943931429\n",
      "46 0.7194614696774098 0.7965692750271769 0.06966293456925139\n",
      "0.7121006962857732 0.6417592065549493\n",
      "47 0.7186796366269276 0.7966274868972401 0.0715065272095211\n",
      "0.7067510206984877 0.6426612948153245\n",
      "48 0.723342957237381 0.8018840118834621 0.07291548521888006\n",
      "0.7273013301550557 0.6286259136271658\n",
      "49 0.7115253345225453 0.7874462550999735 0.06891109062952826\n",
      "0.7099088399787088 0.6356430491652104\n",
      "50 0.7196873037184233 0.7953061825128925 0.07020809896489158\n",
      "0.702828287049557 0.6512140124965142\n",
      "51 0.7235886405386935 0.8000678908254718 0.07220096150364783\n",
      "0.704305453858689 0.6269808604150313\n",
      "52 0.7205263024711943 0.7984386294345829 0.07064857136071069\n",
      "0.7100827247014538 0.6421551329837787\n",
      "53 0.7182093497287856 0.7941792400859493 0.06832305184506\n",
      "0.726485249423128 0.6023448034141925\n",
      "54 0.7226795228235867 0.7976162428322497 0.07552058925818865\n",
      "0.7241413260245656 0.6054549652575338\n",
      "55 0.7190179140587789 0.8013893181442364 0.07026481704477669\n",
      "0.7175190787103793 0.6534810350789153\n",
      "56 0.7234886412211818 0.7999630990325964 0.07158347688897264\n",
      "0.7229531557450906 0.5974932993645574\n",
      "57 0.7080936122909979 0.7839494114247998 0.06883525752385244\n",
      "0.6905088528488587 0.656700529243595\n",
      "58 0.7173974821641678 0.7878884255871795 0.06993786362872961\n",
      "0.7231677901676186 0.5756959557315044\n",
      "59 0.7189147368461064 0.7969891422653566 0.0739751948194326\n",
      "0.7070740075992035 0.6315962456743114\n",
      "working on:  checkpointz\\to_slurm\\rnn_latent32\\300_rnn-128_peptide.ckpt \n",
      "\n",
      "analysis:  model_analyses\\train\\rnn-128_peptide_latent32_train\\saved_info.csv checkpoint:  checkpointz\\to_slurm\\rnn_latent32\\300_rnn-128_peptide.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s_renaud\\Documents\\GitHub\\MSCSAM_TBD\\main_model\\transvae\\trans_models.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.params['CHAR_WEIGHTS'] = torch.tensor(self.params['CHAR_WEIGHTS'], dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rnn-128_peptide\n",
      "0 0.6925458886562504 0.779542194020338 0.06568971487516823\n",
      "0.6322176248304865 0.6938100368916376\n",
      "1 0.6996137554918129 0.7877369480709597 0.0697970803194586\n",
      "0.6340890645447861 0.7052293221120417\n",
      "2 0.695865410590323 0.7808732622338078 0.06842707029263789\n",
      "0.6257106991133291 0.7290141043265063\n",
      "3 0.7010887254944943 0.7865455247662022 0.07486711336908686\n",
      "0.629595200367809 0.7236341933456143\n",
      "4 0.6894655507343049 0.7779910142698239 0.06583217997329467\n",
      "0.6302328176811458 0.6974998190293968\n",
      "5 0.6924466050345424 0.7781191354615891 0.0713878950687409\n",
      "0.6266902325756103 0.7269965292920533\n",
      "6 0.698273514738817 0.7866052092770522 0.06907478838406231\n",
      "0.6298837803963271 0.726573274309411\n",
      "7 0.6915134097032946 0.7748570155728998 0.06707188418409989\n",
      "0.6269446285624869 0.7073599471572354\n",
      "8 0.6893047172781938 0.7793133177818587 0.06713096523475243\n",
      "0.622046350147752 0.7202011345062729\n",
      "9 0.6919642194632307 0.7765213949893951 0.0686821649089238\n",
      "0.6184298073175261 0.7149721650014477\n",
      "10 0.6943749154082193 0.7824574522629529 0.06696738889427288\n",
      "0.6326322599095655 0.70222099861561\n",
      "11 0.6882456434222115 0.7761684648965571 0.06417068425437902\n",
      "0.6226123985204572 0.7084381149364398\n",
      "12 0.686773436804617 0.7761610312237104 0.06204684070288863\n",
      "0.624675607184316 0.7063863061751778\n",
      "13 0.6987248891411615 0.7853746008578366 0.06699773023092673\n",
      "0.6500085644989855 0.6968770083766112\n",
      "14 0.6905066106937054 0.7779522148710852 0.06598932079587931\n",
      "0.6298169723184837 0.7178001754844161\n",
      "15 0.6916443787356297 0.7774874013621957 0.06875604013291713\n",
      "0.6146304182501545 0.7130506090501122\n",
      "16 0.6896275547250535 0.7789987367249499 0.06860498150456908\n",
      "0.6118627140143424 0.7292191500066469\n",
      "17 0.6890266806147609 0.772115613128397 0.06297136087738023\n",
      "0.6189784119873663 0.7108052815994275\n",
      "18 0.6794163764870412 0.7702197145494735 0.060663755035776416\n",
      "0.6168594346698331 0.6978603179786995\n",
      "19 0.6903950313606343 0.7800394062347252 0.06820432350314545\n",
      "0.6148772368063329 0.7174403013297095\n",
      "20 0.6910220948340565 0.7756179283985372 0.0665690912131803\n",
      "0.6083403246095631 0.7312420571492997\n",
      "21 0.6905671648423408 0.7792488847449989 0.06421660229394724\n",
      "0.6200899620134925 0.7203385801286784\n",
      "22 0.6891627318745533 0.7738363415163713 0.06428233847439227\n",
      "0.6322465716116468 0.6982700753724853\n",
      "23 0.7006370945383654 0.7868189716724354 0.07278633811260436\n",
      "0.616365213377921 0.7272991586447762\n",
      "24 0.6946083533091629 0.7831920501626858 0.0693637829482437\n",
      "0.6239931301772421 0.7262748085271721\n",
      "25 0.689505852897796 0.7795263438974126 0.06324696721657383\n",
      "0.6376013854852511 0.6948910987342259\n",
      "26 0.6858404463637703 0.7772259778321609 0.0674780253585443\n",
      "0.6221544489862153 0.7232015187558033\n",
      "27 0.6985453550626338 0.784739309010376 0.06909237509664841\n",
      "0.6232309477981074 0.7245963632064121\n",
      "28 0.6939183357689838 0.7800469550053015 0.0665875107575647\n",
      "0.6106814517838113 0.7186673904552388\n",
      "29 0.6905354776904838 0.7829529300237689 0.06740154463030666\n",
      "0.6160402101851962 0.7217469919245574\n",
      "30 0.6866347265272982 0.7707614299399262 0.06332356098686769\n",
      "0.6170514151736833 0.7248414183110417\n",
      "31 0.6871873030236331 0.7730377521321984 0.06450746634557583\n",
      "0.6211674648690482 0.7222610902157519\n",
      "32 0.6888180075435117 0.7797959796833244 0.06849694768931024\n",
      "0.6317855324539493 0.7213360804565812\n",
      "33 0.6967337529786696 0.7824085809643071 0.06771438545279301\n",
      "0.6211987447272314 0.7253494355965917\n",
      "34 0.6948237778981399 0.7786965203944908 0.06640100642354681\n",
      "0.6284737427413013 0.7172397398748787\n",
      "35 0.6965522518971731 0.784088426398173 0.07221999711425349\n",
      "0.6157037977252564 0.7292066488583606\n",
      "36 0.6960222180603521 0.7800290152655882 0.06716653455793707\n",
      "0.613531492483337 0.717936180017185\n",
      "37 0.6889505311076303 0.7802496596852392 0.06732434577331518\n",
      "0.6300695746956823 0.7099383353137515\n",
      "38 0.6932458425323658 0.7783823198542194 0.06387551665485035\n",
      "0.6220879803071363 0.7136703476373663\n",
      "39 0.7001909634788398 0.7868926666239433 0.06999699986520842\n",
      "0.6408501364424826 0.6895429648204146\n",
      "40 0.6840968640651885 0.7741710532932983 0.06420335304011134\n",
      "0.6192577421232024 0.7072428891101947\n",
      "41 0.6961477555325948 0.7806152633864383 0.06837807108477478\n",
      "0.6302825572435644 0.7208208518784462\n",
      "42 0.6935605709854378 0.7824276077150426 0.07238926457187621\n",
      "0.616117136962276 0.7255534304294604\n",
      "43 0.6929493503568598 0.775617523627478 0.06615611100708876\n",
      "0.6421882361032326 0.6834959158737852\n",
      "44 0.6973134998653612 0.784645183094184 0.0680667423152213\n",
      "0.6287699723545119 0.7244201580730592\n",
      "45 0.6911939124738369 0.7773612529600902 0.06969683321474762\n",
      "0.6532219205104628 0.700092872166504\n",
      "46 0.6815622244837637 0.773340057561134 0.059830479813493556\n",
      "0.6281231212461718 0.702322013372181\n",
      "47 0.6965053623986028 0.7851319632002357 0.06724379997091595\n",
      "0.6277843057101328 0.7038251914107808\n",
      "48 0.6890410821940303 0.7764279095012925 0.06647581579087225\n",
      "0.6392084004255141 0.6853651809293806\n",
      "49 0.6929057411451368 0.7790102695932372 0.06601593124532351\n",
      "0.6125261305117764 0.7126180816090228\n",
      "50 0.6926173042104617 0.7746261077815247 0.06904093937065799\n",
      "0.6104167685496356 0.7275919006442639\n",
      "51 0.6992005624448288 0.7826793856030814 0.07059870546065243\n",
      "0.6139094535682696 0.7251037392623032\n",
      "52 0.6918447350521664 0.7780560278624629 0.06537973804775703\n",
      "0.6394183023696849 0.6818134665934165\n",
      "53 0.6924088839322702 0.7761724654853639 0.06551295698990696\n",
      "0.6189714548683065 0.7167224527140572\n",
      "54 0.6899374124196695 0.7763237044422348 0.0654918983179866\n",
      "0.6344948230740266 0.6865149609549179\n",
      "55 0.6843972725113239 0.7759543497184859 0.06375184572386794\n",
      "0.6388138304264753 0.666093632732988\n",
      "56 0.6904019579594795 0.7764940530263174 0.0692767716058534\n",
      "0.6106537605456149 0.7266112773938709\n",
      "57 0.6899607272088322 0.7794202931097696 0.0660316546399402\n",
      "0.6182186914719225 0.7213335464142621\n",
      "58 0.6901262223340434 0.7776989329435943 0.06997273780631184\n",
      "0.6387501617551039 0.6971295386446652\n",
      "59 0.6891403724737438 0.7767515557419218 0.06497034142812291\n",
      "0.6309825374321338 0.7010624456602961\n",
      "working on:  checkpointz\\to_slurm\\rnn_latent64\\300_rnn-128_peptide.ckpt \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s_renaud\\Documents\\GitHub\\MSCSAM_TBD\\main_model\\transvae\\trans_models.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.params['CHAR_WEIGHTS'] = torch.tensor(self.params['CHAR_WEIGHTS'], dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analysis:  model_analyses\\train\\rnn-128_peptide_latent64_train\\saved_info.csv checkpoint:  checkpointz\\to_slurm\\rnn_latent64\\300_rnn-128_peptide.ckpt\n",
      "rnn-128_peptide\n",
      "0 0.7308433741541486 0.8121961903419505 0.08029625636871243\n",
      "0.7279073794019044 0.6322090194848315\n",
      "1 0.7308330552932353 0.8161629364289258 0.07372994079621953\n",
      "0.731830888900191 0.6019822764876963\n",
      "2 0.7296622195487493 0.8175514400119344 0.07441018159508939\n",
      "0.7267811944111642 0.6167960111872748\n",
      "3 0.7266557384353366 0.8101065021696778 0.07517595445013546\n",
      "0.7199428956150753 0.6227767166169916\n",
      "4 0.7369441168272005 0.8188871363978852 0.08395392490415149\n",
      "0.7315020936724812 0.6100120422767646\n",
      "5 0.7344947019326202 0.8130184531772069 0.07324720488526262\n",
      "0.7201118462314164 0.6188922371746486\n",
      "6 0.7259918655400626 0.8065180444603732 0.07473062976036525\n",
      "0.7331667626359111 0.5798752817263024\n",
      "7 0.7310891187740847 0.8182363215850637 0.07853810912393058\n",
      "0.7155102654527274 0.6340641340921245\n",
      "8 0.7225467135012822 0.8105825718204842 0.07238467415760806\n",
      "0.7199618033346854 0.6154121259629158\n",
      "9 0.732247755705857 0.8161307946120565 0.07591216625472323\n",
      "0.7119024775982188 0.6377077605197048\n",
      "10 0.7261282251950956 0.8110723259512297 0.07530689788608588\n",
      "0.7271654620660064 0.5976933345503803\n",
      "11 0.7340053471193789 0.8150474875525784 0.07176427678346187\n",
      "0.7229086794021697 0.6348070247166921\n",
      "12 0.7281900959436982 0.8097018110745672 0.07694953837589703\n",
      "0.7164760374491488 0.6121087093841636\n",
      "13 0.7317008328858615 0.8147457165070081 0.0757292341382245\n",
      "0.7053508928028109 0.6204569486691174\n",
      "14 0.7380366072705192 0.8197671040434774 0.08023238454735385\n",
      "0.7396686569411752 0.5956954566682062\n",
      "15 0.7329349818546188 0.8165741400171509 0.0791332931097722\n",
      "0.7017103699630768 0.639263578366623\n",
      "16 0.739570520732202 0.8211193181924153 0.07892346488278836\n",
      "0.7149249161672601 0.6552296877689493\n",
      "17 0.7332454556917 0.8154695552247249 0.0772923370768436\n",
      "0.716471395580301 0.6250340893589268\n",
      "18 0.7277590805723543 0.8151301536029617 0.07497752263316633\n",
      "0.7415778189883819 0.6016508118868495\n",
      "19 0.7341877503272335 0.8181240966009 0.08035015507023491\n",
      "0.7180473657179367 0.6289540570505917\n",
      "20 0.7279453448950892 0.8116401606609194 0.07721225846104243\n",
      "0.7038332646642342 0.6337278864010178\n",
      "21 0.7359565084582038 0.8220604998305905 0.08316315849632662\n",
      "0.7319790412015592 0.626179030135561\n",
      "22 0.7284109048493876 0.814869268661495 0.0763399050702062\n",
      "0.7350304414031961 0.6070230886204451\n",
      "23 0.7355975392010246 0.8198443398676106 0.08240932523635719\n",
      "0.7375967512403615 0.5991314171566415\n",
      "24 0.7346248637285919 0.8152267642284314 0.08083333550794708\n",
      "0.703935095600422 0.6441701127666499\n",
      "25 0.7323568039104056 0.8134593813404161 0.07703104505274505\n",
      "0.7168352333489962 0.5914755907757439\n",
      "26 0.735076705807974 0.8177619848385788 0.08036729463263399\n",
      "0.7389845324446669 0.6084646629466273\n",
      "27 0.7261692115194136 0.8103844498683975 0.07518067996939089\n",
      "0.7244046378387757 0.6007700410236285\n",
      "28 0.7270439690588275 0.8088610807361613 0.07434089144055987\n",
      "0.7266785070477608 0.5898747425776983\n",
      "29 0.7208834730162339 0.8068889047308692 0.07726385896556358\n",
      "0.7135659534533612 0.6258647315193464\n",
      "30 0.7287269611885687 0.8137975065768118 0.07837012544826286\n",
      "0.7275116388803542 0.6080871570668962\n",
      "31 0.7487925650601149 0.8202105480665502 0.07978117024799126\n",
      "0.725867666699049 0.6330939185478073\n",
      "32 0.7333222389564171 0.8157544831563741 0.07727590615685025\n",
      "0.7273683267030446 0.6238117169210712\n",
      "33 0.7377798158871722 0.8172488160284519 0.0783335186385054\n",
      "0.7168753503871859 0.6438466581258411\n",
      "34 0.7332119058333567 0.8157724184720297 0.07908371083725763\n",
      "0.7346167370615677 0.6132604324946057\n",
      "35 0.7297021781105297 0.8159709271570545 0.07709716458253335\n",
      "0.7226456676050189 0.6034196412053119\n",
      "36 0.7313251828422981 0.816541859533417 0.07381746935292549\n",
      "0.7255666352571053 0.6343274104852954\n",
      "37 0.7374018642457988 0.8222253873996388 0.07780757895972377\n",
      "0.719000964504654 0.6392789832267048\n",
      "38 0.7270059904451984 0.8108051650068844 0.07555453778213612\n",
      "0.7165249641257915 0.622441067220264\n",
      "39 0.7343171997074956 0.8176011399107637 0.07647437735292487\n",
      "0.7252933383959439 0.6059206610895066\n",
      "40 0.7277926749474324 0.8119866183403904 0.07676663208247773\n",
      "0.7287106637460217 0.5947052927649913\n",
      "41 0.7359934176525535 0.8196096152109259 0.07970810089905384\n",
      "0.7236982478146032 0.6086030699587444\n",
      "42 0.7332704746969555 0.8165171936481896 0.07870857203597938\n",
      "0.7132197214861582 0.6443713846427424\n",
      "43 0.7301866082880781 0.8115161871016058 0.0806706561022553\n",
      "0.711071285509415 0.6373035170929797\n",
      "44 0.7374331707986646 0.8162545279690253 0.07900952162143372\n",
      "0.7142326878287701 0.6304804929443593\n",
      "45 0.7377828855817167 0.8186668579057995 0.07728294364220377\n",
      "0.7233269484061733 0.6304165212281718\n",
      "46 0.7361983063115181 0.8145927501303712 0.0766904381577075\n",
      "0.7401884563667006 0.6118554490230744\n",
      "47 0.7331331995282592 0.8142278536767059 0.07874073956749437\n",
      "0.734044704042353 0.5923872823054963\n",
      "48 0.7362743456595988 0.8138548586084283 0.07739760932256452\n",
      "0.7304571264508748 0.618095992362868\n",
      "49 0.7223086720895671 0.8131952981551586 0.07778366832231316\n",
      "0.7310402550837858 0.6059870686038131\n",
      "50 0.7352677466457862 0.8193528926708605 0.07711568226371475\n",
      "0.7293925612185899 0.6095815383206409\n",
      "51 0.7372650060027298 0.8181382942468357 0.07821312563532687\n",
      "0.7074840936147764 0.6495128924385559\n",
      "52 0.733077030388156 0.8121605360049333 0.07785458780416624\n",
      "0.717715044693195 0.6368996394074977\n",
      "53 0.7384625244637492 0.8191622148197987 0.07867130748780955\n",
      "0.7358521271927485 0.6061982229196314\n",
      "54 0.7336407446974065 0.8192669641663273 0.07830442071460873\n",
      "0.7240718164469637 0.6139502962285724\n",
      "55 0.7287546916574095 0.8114212630288788 0.07477408924451112\n",
      "0.7045307850195477 0.6459759736988626\n",
      "56 0.7333607704819457 0.8174628837257458 0.07938264681518212\n",
      "0.7314779898607575 0.6105844946227309\n",
      "57 0.7325740895026818 0.8111663673488166 0.08053539490867782\n",
      "0.7015793234339451 0.6394936980486281\n",
      "58 0.7355548210591012 0.8134193966161172 0.07753146922722301\n",
      "0.7145214630618315 0.6383577199540946\n",
      "59 0.7359613095651875 0.8155025018576727 0.0820562874243746\n",
      "0.7194369955829722 0.6351335821364841\n",
      "working on:  checkpointz\\to_slurm\\trans_latent128\\300_trans1x-128_peptide.ckpt \n",
      "\n",
      "analysis:  model_analyses\\train\\trans1x-128_peptide_latent128_train\\saved_info.csv checkpoint:  checkpointz\\to_slurm\\trans_latent128\\300_trans1x-128_peptide.ckpt\n",
      "trans1x-128_peptide\n",
      "0 0.5600414401094508 0.6800559405290499 0.02070180701110922\n",
      "0.8754129830340749 0.24480407565531692\n",
      "1 0.5644060876350038 0.6615681475293603 0.02421829500556679\n",
      "0.8599425777937842 0.32525799196479177\n",
      "2 0.5614048481722379 0.6855092305318856 0.023277512303681015\n",
      "0.8708015267389884 0.2968462548445472\n",
      "3 0.5650813304368653 0.6639152571085211 0.02298428977074156\n",
      "0.8818521383273439 0.25409072522989895\n",
      "4 0.565373477208113 0.6668850104097149 0.02524482960444468\n",
      "0.8682707885173472 0.26070148947870264\n",
      "5 0.5577844925062686 0.6626717446330453 0.023217595223819835\n",
      "0.8460006167940937 0.30922572562713224\n",
      "6 0.5549513611867176 0.6694851290539248 0.020319779807611293\n",
      "0.8743529575227225 0.29212561680685234\n",
      "7 0.5593607154370625 0.6731020447560601 0.02206651580550519\n",
      "0.8588010327443532 0.29907324673752145\n",
      "8 0.5606689292364676 0.6713132421922908 0.021427026170725776\n",
      "0.8655337309721003 0.26978598539697196\n",
      "9 0.5564985948861138 0.661880175761397 0.021442740928653857\n",
      "0.8478279705078436 0.29232854349820947\n",
      "10 0.5538169901907488 0.6544957516403594 0.024485214978538856\n",
      "0.8738175556337866 0.28187422049032995\n",
      "11 0.5623284902058672 0.6619632026229415 0.022249942874236314\n",
      "0.8577107555652539 0.2802883003479326\n",
      "12 0.5651545006962554 0.6803065596565342 0.021998541259159333\n",
      "0.8726240860532002 0.2806175687928447\n",
      "13 0.560078845808586 0.6572321162622718 0.022782388948527896\n",
      "0.8533085951980165 0.27499097550905527\n",
      "14 0.5595251254563544 0.6639865557502812 0.023830632242845637\n",
      "0.8498040361894981 0.2790873498772488\n",
      "15 0.5614853634678465 0.6690160068911447 0.022554355581560572\n",
      "0.8675916532295501 0.26744682396424224\n",
      "16 0.5659992100863782 0.6842442533514017 0.026894878951736128\n",
      "0.8633123332101125 0.2811363050009297\n",
      "17 0.5529497497746299 0.6551378571623638 0.022900656546577504\n",
      "0.8844832415817837 0.25211073634983017\n",
      "18 0.5593784356726679 0.6611516002680478 0.02218610374966528\n",
      "0.8398174252151512 0.254829558615147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 0.5550666769061712 0.6557326958735437 0.020235114932222836\n",
      "0.8445657761384945 0.28387680003204696\n",
      "20 0.5662718083515231 0.6722570601272969 0.02403781759409308\n",
      "0.850520322783454 0.26196777012879846\n",
      "21 0.561236661211829 0.6750677971805142 0.023102075566850233\n",
      "0.8544457243688836 0.27133326248368594\n",
      "22 0.5578307926852268 0.6775586365420037 0.022332266514222827\n",
      "0.8579352082430209 0.3069450595825183\n",
      "23 0.5585692536723637 0.6671497470797783 0.021150444123133175\n",
      "0.8581277323015811 0.28224037849269923\n",
      "24 0.5638134618316097 0.6639328088191799 0.029463443409943744\n",
      "0.8577085006740122 0.29485609878782415\n",
      "25 0.5586026263787798 0.6701164051710181 0.01882378814999057\n",
      "0.8688553297540081 0.23532253906749945\n",
      "26 0.5622491980681293 0.6735382276122215 0.025614233827660916\n",
      "0.8846927221871946 0.2416237573995884\n",
      "27 0.5582012019148854 0.6637289339332101 0.024064438552290394\n",
      "0.8600858333067722 0.28184081138273664\n",
      "28 0.5558587738607927 0.6630019674294734 0.020105503899200544\n",
      "0.8751893802911742 0.26278538680575303\n",
      "29 0.5598706766750241 0.6808976673227705 0.019708325042652627\n",
      "0.854573233498443 0.26958362806065406\n",
      "30 0.5589298028321436 0.6646764553941991 0.021736432740219927\n",
      "0.8701386819716657 0.2727503221629055\n",
      "31 0.564332514321689 0.6710449418301471 0.022449554248328327\n",
      "0.8655765768621315 0.26784678781976023\n",
      "32 0.5649675033279425 0.6777975794987228 0.021551709528080325\n",
      "0.8747371491278311 0.25419272594524367\n",
      "33 0.5609313847349815 0.6752134333471735 0.021323165739740826\n",
      "0.8677420863197063 0.25871686846482145\n",
      "34 0.553542741617657 0.6585072772728371 0.023376355727935776\n",
      "0.8722175055681194 0.2591147447361707\n",
      "35 0.562900762213311 0.6663376907710927 0.022781194237967137\n",
      "0.8478815138397415 0.3146693006730571\n",
      "36 0.5588087739672383 0.6737305306486239 0.02249589276107655\n",
      "0.8533717296418895 0.2614010442066913\n",
      "37 0.5632000276754996 0.6753006104536213 0.023552211791212762\n",
      "0.8446420479093241 0.2935184464478634\n",
      "38 0.5596510248406579 0.675599485891575 0.023199520249851748\n",
      "0.8822867593947146 0.2619227788344044\n",
      "39 0.5617423601524117 0.6783194042555248 0.02048151878636929\n",
      "0.8491875894202436 0.3031607955389799\n",
      "40 0.5569066588393528 0.6545841169494121 0.02372004654892625\n",
      "0.8556631747448331 0.25995436399797034\n",
      "41 0.5634663358214532 0.6694360386241119 0.022585451406624154\n",
      "0.8878524213185219 0.2751499538095963\n",
      "42 0.564321926522803 0.6727011792934456 0.025099628638865567\n",
      "0.8855716107636721 0.26304610040968535\n",
      "43 0.5584502518200631 0.676931924184492 0.02170277129027579\n",
      "0.8735001288042011 0.2584884511935206\n",
      "44 0.5601733810961486 0.664295973076537 0.02346989360958826\n",
      "0.8666282391257951 0.2641482916991571\n",
      "45 0.5612663366403116 0.6713515186601613 0.022235017279578998\n",
      "0.8915797199117041 0.2548324148222063\n",
      "46 0.566419745984226 0.6876264052620188 0.0256115137396019\n",
      "0.8716088163887049 0.2746700317506222\n",
      "47 0.5555063251017348 0.6535168284998326 0.023368905917411538\n",
      "0.8712531994379092 0.2837196297001898\n",
      "48 0.5572469768486258 0.6688900169785802 0.02333602765280644\n",
      "0.8751667646228396 0.25538847170761825\n",
      "49 0.5598236403524 0.6790966162116348 0.023079602300006264\n",
      "0.8869749765514141 0.2706530555046265\n",
      "50 0.558793095102378 0.6772395062510608 0.02157478163012691\n",
      "0.8628976409864099 0.2624142362549583\n",
      "51 0.5588121201465138 0.6584099535213577 0.020422214840547247\n",
      "0.8881573046219429 0.24693821418952466\n",
      "52 0.5600298099518837 0.6531957404932838 0.026774034216251037\n",
      "0.869270168827281 0.2893817610564934\n",
      "53 0.5631054855287401 0.679697932843868 0.02096422171727055\n",
      "0.8834389416843715 0.23864466062693068\n",
      "54 0.5610153029592263 0.6700840550971623 0.02246236207072824\n",
      "0.8433651058743025 0.31196957658021507\n",
      "55 0.5614547982386392 0.665112734277504 0.022935069851519323\n",
      "0.8574231973147073 0.28897833817908225\n",
      "56 0.5586305284681606 0.6787978878279144 0.021855833141877404\n",
      "0.8743764335898242 0.2534231515056996\n",
      "57 0.560007444783303 0.6666882722325046 0.02612389155031594\n",
      "0.8622933366969825 0.29020101048851477\n",
      "58 0.5590491202588336 0.6700578451376217 0.023262012366851565\n",
      "0.8456993049016663 0.262036313626828\n",
      "59 0.5622998936632992 0.6706766150694534 0.020721990263765463\n",
      "0.8630378787022512 0.26312917757689236\n",
      "working on:  checkpointz\\to_slurm\\trans_latent32\\300_trans1x-128_peptide.ckpt \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s_renaud\\Documents\\GitHub\\MSCSAM_TBD\\main_model\\transvae\\trans_models.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.params['CHAR_WEIGHTS'] = torch.tensor(self.params['CHAR_WEIGHTS'], dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analysis:  model_analyses\\train\\trans1x-128_peptide_latent32_train\\saved_info.csv checkpoint:  checkpointz\\to_slurm\\trans_latent32\\300_trans1x-128_peptide.ckpt\n",
      "trans1x-128_peptide\n",
      "0 0.6453332813066626 0.7554646706782072 0.04455793345781125\n",
      "0.6990180526743552 0.5257207200607777\n",
      "1 0.6428294923684736 0.7497910581796968 0.049724020831503464\n",
      "0.6853645924975136 0.569411569189422\n",
      "2 0.6312822500648472 0.7436854045926069 0.041060664219287245\n",
      "0.7054195699238168 0.4972882805204951\n",
      "3 0.6430149543959891 0.7519958037854999 0.04736058659011417\n",
      "0.7045147855973151 0.4968417333481028\n",
      "4 0.6366189663549922 0.7404671940343279 0.044522223680456795\n",
      "0.7093656367836176 0.4906615105237343\n",
      "5 0.6365811910241445 0.747570488303054 0.04343318861755578\n",
      "0.7024055728817902 0.5104729200914382\n",
      "6 0.6423954704488969 0.7509478320393823 0.04601478251515443\n",
      "0.7048157707175097 0.5049333208719553\n",
      "7 0.633541590825366 0.7454009451018545 0.04256844468448611\n",
      "0.6998347976657624 0.48584809501158754\n",
      "8 0.6377317780160432 0.7495748681991907 0.046158286649548685\n",
      "0.7032030130624624 0.5107234362457519\n",
      "9 0.6465380012776296 0.7534663502573574 0.046737637026213426\n",
      "0.7126360286770932 0.5222926879123286\n",
      "10 0.6354071619737124 0.7502557166939199 0.04636498044334481\n",
      "0.7199907899322688 0.4864263229532635\n",
      "11 0.6430995285608453 0.7489436058756879 0.044779153013074356\n",
      "0.7046215557565038 0.5170342616428804\n",
      "12 0.6406007375315897 0.7462351529991696 0.04655393895053032\n",
      "0.7169013364280574 0.48707303826366766\n",
      "13 0.6346489060283703 0.7456529860710566 0.04228555296055003\n",
      "0.7055008422607828 0.4944190628108127\n",
      "14 0.6372273958902575 0.7471118910995492 0.04526578867456683\n",
      "0.7104604496260815 0.48759658230487213\n",
      "15 0.6308110341297152 0.7421337291176423 0.041651999345353054\n",
      "0.7003553526704456 0.4901883056468225\n",
      "16 0.6347566283796836 0.7412060537737098 0.04497871242237072\n",
      "0.7046861813821466 0.48452597968183775\n",
      "17 0.6359664530667536 0.7449443544865043 0.04356729955416707\n",
      "0.7016887873581718 0.49412381863607113\n",
      "18 0.6419894420363624 0.7468192325453757 0.04845961385219075\n",
      "0.6936225013013824 0.51837958309909\n",
      "19 0.6346722665933229 0.7452511562522727 0.041171262403377776\n",
      "0.6961745691389238 0.4924387701288053\n",
      "20 0.6398818658231726 0.7450277654784608 0.04338722301519693\n",
      "0.703310134293926 0.5198475100590674\n",
      "21 0.6396045420677425 0.7435583934610226 0.04017163568529538\n",
      "0.7232650315564454 0.4945260869562529\n",
      "22 0.6433668184905118 0.7488360641971604 0.04656751445643626\n",
      "0.7124742105005357 0.4999508912716124\n",
      "23 0.6341121750171961 0.7444427735845797 0.04250754216027136\n",
      "0.7101567799145381 0.4922223264605782\n",
      "24 0.6414358980467318 0.7483739966935056 0.04821506383636818\n",
      "0.7211888007869008 0.47938172406629653\n",
      "25 0.6306685010673172 0.7359347804153221 0.0422716037385797\n",
      "0.677852959668293 0.5371265010243401\n",
      "26 0.6517343098129325 0.7597124759906124 0.048397276681874414\n",
      "0.7208390087753537 0.4754108364526791\n",
      "27 0.6477070162412863 0.7579296419258802 0.049110567618578196\n",
      "0.7437315195141381 0.4625332790550821\n",
      "28 0.6428247249057633 0.7508689928456838 0.049013242999462484\n",
      "0.7122002156698841 0.5190981894274844\n",
      "29 0.6421834877800848 0.7478752382339013 0.04745279648999483\n",
      "0.7235212141971652 0.5098774384034089\n",
      "30 0.6324472672724701 0.7353699230414226 0.03902468462384028\n",
      "0.6978927029856081 0.49861887466259447\n",
      "31 0.6412598631205314 0.7475905852774877 0.04704935572395322\n",
      "0.7021105776008923 0.508765139564957\n",
      "32 0.6385352804052332 0.7524420748731523 0.044769984522841866\n",
      "0.7003461145889545 0.4961708621783397\n",
      "33 0.6315294099094941 0.7372867785261799 0.04318569269570645\n",
      "0.682524152792424 0.5461500475762459\n",
      "34 0.6262610105758155 0.7370226317490071 0.041689140708184795\n",
      "0.6700422782733245 0.563232497189958\n",
      "35 0.6413297158206277 0.7544693434866518 0.04619515926866906\n",
      "0.7116751926112932 0.4813676815349842\n",
      "36 0.637569865780383 0.7465148516162665 0.04569475179792587\n",
      "0.7134549404794611 0.49183100863920726\n",
      "37 0.640622498934329 0.7531122997369175 0.04371101153540098\n",
      "0.7089476794005141 0.47213042136803973\n",
      "38 0.6424138846681334 0.748226606202665 0.04679781731925164\n",
      "0.7164987494005003 0.5145437011203962\n",
      "39 0.6406100595931004 0.7477223659495946 0.04644632992589838\n",
      "0.706358012098738 0.47681520620573326\n",
      "40 0.6511119860693876 0.7648297052442596 0.04808935296780619\n",
      "0.7236749030881222 0.49838701842046995\n",
      "41 0.6385364200020514 0.7496824485787489 0.040303271011284114\n",
      "0.7075220045293562 0.4718485727947874\n",
      "42 0.643490531039184 0.7568698705434119 0.049025800538952787\n",
      "0.721307342600048 0.480618270544076\n",
      "43 0.6366904676048625 0.7442036850546583 0.04451765792857236\n",
      "0.7178204834330961 0.5107138557737524\n",
      "44 0.6402048285152097 0.7547886820361128 0.04831699972024568\n",
      "0.7208499352035105 0.48926404121926814\n",
      "45 0.6377934996503619 0.7493211654744499 0.043635696761441974\n",
      "0.7135532355083891 0.48027008329471255\n",
      "46 0.6348927097260834 0.7399270083358019 0.043352126932180976\n",
      "0.6902016694724737 0.5043788426714206\n",
      "47 0.6426865120654852 0.7499031671368943 0.050332365804772204\n",
      "0.7081744506850354 0.4999432589926873\n",
      "48 0.6422114739624519 0.7521861251506715 0.04508884903255477\n",
      "0.7188978120914464 0.4946517771047818\n",
      "49 0.6433519075002538 0.7558202123841933 0.04306468794048499\n",
      "0.7627389874773114 0.4238918010878634\n",
      "50 0.6295065800892062 0.739332353861555 0.041542392462200005\n",
      "0.7162000127711181 0.4919025766634423\n",
      "51 0.635836116967332 0.7429834640786321 0.04070508495066\n",
      "0.7080413141430366 0.4788805211186066\n",
      "52 0.6402051644376429 0.7562125512636315 0.046992586305183846\n",
      "0.7056046867928366 0.5110745817492258\n",
      "53 0.6396722413219509 0.7475040387973163 0.04372633614984036\n",
      "0.7180123191938268 0.48773251944473217\n",
      "54 0.637064082015165 0.7464615330998238 0.04688079738057516\n",
      "0.6994733209154012 0.5222362651235111\n",
      "55 0.6374940768235211 0.7500266964804945 0.043316411316423414\n",
      "0.7177318740256641 0.4667434998466786\n",
      "56 0.6387826809057363 0.749593347421602 0.04594798524961599\n",
      "0.6963598697809092 0.531126075474532\n",
      "57 0.636036510144222 0.7419340229635709 0.040572507842857694\n",
      "0.7118526734947266 0.48453566167514106\n",
      "58 0.645520976965918 0.7538024109628091 0.043656864261043864\n",
      "0.7068221064354825 0.5209306103748642\n",
      "59 0.6417825444380929 0.7471664305098522 0.04664806746282303\n",
      "0.6931899248101352 0.5289496273805661\n",
      "working on:  checkpointz\\to_slurm\\trans_latent64\\300_trans1x-128_peptide.ckpt \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s_renaud\\Documents\\GitHub\\MSCSAM_TBD\\main_model\\transvae\\trans_models.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.params['CHAR_WEIGHTS'] = torch.tensor(self.params['CHAR_WEIGHTS'], dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analysis:  model_analyses\\train\\trans1x-128_peptide_latent64_train\\saved_info.csv checkpoint:  checkpointz\\to_slurm\\trans_latent64\\300_trans1x-128_peptide.ckpt\n",
      "trans1x-128_peptide\n",
      "0 0.6169087852848969 0.7028837435291807 0.031586502298726116\n",
      "0.8051474499231248 0.4137631353157405\n",
      "1 0.6175430044140191 0.7071603194631924 0.03617254179857269\n",
      "0.8048837507686272 0.436204928274859\n",
      "2 0.6142226116225626 0.692270100745721 0.030630754953477224\n",
      "0.8142773863917708 0.3978707584162944\n",
      "3 0.617400579450524 0.7065148481321993 0.029894115231895346\n",
      "0.8197502269754584 0.42551108700813256\n",
      "4 0.6191980132053323 0.7014379981626868 0.030760837071559344\n",
      "0.8192000491861904 0.4478087070444615\n",
      "5 0.6123737821161777 0.6995982111260445 0.027730488547327264\n",
      "0.8320196151350117 0.391228790559479\n",
      "6 0.6148351704589655 0.7057811094117592 0.03129681826336753\n",
      "0.8194484840363706 0.4329563575931845\n",
      "7 0.618754041179305 0.7051626260935218 0.030614317826026437\n",
      "0.8126321896869805 0.42679882312912565\n",
      "8 0.6179592091689013 0.6998015003786853 0.031129675332018398\n",
      "0.8233380200132745 0.39935866969477885\n",
      "9 0.6170355648792728 0.6974300422930895 0.03203762616709876\n",
      "0.8230197702702869 0.4391880148296198\n",
      "10 0.6219980982954554 0.7070879337743632 0.03208289338492159\n",
      "0.802018191649983 0.4519292111949659\n",
      "11 0.6207249659350081 0.7039253304067538 0.030951317753113623\n",
      "0.8043731400845191 0.44031687042039713\n",
      "12 0.6234032230567561 0.71373544353074 0.03354956956381722\n",
      "0.8198975646786579 0.40678153319089383\n",
      "13 0.6174313818071597 0.7064767726931601 0.031026767606918762\n",
      "0.8119026299781776 0.39952620762182545\n",
      "14 0.61231262877836 0.7071811768055232 0.027847067607304765\n",
      "0.8010587535422063 0.40791354434765303\n",
      "15 0.6206087389815533 0.7050974581253114 0.031597423789593806\n",
      "0.7962124402335883 0.4407495273141927\n",
      "16 0.6128814497595549 0.7015884133605372 0.030640842612631235\n",
      "0.8188721534000152 0.43865260521797744\n",
      "17 0.6257566794390202 0.7082655273808398 0.033799583905359504\n",
      "0.7883787952243634 0.45562251303484824\n",
      "18 0.6129394558282762 0.6998468448915423 0.02950226418267793\n",
      "0.8017590128710036 0.432062038400456\n",
      "19 0.624517764047199 0.7006752365631121 0.03217096548649078\n",
      "0.8107528600229331 0.3855143307516141\n",
      "20 0.6158638108551369 0.7000209887299927 0.027869904971879102\n",
      "0.8276100871480171 0.42356503165306547\n",
      "21 0.6185894506759329 0.7069033741885709 0.03294782990568495\n",
      "0.8266310759138986 0.41516277781870625\n",
      "22 0.6273558089942554 0.7177871508647641 0.03289134679457448\n",
      "0.8271946838933992 0.44450963683748124\n",
      "23 0.6176225533297157 0.7092375223514558 0.029524477985641798\n",
      "0.8248435923761261 0.4234567990335856\n",
      "24 0.6158212906715954 0.7007267685142351 0.033455857215558556\n",
      "0.8109158443293486 0.4260393946978771\n",
      "25 0.6177047751736712 0.706659070893688 0.030550020110742912\n",
      "0.7936200498614011 0.4751220498300752\n",
      "26 0.6185404521484965 0.6962206032512456 0.03450969585884401\n",
      "0.8386194888196741 0.4044064840866497\n",
      "27 0.6181697964255746 0.704783280387256 0.03227682673172213\n",
      "0.8234529509082591 0.4299726761483402\n",
      "28 0.6174945653190833 0.7106967053481407 0.030161811238705566\n",
      "0.8185225591342838 0.4328232455495925\n",
      "29 0.6159032112084761 0.6981074497590797 0.029004292426736165\n",
      "0.7999276277641498 0.43369236950234535\n",
      "30 0.6184386916389668 0.7090213965581447 0.031588216465622626\n",
      "0.8135006223596589 0.4109603164377472\n",
      "31 0.6219937543399686 0.7170912449972072 0.033359234889444686\n",
      "0.8083655493633111 0.4576021540618159\n",
      "32 0.620066428178084 0.7071109353475905 0.032574377123044077\n",
      "0.8082449988492005 0.42944543523128875\n",
      "33 0.6186700276920489 0.7073117285619606 0.03293546317541666\n",
      "0.8081954655530527 0.4242417050281644\n",
      "34 0.6112135908063691 0.6973138705401061 0.02910523236667633\n",
      "0.7886895496625651 0.45518360239631506\n",
      "35 0.6247351612494173 0.7051692808427555 0.03404698212551656\n",
      "0.8115433660997019 0.42982410098816026\n",
      "36 0.6169423513460701 0.7048559069350033 0.032231262162298775\n",
      "0.7963373368758676 0.4485492212100304\n",
      "37 0.6175634109023478 0.7028264766386045 0.03187178101050582\n",
      "0.7984023972487557 0.46125372275526\n",
      "38 0.6140414808822091 0.7097504309113408 0.028293504976575304\n",
      "0.8079885433631873 0.3981729084467127\n",
      "39 0.6194563518717704 0.7126275737778058 0.029084616352128945\n",
      "0.8215133529201261 0.42453326301703675\n",
      "40 0.6235075712281645 0.7036590380351483 0.034810338411150214\n",
      "0.8031493675239645 0.45091108537632174\n",
      "41 0.61473581112678 0.7047407371391325 0.02591396457188706\n",
      "0.8084568497914199 0.40700653852413604\n",
      "42 0.6136416986034134 0.7025071234123776 0.030142321655851914\n",
      "0.8187715485141374 0.408455023116736\n",
      "43 0.618796747890621 0.7037615333320957 0.03214600590191072\n",
      "0.8030510976611862 0.43353662519265657\n",
      "44 0.6187609508315615 0.701089768447751 0.029056132290940215\n",
      "0.8015955545791802 0.43092558137148895\n",
      "45 0.620605693046984 0.7090531779037187 0.031743496337207786\n",
      "0.7927764115081565 0.45806250486544586\n",
      "46 0.6161874350496644 0.7035407105583205 0.030148435200250567\n",
      "0.8045653165877016 0.4395721742299705\n",
      "47 0.6167930897688741 0.6978683649559589 0.032176149844651156\n",
      "0.7948818097083886 0.48375665921138145\n",
      "48 0.6212273309317492 0.6972441411200564 0.032500568084176416\n",
      "0.8236681455173602 0.42310452364225104\n",
      "49 0.6181855816147203 0.6984042796552755 0.0314915907012506\n",
      "0.8223781097417996 0.39838159444257026\n",
      "50 0.6215181949096487 0.7095294272735035 0.0306798579925381\n",
      "0.7885315684919258 0.44304889640214096\n",
      "51 0.6186028076739062 0.6978751718697065 0.029007169976842863\n",
      "0.8013642786968003 0.43256392165296853\n",
      "52 0.6174928544435964 0.7058489376438105 0.032807364349973056\n",
      "0.8170623790856493 0.4572760806834809\n",
      "53 0.6228574272207139 0.7045893209735098 0.03282227558434209\n",
      "0.8459891989604731 0.3683852066889267\n",
      "54 0.6146149065671465 0.6949659547631473 0.029989496372606488\n",
      "0.8183502466173728 0.42040256220922667\n",
      "55 0.6096237459168226 0.6932564388179001 0.02979181405058426\n",
      "0.8079889529662249 0.4577066735827853\n",
      "56 0.614997163702303 0.7092625995684879 0.029727763581170918\n",
      "0.8236973302566077 0.40743110026384255\n",
      "57 0.6216758662358298 0.7092800237858646 0.029718196854348598\n",
      "0.8194761539554382 0.37977315155445635\n",
      "58 0.6111140947414238 0.7084322934832833 0.028937368394724564\n",
      "0.8421662503763928 0.381647065305322\n",
      "59 0.620717209963307 0.7118036372689139 0.03323555351854352\n",
      "0.8052822337060404 0.46529338093856354\n",
      "working on:  checkpointz\\to_slurm\\wae_latent128\\300_wae-128_peptide.ckpt \n",
      "\n",
      "WAE class init called /n\n",
      "WAE class build_model called /n\n",
      "analysis:  model_analyses\\train\\wae-128_peptide_latent128_train\\saved_info.csv checkpoint:  checkpointz\\to_slurm\\wae_latent128\\300_wae-128_peptide.ckpt\n",
      "wae-128_peptide\n",
      "0 0.6515072088892288 0.7283748253210328 0.05399885292774592\n",
      "0.6356468570806102 0.6305104683507501\n",
      "1 0.6453695290576937 0.7142997656341344 0.0537290886364741\n",
      "0.6403331619588251 0.6335573377621446\n",
      "2 0.6511094313337967 0.7184562593026834 0.05463687102017115\n",
      "0.6424186613869152 0.6165551886971326\n",
      "3 0.6459132228759494 0.7187520110791582 0.05238630977393255\n",
      "0.6347791035266037 0.6135118284637963\n",
      "4 0.6480692001863124 0.7204053898526089 0.051369869344438636\n",
      "0.6454800720492873 0.639409017951619\n",
      "5 0.6463267690910935 0.7208965352552772 0.052507418129177384\n",
      "0.6422372016613188 0.6378243391732492\n",
      "6 0.6429691447492603 0.714379254365517 0.04841148981459427\n",
      "0.6251557447946098 0.6210377290967224\n",
      "7 0.6450278311798748 0.7145652836185596 0.05637667453926609\n",
      "0.6303400866724262 0.6307415991872332\n",
      "8 0.6491414987088631 0.7143013126338207 0.053781856075917595\n",
      "0.6426023787526454 0.6065457690580582\n",
      "9 0.6493579348402535 0.7182705260673473 0.052941248338446024\n",
      "0.6370121969466479 0.6413351582529427\n",
      "10 0.6430252814487065 0.7117979577974863 0.05090898862962305\n",
      "0.6259314747550132 0.6341916932972278\n",
      "11 0.6401370635523316 0.7129929861814418 0.05048686217898429\n",
      "0.6436339567512458 0.6238895729178934\n",
      "12 0.6467475419159197 0.7185668057294724 0.05378791773439236\n",
      "0.6372355238140714 0.636533683849523\n",
      "13 0.650483788717237 0.7221608590111147 0.050578077527922545\n",
      "0.6399545950065968 0.6478711573777285\n",
      "14 0.6493960836190432 0.7256544307976956 0.05277189592908801\n",
      "0.6438088152465827 0.6302458359618184\n",
      "15 0.6440699101010461 0.7159412996679492 0.05356812086463007\n",
      "0.6306567254621516 0.6335162057440091\n",
      "16 0.6500553455740188 0.7174725515345625 0.055770594393641064\n",
      "0.6264975208362414 0.6233045914339599\n",
      "17 0.6524812674477274 0.7255547522087118 0.05332412007279678\n",
      "0.6498890672477482 0.614820044656561\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 0.6441424931044991 0.7114321487261211 0.050670205702933395\n",
      "0.6447249085198223 0.6034435095229548\n",
      "19 0.6483796197682818 0.7213876362579544 0.05273141315500922\n",
      "0.6312655845055488 0.5963407610238591\n",
      "20 0.6539027659242432 0.7246044597142558 0.057479778189667464\n",
      "0.6436422624880416 0.6292376899263696\n",
      "21 0.6459289839276479 0.7128658220490899 0.052270778566103335\n",
      "0.6360641802503378 0.6176128924223505\n",
      "22 0.6480849557676365 0.7157652229217646 0.05169182443775347\n",
      "0.6573107332052995 0.5907333598608179\n",
      "23 0.6525131746520394 0.7312815651098803 0.05615070945382085\n",
      "0.6466409234690251 0.6332646992053848\n",
      "24 0.6537976291007945 0.723810076416517 0.05128387849777175\n",
      "0.640376372846698 0.6545248151335898\n",
      "25 0.6509400269117791 0.7189177326495519 0.054780515326713866\n",
      "0.6434732727065726 0.6106767350874192\n",
      "26 0.6448745695679784 0.7170250033593814 0.05291935070427535\n",
      "0.6395167247893919 0.6259601042403689\n",
      "27 0.6546074484340256 0.7248279256237935 0.054152641605741535\n",
      "0.6274657102910373 0.6546618422861096\n",
      "28 0.6535024932134986 0.725472017519509 0.05653197930416734\n",
      "0.6537073518229883 0.6206703582776911\n",
      "29 0.6468793809252716 0.7208743409529076 0.05123045698448703\n",
      "0.6360325251321901 0.6283940901680438\n",
      "30 0.6531728031920485 0.7228448908538777 0.054227242200479364\n",
      "0.6575194857527866 0.6176099441602667\n",
      "31 0.6515665593552546 0.7271036204671613 0.054371463383706235\n",
      "0.651217565635331 0.6244142382136135\n",
      "32 0.6455433769791294 0.711647179686201 0.05598132069499264\n",
      "0.6439855594118745 0.6125878408214116\n",
      "33 0.6485530693367869 0.7168420681482095 0.05683351520562057\n",
      "0.6428322686991708 0.6024919181211883\n",
      "34 0.6565319795087021 0.7308552100408523 0.056134533265596864\n",
      "0.6456941885898756 0.6162730736921984\n",
      "35 0.6495786053112561 0.7277624282859672 0.055367108022862876\n",
      "0.6337073661388491 0.6478065243637126\n",
      "36 0.6488390825167412 0.7255856030557173 0.05379423892935045\n",
      "0.6490237522760461 0.6254541149457208\n",
      "37 0.6475548001846906 0.7192657513377745 0.05311816317818667\n",
      "0.6346478274427656 0.6307089104633337\n",
      "38 0.6444205649982122 0.7260994017768051 0.05160846440300727\n",
      "0.6573574177616288 0.6215049477158366\n",
      "39 0.6438254904128308 0.714969802406783 0.0504816128381632\n",
      "0.6502094629936692 0.5947444535234059\n",
      "40 0.6467988657211518 0.7211598189539594 0.05333318882945577\n",
      "0.6346295131363412 0.6202576608870708\n",
      "41 0.6488457453796848 0.7183937584553096 0.051182732729973784\n",
      "0.6247314071851415 0.6389595733366713\n",
      "42 0.646469090558967 0.7149401768467755 0.05159444331832426\n",
      "0.6298551161754642 0.6589877327725728\n",
      "43 0.654765557584745 0.7227819078024249 0.05534417832670504\n",
      "0.6340205453419194 0.6320405628527734\n",
      "44 0.6494899471254275 0.7213825652699409 0.05232560265254123\n",
      "0.6335991319206478 0.6298636719562282\n",
      "45 0.6438571842484248 0.7141845944482134 0.05604680285488739\n",
      "0.6555075668638504 0.5985675778669765\n",
      "46 0.6468955806187942 0.712967751098164 0.05298743419237588\n",
      "0.6494290669205623 0.6045409185263795\n",
      "47 0.6562319552378906 0.7296527830710275 0.05499870324971556\n",
      "0.6485468190599636 0.6262600586169456\n",
      "48 0.6434576319147598 0.7116423484132475 0.04889284612872633\n",
      "0.6345146522059737 0.601195921862274\n",
      "49 0.6450620956210256 0.7186822685295452 0.05129699976257045\n",
      "0.6349983797453183 0.6359285058425512\n",
      "50 0.6501104752300791 0.7249602817337518 0.055122174720298005\n",
      "0.6397102564122437 0.6254241836446248\n",
      "51 0.6484758452290695 0.7186406410313485 0.051262550818242957\n",
      "0.6358132452901437 0.6229525255446844\n",
      "52 0.6485183796987111 0.716539244502866 0.05400958721177925\n",
      "0.6214476075572725 0.6219505565307799\n",
      "53 0.6415341490302212 0.7165168651517608 0.049891510693139546\n",
      "0.6255945925217592 0.6547343527945415\n",
      "54 0.6474504001242426 0.7190491006934063 0.05200859035263586\n",
      "0.6506295487119005 0.6343288566675505\n",
      "55 0.6413577316300698 0.7120180369514577 0.0494757748425856\n",
      "0.6287615999080511 0.6280965662342397\n",
      "56 0.650591283006005 0.7186030198352154 0.05328609347973552\n",
      "0.6383624922274368 0.61661671065716\n",
      "57 0.6494759740371092 0.723350616872346 0.05552743459032303\n",
      "0.645481921498203 0.6408114324953297\n",
      "58 0.6478475558440143 0.7176707325432096 0.05351663980278627\n",
      "0.6431974092690136 0.633650922612149\n",
      "59 0.6481502278226109 0.7223670345572576 0.05104260788860253\n",
      "0.6506799295795638 0.6207909301403259\n",
      "working on:  checkpointz\\to_slurm\\wae_latent32\\300_wae-128_peptide.ckpt \n",
      "\n",
      "WAE class init called /n\n",
      "WAE class build_model called /n\n",
      "analysis:  model_analyses\\train\\wae-128_peptide_latent32_train\\saved_info.csv checkpoint:  checkpointz\\to_slurm\\wae_latent32\\300_wae-128_peptide.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s_renaud\\Documents\\GitHub\\MSCSAM_TBD\\main_model\\transvae\\trans_models.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.params['CHAR_WEIGHTS'] = torch.tensor(self.params['CHAR_WEIGHTS'], dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wae-128_peptide\n",
      "0 0.6533861998094367 0.7596937381169351 0.056146647442919875\n",
      "0.5994927006355097 0.6745691373483793\n",
      "1 0.6542696644647356 0.7654182707127107 0.05216003689681034\n",
      "0.6186910585594534 0.6572056459501765\n",
      "2 0.6511256504348816 0.7515748722659992 0.05197557734904743\n",
      "0.5867188131629231 0.6856526360293453\n",
      "3 0.6622824985920739 0.768973813612257 0.055979147395225554\n",
      "0.6085858174944587 0.6568678414524509\n",
      "4 0.6561657108123882 0.7634690339676926 0.051409268591172924\n",
      "0.5874339760394758 0.6965821470240479\n",
      "5 0.6579963100665607 0.7640848055662842 0.057816329136517824\n",
      "0.583987814567066 0.6840533019372097\n",
      "6 0.656341726919733 0.7668112678511916 0.057376381915142\n",
      "0.5840982687464431 0.693343074001987\n",
      "7 0.657113913860048 0.7599543984900881 0.05494393107732777\n",
      "0.5836697726277515 0.6934413147183516\n",
      "8 0.6620320187041927 0.7656985513572914 0.05891806338823538\n",
      "0.6067730669257121 0.6666301392382901\n",
      "9 0.6535101792132849 0.76278801158917 0.05559883413195974\n",
      "0.590972064516188 0.6943269335666505\n",
      "10 0.655586336738069 0.7594008057525182 0.05517760553476669\n",
      "0.5946460629602485 0.6846904689611308\n",
      "11 0.6518993589262949 0.7658652530402932 0.055287443016955616\n",
      "0.5992466749212084 0.6712439131277914\n",
      "12 0.6569805304031217 0.762731332650929 0.05212537896333078\n",
      "0.5862068445664188 0.6720091950139213\n",
      "13 0.6493350596219325 0.7560203822549623 0.055126086755950765\n",
      "0.5761728053722719 0.7022077126949786\n",
      "14 0.651735005325087 0.7615847580286375 0.057494196374008974\n",
      "0.5900626107276152 0.6843568802237423\n",
      "15 0.6638748741468822 0.7712408947751029 0.057719026793144605\n",
      "0.5976613529306243 0.6885927707196202\n",
      "16 0.6503660043554267 0.7566916894905308 0.053612343864076366\n",
      "0.5870902186130946 0.7018713127436994\n",
      "17 0.6586261013399243 0.7616277775759893 0.05918482970307959\n",
      "0.5922661994135268 0.678094827783865\n",
      "18 0.6546587845754785 0.7630421421937327 0.054914090170907225\n",
      "0.6092590222561075 0.6558825162829595\n",
      "19 0.6524278462059658 0.7568637874626511 0.05294006789443013\n",
      "0.6058127007820873 0.6612494311567787\n",
      "20 0.6506376133543398 0.7571196766755133 0.05349652685204857\n",
      "0.588731322775328 0.6775799974589464\n",
      "21 0.6581927575849486 0.7653418583461169 0.05436225198388006\n",
      "0.5959702449785154 0.6682911782152643\n",
      "22 0.6555036274223894 0.7658988902352822 0.05810918234670161\n",
      "0.6102567999656154 0.6552971379073724\n",
      "23 0.6537152780437737 0.7560990780650964 0.052068810511618165\n",
      "0.5748390528160625 0.6951684932693716\n",
      "24 0.6498166582826191 0.7603464939526172 0.05364417309464092\n",
      "0.6009517090869954 0.6705387376940634\n",
      "25 0.6567732416701307 0.7603184166024336 0.0555684851691966\n",
      "0.5888775071062652 0.6988048723448137\n",
      "26 0.6578992008967304 0.7653484559255365 0.05549018747787526\n",
      "0.5884526547054161 0.6957803949373784\n",
      "27 0.654428568052405 0.7626418413923509 0.05167864134381145\n",
      "0.5841652342999692 0.6856031356450161\n",
      "28 0.6604404006267691 0.769288962791616 0.05818990655891615\n",
      "0.6242217670773968 0.6610602006952411\n",
      "29 0.6575342673624022 0.7640767007567105 0.05631444114676819\n",
      "0.5850491952153762 0.6918068678228528\n",
      "30 0.6528590472587774 0.7583881797719627 0.050371718727048535\n",
      "0.6223437190665448 0.6254518647513897\n",
      "31 0.6525669777884271 0.7586037070283751 0.052311487278741554\n",
      "0.6114346671349806 0.6602842618403375\n",
      "32 0.6640598463302135 0.7671834401666765 0.05662249069219643\n",
      "0.6053782741065941 0.6771712992597656\n",
      "33 0.6604899098289938 0.7662635651991128 0.055830319299268676\n",
      "0.5850327193698788 0.6816150415116354\n",
      "34 0.6567977558703657 0.7679329277725714 0.05577474876721444\n",
      "0.6001096888102944 0.6648270361739462\n",
      "35 0.6582416621033791 0.7625550203914404 0.0545813339833788\n",
      "0.602763263501866 0.6613447944010169\n",
      "36 0.6601752292652937 0.7655189668535817 0.06017877914725864\n",
      "0.5932671631639643 0.687250988338391\n",
      "37 0.6549743112967361 0.7611734714381679 0.051962212941552954\n",
      "0.5998426264216907 0.6747485945282546\n",
      "38 0.6542309717042344 0.7642539326270149 0.05218865282073631\n",
      "0.5950606486802852 0.6643381594192646\n",
      "39 0.6568498562186841 0.7628821650302826 0.056946787869699464\n",
      "0.5810097636790177 0.6945661287704945\n",
      "40 0.6585134208341302 0.7670710754391935 0.0522269908745753\n",
      "0.60717378697952 0.6627337181484043\n",
      "41 0.6516673248081307 0.7609196154987676 0.0547349730836511\n",
      "0.600401903555358 0.6702566303971018\n",
      "42 0.6521998109394037 0.7588154333645888 0.05089648336396931\n",
      "0.5913250362324962 0.6806707525290601\n",
      "43 0.6550278601748553 0.7638569534081606 0.056491987863787424\n",
      "0.5857118544077923 0.6875890261597766\n",
      "44 0.6482823816957851 0.7564257445666505 0.04850204931148522\n",
      "0.5869393587999203 0.6680149608567534\n",
      "45 0.6556574534776306 0.7604171488738785 0.05176831500191051\n",
      "0.6137819733131997 0.6511659370946835\n",
      "46 0.6595146065954746 0.7650899826035694 0.055270511563416606\n",
      "0.5971947068763268 0.6802731012364911\n",
      "47 0.6540189971283699 0.7603444906229462 0.05386993658034101\n",
      "0.5879490448596218 0.675974347483277\n",
      "48 0.6595173386435923 0.7584518069409213 0.05526810086688979\n",
      "0.5913645790296505 0.7028575853947802\n",
      "49 0.6611996338481374 0.7698413129325299 0.055622556005573934\n",
      "0.6173487275055799 0.6633331347525555\n",
      "50 0.6511811569722373 0.7565422994639094 0.05278858901722389\n",
      "0.6088317012781161 0.6452252733455393\n",
      "51 0.6571614421303391 0.7630566160467637 0.05481440205494917\n",
      "0.580694382059253 0.6918059559664135\n",
      "52 0.6608778795024165 0.765220586269328 0.058810008794371704\n",
      "0.5891605296442206 0.6913537849136642\n",
      "53 0.6553625031473843 0.7618074971239944 0.05717137278556385\n",
      "0.6042302373457424 0.6684702277433419\n",
      "54 0.6544668128724862 0.7643421526559352 0.054564438644571056\n",
      "0.6083569646374639 0.6681771944311672\n",
      "55 0.657500273639731 0.7619085673184863 0.05217645895341324\n",
      "0.5873783891602142 0.6916053007991207\n",
      "56 0.6486163248328932 0.7613637218826234 0.052055309940561845\n",
      "0.5948298296040198 0.6749797666868367\n",
      "57 0.6516843204961482 0.7582834336377094 0.055311866636907035\n",
      "0.592559378286576 0.6848471566700092\n",
      "58 0.6587728371609747 0.7635786200394076 0.05607800457209406\n",
      "0.5877626007436624 0.6810308517171981\n",
      "59 0.6664348122821261 0.7727796778021196 0.06031209924258196\n",
      "0.5887385398138762 0.704036429035191\n",
      "working on:  checkpointz\\to_slurm\\wae_latent64\\300_wae-128_peptide.ckpt \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s_renaud\\Documents\\GitHub\\MSCSAM_TBD\\main_model\\transvae\\trans_models.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.params['CHAR_WEIGHTS'] = torch.tensor(self.params['CHAR_WEIGHTS'], dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WAE class init called /n\n",
      "WAE class build_model called /n\n",
      "analysis:  model_analyses\\train\\wae-128_peptide_latent64_train\\saved_info.csv checkpoint:  checkpointz\\to_slurm\\wae_latent64\\300_wae-128_peptide.ckpt\n",
      "wae-128_peptide\n",
      "0 0.656731186351523 0.7580854165275753 0.04881063153277089\n",
      "0.672706665014953 0.5698886616079087\n",
      "1 0.6623077458451045 0.7619479843679026 0.056134988269557526\n",
      "0.6541197677478143 0.5927507440935762\n",
      "2 0.661222415887066 0.7583983315776829 0.053471539501130254\n",
      "0.6309764245222632 0.6423795107115589\n",
      "3 0.6605614764713568 0.7591193859779423 0.05321049298052406\n",
      "0.650097904302688 0.6158396651997711\n",
      "4 0.6617669373434427 0.7607279326074695 0.0500636004213824\n",
      "0.6729080288858822 0.5899021783191438\n",
      "5 0.6572279565882697 0.7602145836899937 0.05257734780967844\n",
      "0.637566133394039 0.631234162893825\n",
      "6 0.6610278486813069 0.76167923525218 0.056205421445351275\n",
      "0.6597238067919484 0.6076062590185283\n",
      "7 0.6528714868218102 0.7525992309753172 0.05118277361276518\n",
      "0.6405134058792176 0.6087525955044328\n",
      "8 0.6579409015497933 0.7583268765598168 0.05459051651324857\n",
      "0.6699158904362291 0.5888288833388414\n",
      "9 0.6649153092676157 0.7736490419201534 0.0539598855423535\n",
      "0.6794363206714926 0.5966493152473666\n",
      "10 0.6622955540266168 0.7594342946111439 0.05120352828701397\n",
      "0.6586077694861446 0.6039160053599038\n",
      "11 0.6645870039851767 0.7615352230883636 0.05347467164975172\n",
      "0.6577132649968045 0.5970889118681904\n",
      "12 0.6657654562272962 0.7709347763709451 0.05790693442530056\n",
      "0.6376169119466049 0.6447583416203053\n",
      "13 0.6551221821414586 0.7535158221580494 0.05127912359865382\n",
      "0.6480660974634311 0.6019803962136154\n",
      "14 0.6670837747204668 0.7613141671867433 0.05932820907480719\n",
      "0.6399558179648612 0.634932730014299\n",
      "15 0.6660096811405077 0.7653760101558374 0.054978664606396534\n",
      "0.6725848162134732 0.5801909869572184\n",
      "16 0.6568273193520393 0.759337417529804 0.05260795854468106\n",
      "0.6713446657768585 0.5816709398246398\n",
      "17 0.6635788500919862 0.7653210004605882 0.0525972705150464\n",
      "0.667590902682432 0.5925810551171273\n",
      "18 0.663290692661083 0.7585701732819662 0.054987541327918145\n",
      "0.6475974519798566 0.624979193423689\n",
      "19 0.6667163969259258 0.7687517902826134 0.05704253522790973\n",
      "0.6594927798445409 0.6232022876370344\n",
      "20 0.6617373610156169 0.760680727720486 0.0548899294142994\n",
      "0.648788286136871 0.6274693450140909\n",
      "21 0.6653036984647476 0.7658426142415078 0.055709094016399174\n",
      "0.6550294816719927 0.6279530173813314\n",
      "22 0.6535158439550357 0.7515388976354429 0.051580385380878876\n",
      "0.6385210827336318 0.6081820892098823\n",
      "23 0.6616208245202699 0.7648170094189646 0.056121789782930115\n",
      "0.6382166843878616 0.616048709439453\n",
      "24 0.666100478142432 0.7648753801380362 0.056165177557053976\n",
      "0.6627314909112572 0.5996145800328985\n",
      "25 0.6628590202310696 0.7623833840259616 0.054152218355481826\n",
      "0.6605663607821822 0.6016344516690904\n",
      "26 0.6651095482064865 0.7666737051802575 0.05670541274572268\n",
      "0.6688367616675595 0.5871381572466996\n",
      "27 0.6684087206071636 0.7718174420964637 0.057464033154287227\n",
      "0.6608699793466227 0.6154736903251399\n",
      "28 0.6666771326548735 0.7646055653113364 0.054828696303748516\n",
      "0.6666243690781741 0.5989580239498403\n",
      "29 0.6670900145102286 0.7669630360028169 0.05651794601399065\n",
      "0.6397493398071412 0.6258470450942144\n",
      "30 0.6620439342947714 0.758832032350003 0.054201280089615715\n",
      "0.659687868504244 0.5992972093662212\n",
      "31 0.6649567536980963 0.7590797133743583 0.055648627114798656\n",
      "0.6564607830290432 0.6151557812229951\n",
      "32 0.6524648358634416 0.7537483064833874 0.05025866909902834\n",
      "0.644900237833233 0.612953316337669\n",
      "33 0.6618977867284389 0.759951263852667 0.05295810487352084\n",
      "0.649239806710267 0.613254939219154\n",
      "34 0.6669490123542393 0.768163268517956 0.05635204578958675\n",
      "0.6637002838062933 0.5934717094332755\n",
      "35 0.663476148228189 0.7667269224648838 0.05701880221627244\n",
      "0.6477067542456392 0.6320867732609425\n",
      "36 0.6630072291235456 0.7580658565392715 0.05410445301547584\n",
      "0.646346125381154 0.6152163188122438\n",
      "37 0.663579032110266 0.7601878589588152 0.05452035439532582\n",
      "0.6508040263968032 0.6124829435079264\n",
      "38 0.6658518476517997 0.7622834951457155 0.056248533988565275\n",
      "0.6487765304930391 0.6502028735233574\n",
      "39 0.6652042735279595 0.7666681170695567 0.05340145329533084\n",
      "0.6278882676449447 0.6475657598453057\n",
      "40 0.6547259224106323 0.7520373803279472 0.04872996834822559\n",
      "0.6430220623014674 0.615104253741586\n",
      "41 0.6546143819414066 0.7561838203527386 0.05357607579029605\n",
      "0.6695091893110141 0.6221473215400615\n",
      "42 0.6696312350125497 0.7686749269708842 0.0575903667976714\n",
      "0.6679415888564477 0.6037430302230333\n",
      "43 0.6549314147314211 0.7517258679947076 0.053364534625635715\n",
      "0.6430580404503408 0.5870801699401589\n",
      "44 0.663017046681495 0.7619129508000264 0.05683655326233251\n",
      "0.6356050346572822 0.6163978767235057\n",
      "45 0.657825387233439 0.7563736392558534 0.05465689814798737\n",
      "0.6625205817835479 0.5944015980521529\n",
      "46 0.6665461012265789 0.765673821217481 0.05572782960427406\n",
      "0.6650699340829369 0.5963282410599515\n",
      "47 0.6651911993571404 0.7611650501519429 0.05385955440732282\n",
      "0.6422446389108206 0.6299887851868173\n",
      "48 0.6603723177534222 0.7587943142320009 0.05175404081461657\n",
      "0.6571238996002899 0.5998890899672444\n",
      "49 0.6580020707016704 0.7582500889435105 0.0546911028786108\n",
      "0.6491311986991999 0.6006963781017965\n",
      "50 0.6646223393233578 0.7648239554155307 0.053886062577072416\n",
      "0.660524486563917 0.6119337427939964\n",
      "51 0.6675772187933052 0.7698513655147277 0.056268249446978155\n",
      "0.6750872983469317 0.5844062576632132\n",
      "52 0.6667302606159259 0.7590504874967758 0.05580627505026816\n",
      "0.6703150997381114 0.5721993841886239\n",
      "53 0.6604973658255516 0.7574518599909402 0.054672406940302426\n",
      "0.6512185324233466 0.595016928046992\n",
      "54 0.6669758164186372 0.7645375147273359 0.05756375910975144\n",
      "0.6609538381148304 0.5811121934309376\n",
      "55 0.6666843758049581 0.7619425375607014 0.055156437838397646\n",
      "0.6515242667413057 0.6206829600003456\n",
      "56 0.6572182645611898 0.7580140569519731 0.05571868726014965\n",
      "0.6501898978642358 0.630038415086852\n",
      "57 0.6566589245249923 0.7572112079146872 0.05258729500856641\n",
      "0.6483675800158484 0.6118535014897714\n",
      "58 0.6651129511453324 0.7642280882311211 0.055228225716868036\n",
      "0.6431294043475297 0.629355306785297\n",
      "59 0.6501141186954785 0.7507772994335941 0.0501554786375137\n",
      "0.6232574580213148 0.6322087234500466\n"
     ]
    }
   ],
   "source": [
    "import coranking #coranking.readthedocs.io\n",
    "from coranking.metrics import trustworthiness, continuity, LCMC\n",
    "from transvae.snc import SNC #github.com/hj-n/steadiness-cohesiveness\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "from transvae import trans_models\n",
    "from transvae.transformer_models import TransVAE\n",
    "from transvae.rnn_models import RNN, RNNAttn\n",
    "from transvae.wae_models import WAE\n",
    "from transvae.aae_models import AAE\n",
    "from transvae.tvae_util import *\n",
    "from transvae import analysis\n",
    "import glob\n",
    "import re\n",
    "\n",
    "\n",
    "gpu = True\n",
    "\n",
    "example_data = 'data\\\\peptides\\\\datasets\\\\uniprot_v2\\\\peptide_train.txt'\n",
    "test_train='train'\n",
    "ckpt_list = glob.glob(\"\"+\"checkpointz\\\\to_slurm//**//*.ckpt\", recursive = True) #grab all checkpoints\n",
    "analyses_list = glob.glob(\"model_analyses\\\\train//**/*.csv\", recursive=True) #grab all analyses\n",
    "print('current working directory: ',os.getcwd())\n",
    "\n",
    "for i in range(len(ckpt_list)):\n",
    "    \n",
    "    #search the current directory for the model name and load that model\n",
    "    model_dic = {'trans':'TransVAE','aae':'AAE','rnnattn':'RNNAttn','rnn':'RNN','wae':'WAE'}\n",
    "    model_src = ckpt_list[i]\n",
    "    print('working on: ',model_src,'\\n')\n",
    "    model_name = list(filter(None,[key for key in model_dic.keys() if key in model_src.split('//')[-1]]))\n",
    "    model = locals()[model_dic[model_name[0]]](load_fn=model_src) #use locals to call model specific constructor\n",
    "    \n",
    "    #load the analysis file corresponding to the model from the CC outputs\n",
    "    for idx in range(len(analyses_list)):\n",
    "        if analyses_list[idx].split(\"\\\\\")[-2].find(model_src.split(\"\\\\\")[-2].split(\"_\")[0]) != -1 and analyses_list[idx].split(\"\\\\\")[-2].find(model_src.split(\"\\\\\")[-2].split(\"_\")[1]) != -1:\n",
    "            if analyses_list[idx].find(\"rnnattn\")  != -1 and model_src.find(\"rnnattn\") == -1: continue\n",
    "            save_dir = analyses_list[idx]\n",
    "            cur_analysis = pd.read_csv(save_dir)\n",
    "    print(\"analysis: \",save_dir, \"checkpoint: \",model_src)\n",
    "    save_df = cur_analysis #this will hold the number variables and save to CSV\n",
    "    \n",
    "    #load the true labels\n",
    "    data = pd.read_csv(example_data).to_numpy() \n",
    "    data_1D = data[:,0] #gets rid of extra dimension\n",
    "    \n",
    "    #moving into memory and entropy\n",
    "    if model.model_type =='aae':\n",
    "        mus, _, _ = model.calc_mems(data[:60_000], log=False, save=False) \n",
    "    elif model.model_type == 'wae':\n",
    "        mus, _, _ = model.calc_mems(data[:60_000], log=False, save=False) \n",
    "    else:\n",
    "        mems, mus, logvars = model.calc_mems(data[:60_000], log=False, save=False) #subset size 1200*35=42000 would be ok\n",
    "\n",
    "    #create random index and re-index ordered memory list creating n random sub-lists (ideally resulting in IID random lists)\n",
    "    random_idx = np.random.permutation(np.arange(stop=mus.shape[0]))\n",
    "    mus[:] = mus[random_idx]\n",
    "    data = data[random_idx]\n",
    "    \n",
    "    #need to perform PCA to be able to compare dimensionality reduction quality\n",
    "    pca = PCA(n_components=2)\n",
    "    pca_batch =pca.fit_transform(X=mus[:])\n",
    "    \n",
    "    #now ready to calculation dimensionality reduction accuracy with metrics\n",
    "    trust_subsamples = []\n",
    "    cont_subsamples = []\n",
    "    lcmc_subsamples = []\n",
    "    steadiness_subsamples = []\n",
    "    cohesiveness_subsamples = []\n",
    "    if 'test' in test_train: #different number of bootsraps for train vs test\n",
    "        n=15\n",
    "    else:\n",
    "        n=60\n",
    "    parameter = { \"k\": 50,\"alpha\": 0.1 } #for steadiness and cohesiveness\n",
    "    for s in range(n):\n",
    "        s_len = len(mus)//n\n",
    "        Q = coranking.coranking_matrix(mus[s_len*s:s_len*(s+1)], pca_batch[s_len*s:s_len*(s+1)])\n",
    "        trust_subsamples.append( np.mean(trustworthiness(Q, min_k=1, max_k=50)) )\n",
    "        cont_subsamples.append( np.mean(continuity(Q, min_k=1, max_k=50)) )\n",
    "        lcmc_subsamples.append( np.mean(LCMC(Q, min_k=1, max_k=50)) )\n",
    "        print(s,trust_subsamples[s],cont_subsamples[s],lcmc_subsamples[s])\n",
    "\n",
    "        metrics = SNC(raw=mus[s_len*s:s_len*(s+1)], emb=pca_batch[s_len*s:s_len*(s+1)], iteration=300, dist_parameter=parameter)\n",
    "        metrics.fit() #solve for steadiness and cohesiveness\n",
    "        steadiness_subsamples.append(metrics.steadiness())\n",
    "        cohesiveness_subsamples.append(metrics.cohesiveness())\n",
    "        print(metrics.steadiness(),metrics.cohesiveness())\n",
    "        Q=0 #trying to free RAM\n",
    "        metrics=0\n",
    "        torch.cuda.empty_cache() #free allocated CUDA memory\n",
    "\n",
    "    save_df = pd.concat([save_df,pd.DataFrame({'latent_to_PCA_trustworthiness':trust_subsamples})], axis=1)\n",
    "    save_df = pd.concat([save_df,pd.DataFrame({'latent_to_PCA_continuity':cont_subsamples})], axis=1)\n",
    "    save_df = pd.concat([save_df,pd.DataFrame({'latent_to_PCA_lcmc':lcmc_subsamples})], axis=1)\n",
    "    save_df = pd.concat([save_df,pd.DataFrame({'latent_to_PCA_steadiness':steadiness_subsamples})], axis=1)\n",
    "    save_df = pd.concat([save_df,pd.DataFrame({'latent_to_PCA_cohesiveness':cohesiveness_subsamples})], axis=1)  \n",
    "    \n",
    "    save_df.to_csv(save_dir, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5c006d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
