{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1d136af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current working directory:  C:\\Users\\s_renaud\\Documents\\GitHub\\MSCSAM_TBD\\main_model\n",
      "working on:  temp_ckpt\\rnn_latent128\\300_rnn-128_peptide.ckpt \n",
      "\n",
      "log_rnn-128_peptide.txt temp_ckpt\\rnn_latent128\\log_rnn-128_peptide.txt\n",
      "rnn-128_peptide\n",
      "cuda\n",
      "decoding sequences of max length  125 current position:  0\n",
      "decoding sequences of max length  125 current position:  10\n",
      "decoding sequences of max length  125 current position:  20\n",
      "decoding sequences of max length  125 current position:  30\n",
      "decoding sequences of max length  125 current position:  40\n",
      "decoding sequences of max length  125 current position:  50\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14756/3314713543.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    122\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'BATCH_SIZE'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mreconstruct\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m         \u001b[0mreconstructed_seq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprops\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreconstruct\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mnum_sequences\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_mems\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    125\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m         data, data_1D, true_props, props, reconstructed_seq = load_reconstructions(data, data_1D,latent_size,\n",
      "\u001b[1;32m~\\Documents\\GitHub\\MSCSAM_TBD\\main_model\\transvae\\trans_models.py\u001b[0m in \u001b[0;36mreconstruct\u001b[1;34m(self, data, method, log, return_mems, return_str)\u001b[0m\n\u001b[0;32m    641\u001b[0m                     \u001b[1;31m### Decode logic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    642\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'greedy'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 643\u001b[1;33m                         \u001b[0mdecoded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgreedy_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msrc_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    644\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    645\u001b[0m                         \u001b[0mdecoded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\MSCSAM_TBD\\main_model\\transvae\\trans_models.py\u001b[0m in \u001b[0;36mgreedy_decode\u001b[1;34m(self, mem, src_mask)\u001b[0m\n\u001b[0;32m    560\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    561\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 562\u001b[1;33m                 \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtgt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    563\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    564\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\MSCSAM_TBD\\main_model\\transvae\\rnn_models.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, tgt, mem)\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtgt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 220\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtgt_embed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtgt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmem\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#change back to src\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    221\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict_property\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrue_prop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\amp21\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\MSCSAM_TBD\\main_model\\transvae\\rnn_models.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, tgt, mem)\u001b[0m\n\u001b[0;32m    389\u001b[0m         \u001b[0mmem\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    390\u001b[0m         \u001b[0mmem\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 391\u001b[1;33m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgru\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    392\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    393\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\amp21\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\amp21\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    847\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    848\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 849\u001b[1;33m             result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0m\u001b[0;32m    850\u001b[0m                              self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0;32m    851\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "from transvae import trans_models\n",
    "from transvae.transformer_models import TransVAE\n",
    "from transvae.rnn_models import RNN, RNNAttn\n",
    "from transvae.wae_models import WAE\n",
    "from transvae.aae_models import AAE\n",
    "from transvae.tvae_util import *\n",
    "from transvae import analysis\n",
    "import glob\n",
    "import re\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import Isomap\n",
    "from sklearn import metrics\n",
    "from sklearn.manifold import trustworthiness\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import NullFormatter\n",
    "import plotly.express as px\n",
    "\n",
    "import coranking #coranking.readthedocs.io\n",
    "from coranking.metrics import trustworthiness, continuity, LCMC\n",
    "from transvae.snc import SNC #github.com/hj-n/steadiness-cohesiveness\n",
    "\n",
    "def loss_plots(loss_src):\n",
    "    tot_loss = analysis.plot_loss_by_type(src,loss_types=['tot_loss'])\n",
    "    plt.savefig(save_dir+'tot_loss.png')\n",
    "    recon_loss = analysis.plot_loss_by_type(src,loss_types=['recon_loss'])\n",
    "    plt.savefig(save_dir+'recon_loss.png')\n",
    "    kld_loss = analysis.plot_loss_by_type(src,loss_types=['kld_loss'])\n",
    "    plt.savefig(save_dir+'kld_loss.png')\n",
    "    prob_bce_loss = analysis.plot_loss_by_type(src,loss_types=['prop_bce_loss'])\n",
    "    plt.savefig(save_dir+'prob_bce_loss.png')\n",
    "    if 'aae' in src:\n",
    "        disc_loss = analysis.plot_loss_by_type(src,loss_types=['disc_loss'])\n",
    "        plt.savefig(save_dir+'disc_loss.png')\n",
    "    if 'wae' in src:\n",
    "        mmd_loss = analysis.plot_loss_by_type(src,loss_types=['mmd_loss'])\n",
    "        plt.savefig(save_dir+'mmd_loss.png')\n",
    "    plt.close('all')\n",
    "    \n",
    "def load_reconstructions(data,data_1D,latent_size, load_src, true_props=None,subset=None):\n",
    "    \n",
    "    recon_src = load_src+model.name+\"_\"+re.split('(\\d{2,3})',latent_size[0])[0]+\"_\"+re.split('(\\d{2,3})',latent_size[0])[1]+\"//saved_info.csv\"\n",
    "    recon_df = pd.read_csv(recon_src)\n",
    "    reconstructed_seq = recon_df['reconstructions'].to_list()[:num_sequences]\n",
    "    props = torch.Tensor(recon_df['predicted properties'][:num_sequences])\n",
    "    true_props_data = pd.read_csv(true_props).to_numpy()\n",
    "    true_props = true_props_data[0:num_sequences,0]\n",
    "    \n",
    "    if subset:\n",
    "        testing = pd.read_csv(subset).to_numpy()\n",
    "        test_idx_list = [np.where(data==testing[idx][0]) for idx in range(len(testing))]\n",
    "\n",
    "\n",
    "        batch_recon_len = len(reconstructed_seq)\n",
    "        reconstructed_seq = [reconstructed_seq[test_idx_list[i][0][0]] for i in range(len(test_idx_list)) if test_idx_list[i][0][0]<batch_recon_len]\n",
    "        data_1D= [data_1D[test_idx_list[i][0][0]] for i in range(len(test_idx_list)) if test_idx_list[i][0][0]<batch_recon_len]\n",
    "        props = [props[test_idx_list[i][0][0]] for i in range(len(test_idx_list)) if test_idx_list[i][0][0]<batch_recon_len]\n",
    "        props=torch.Tensor(props)\n",
    "        data = testing[:][0]\n",
    "        true_props_data = pd.read_csv(true_props).to_numpy()\n",
    "        true_props = true_props_data[0:num_sequences,0]\n",
    "        true_props= [true_props[test_idx_list[i][0][0]] for i in range(len(test_idx_list)) if test_idx_list[i][0][0]<batch_recon_len]\n",
    "\n",
    "    return data, data_1D, true_props, props, reconstructed_seq\n",
    "\n",
    "########################################################################################\n",
    "gpu = True\n",
    "\n",
    "num_sequences = 500#_000\n",
    "batch_size = 200 #setting for reconstruction\n",
    "example_data = 'slurm_analyses//data//sunistar//peptide_train.txt'\n",
    "save_dir_loc = 'slurm_analyses' #folder in which to save outpts\n",
    "save_dir_name = 'train' #appended to identify data: train|test|other|etc...\n",
    "\n",
    "reconstruct=True #True:reconstruct data here; False:load reconstructions from file\n",
    "recon_src = \"checkpointz//analyses_ckpts//\" #directory in which all reconstructions are stored\n",
    "true_prop_src = \"slurm_analyses//data//sunistar//function_train.txt\" #if property predictor load the true labels\n",
    "subset_src = \"\" #(optional) this file should have the true sequences for a subset of the \"example data\" above\n",
    "\n",
    "ckpt_list = glob.glob(\"\"+\"temp_ckpt//**//*.ckpt\", recursive = True) #grab all checkpoint\n",
    "print('current working directory: ',os.getcwd())\n",
    "\n",
    "\n",
    "for i in range(len(ckpt_list)):\n",
    "    \n",
    "    #search the current directory for the model name and load that model\n",
    "    model_dic = {'trans':'TransVAE','aae':'AAE','rnn':'RNN','rnnattn':'RNNAttn','wae':'WAE'}\n",
    "    model_src = ckpt_list[i]\n",
    "    print('working on: ',model_src,'\\n')\n",
    "    model_name = list(filter(None,[key for key in model_dic.keys() if key in model_src.split('\\\\')[-1]]))\n",
    "    model = locals()[model_dic[model_name[0]]](load_fn=model_src) #use locals to call model specific constructor\n",
    "    \n",
    "    #create save directory for the current model according to latent space size\n",
    "    latent_size = re.findall('(latent[\\d]{2,3})', model_src)\n",
    "    save_dir= save_dir_loc+model.name+latent_size[0]+save_dir_name\n",
    "    if not os.path.exists(save_dir):os.mkdir(save_dir) \n",
    "    save_dir= save_dir+\"//\" \n",
    "    save_df = pd.DataFrame() #this will hold the number variables and save to CSV\n",
    "    \n",
    "    #load the true labels\n",
    "    data = pd.read_csv(example_data).to_numpy() \n",
    "    data_1D = data[:num_sequences,0] #gets rid of extra dimension\n",
    "    true_props_data = pd.read_csv(true_prop_src).to_numpy()\n",
    "    true_props = true_props_data[0:num_sequences,0]\n",
    "\n",
    "    \n",
    "    #get the log.txt file from the ckpt and model name then plot loss curves\n",
    "    \n",
    "    loss_src = '_'.join( (\"log\",model_src.split('\\\\')[-1].split('_')[1],model_src.split('\\\\')[-1].split('_')[2][:-4]+\"txt\") )\n",
    "    src= '\\\\'.join([str(i) for i in model_src.split('\\\\')[:-1]])+\"\\\\\"+loss_src\n",
    "    print(loss_src, src)\n",
    "    loss_plots(src)\n",
    "    \n",
    "    #set the batch size and reconstruct the data\n",
    "    model.params['BATCH_SIZE'] = batch_size\n",
    "    if reconstruct:\n",
    "        reconstructed_seq, props = model.reconstruct(data[:num_sequences], log=False, return_mems=False)\n",
    "    else:\n",
    "        data, data_1D, true_props, props, reconstructed_seq = load_reconstructions(data, data_1D,latent_size,\n",
    "                                                                                   load_src=recon_src,\n",
    "                                                                                   true_props=true_prop_src)\n",
    "    if gpu:torch.cuda.empty_cache() #free allocated CUDA memory\n",
    "    \n",
    "    #save the metrics to the dataframe\n",
    "    save_df['reconstructions'] = reconstructed_seq #placing the saves on a line separate from the ops allows for editing\n",
    "    save_df['predicted properties'] = [prop.item() for prop in props[:len(reconstructed_seq)]]\n",
    "    prop_acc, prop_conf, MCC=calc_property_accuracies(props[:len(reconstructed_seq)],true_props[:len(reconstructed_seq)], MCC=True)\n",
    "    save_df['property prediction accuracy'] = prop_acc\n",
    "    save_df['property prediction confidence'] = prop_conf\n",
    "    save_df['MCC'] = MCC\n",
    "    \n",
    "\n",
    "#   First we tokenize the input and reconstructed smiles\n",
    "    input_sequences = []\n",
    "    for seq in data_1D:\n",
    "        input_sequences.append(peptide_tokenizer(seq))\n",
    "    output_sequences = []\n",
    "    for seq in reconstructed_seq:\n",
    "        output_sequences.append(peptide_tokenizer(seq))\n",
    "    \n",
    "    seq_accs, tok_accs, pos_accs, seq_conf, tok_conf, pos_conf = calc_reconstruction_accuracies(input_sequences, output_sequences)\n",
    "    save_df['sequence accuracy'] = seq_accs\n",
    "    save_df['sequence confidence'] = seq_conf\n",
    "    save_df['token accuracy'] = tok_accs\n",
    "    save_df['token confidence'] = tok_conf\n",
    "    save_df = pd.concat([pd.DataFrame({'position_accs':pos_accs,'position_confidence':pos_conf }), save_df], axis=1)\n",
    "    \n",
    "    ##moving into memory and entropy\n",
    "    if model.model_type =='aae':\n",
    "        mus, _, _ = model.calc_mems(data[:], log=False, save=False) \n",
    "    elif model.model_type == 'wae':\n",
    "        mus, _, _ = model.calc_mems(data[:], log=False, save=False) \n",
    "    else:\n",
    "        mems, mus, logvars = model.calc_mems(data[:1_000], log=False, save=False) #subset size 1200*35=42000 would be ok\n",
    "\n",
    "\n",
    "    ##calculate the entropies\n",
    "    vae_entropy_mus = calc_entropy(mus)\n",
    "    save_df = pd.concat([save_df,pd.DataFrame({'mu_entropies':vae_entropy_mus})], axis=1)\n",
    "    if model.model_type != 'wae' and model.model_type!= 'aae': #these don't have a variational type bottleneck\n",
    "        vae_entropy_mems  = calc_entropy(mems)\n",
    "        save_df = pd.concat([save_df,pd.DataFrame({'mem_entropies':vae_entropy_mems})], axis=1)\n",
    "        vae_entropy_logvars = calc_entropy(logvars)\n",
    "        save_df = pd.concat([save_df,pd.DataFrame({'logvar_entropies':vae_entropy_logvars})], axis=1)\n",
    "    \n",
    "\n",
    "\n",
    "    #create random index and re-index ordered memory list creating n random sub-lists (ideally resulting in IID random lists)\n",
    "    random_idx = np.random.permutation(np.arange(stop=mus.shape[0]))\n",
    "    mus[:] = mus[random_idx]\n",
    "    data = data[random_idx]\n",
    "\n",
    "    #define the subset of the data to sample for PCA and silhouette/cluster metrics\n",
    "    subsample_start=0\n",
    "    subsample_length=mus.shape[0]\n",
    "\n",
    "    #(for length based coloring): record all peptide lengths iterating through input\n",
    "    pep_lengths = []\n",
    "    for idx, pep in enumerate(data[subsample_start:(subsample_start+subsample_length)]):\n",
    "        pep_lengths.append( len(pep[0]) )   \n",
    "    #(for function based coloring): pull function from csv with peptide functions\n",
    "\n",
    "    s_to_f =pd.read_csv(true_prop_src)    \n",
    "    function = s_to_f['peptides'][subsample_start:(subsample_start+subsample_length)]\n",
    "    function = function[random_idx] #account for random permutation\n",
    "\n",
    "    pca = PCA(n_components=2)\n",
    "    pca_batch =pca.fit_transform(X=mus[:])\n",
    "\n",
    "    fig = px.scatter(pca_batch,color= pep_lengths ,opacity=0.7)\n",
    "    fig.update_traces(marker=dict(size=3))\n",
    "    fig.write_image(save_dir+'pca_length.png', width=1920, height=1080)\n",
    "\n",
    "    fig = px.scatter_matrix(pca_batch, color= [str(itm) for itm in function], opacity=0.7)\n",
    "    fig.update_traces(marker=dict(size=3))\n",
    "    fig.write_image(save_dir+'pca_function.png', width=1920, height=1080)\n",
    "\n",
    "    #create n subsamples and calculate silhouette score for each\n",
    "    latent_mem_func_subsamples = []\n",
    "    pca_func_subsamples = []\n",
    "    n=250\n",
    "    for s in range(n):\n",
    "        s_len = len(mus)//n #sample lengths\n",
    "        mem_func_sil = metrics.silhouette_score(mus[s_len*s:s_len*(s+1)], function[s_len*s:s_len*(s+1)], metric='euclidean')\n",
    "        latent_mem_func_subsamples.append(mem_func_sil)\n",
    "        XY = [i for i in zip(pca_batch[s_len*s:s_len*(s+1),0], pca_batch[s_len*s:s_len*(s+1),1])]\n",
    "        pca_func_sil = metrics.silhouette_score(XY, function[s_len*s:s_len*(s+1)], metric='euclidean')\n",
    "        pca_func_subsamples.append(pca_func_sil)\n",
    "    save_df = pd.concat([save_df,pd.DataFrame({'latent_mem_func_silhouette':latent_mem_func_subsamples})], axis=1)\n",
    "    save_df = pd.concat([save_df,pd.DataFrame({'pca_func_silhouette':pca_func_subsamples})], axis=1)\n",
    "\n",
    "    \n",
    "    save_df.to_csv(save_dir+\"saved_info.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8307e206",
   "metadata": {},
   "source": [
    "<H4>Since Compute Canada does not do the dimensionality reduction metrics we need to do them below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29fc2219",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current working directory:  C:\\Users\\s_renaud\\Documents\\GitHub\\MSCSAM_TBD\\main_model\n",
      "working on:  checkpointz\\to_slurm\\rnnattn_latent128\\300_rnnattn-128_peptide.ckpt \n",
      "\n",
      "analysis:  model_analyses\\train\\rnnattn-128_peptide_latent128_train\\saved_info.csv checkpoint:  checkpointz\\to_slurm\\rnnattn_latent128\\300_rnnattn-128_peptide.ckpt\n",
      "rnnattn-128_peptide\n",
      "0 0.6364532925810359 0.7671578504145421 0.048049687543560246\n",
      "0.693426613960434 0.4642010741425193\n",
      "1 0.6429021051520151 0.7698001259452246 0.05026090656692429\n",
      "0.6814258682107179 0.4384161055892021\n",
      "2 0.6371543939858083 0.7621945324350033 0.04287160411737432\n",
      "0.6894073103770594 0.4303143792561397\n",
      "3 0.6423069199657505 0.7643610599585914 0.04637994462602432\n",
      "0.6889322599421552 0.43706911185346675\n",
      "4 0.643468631107008 0.7651989629120498 0.04677060821780094\n",
      "0.6816201246842429 0.44863847053714667\n",
      "5 0.6344647165286765 0.7630344157383262 0.046307439650174614\n",
      "0.6796554784865723 0.4531843915586711\n",
      "6 0.638226509053012 0.7711833551419967 0.04832766522873867\n",
      "0.6994527580800054 0.4414071839217347\n",
      "7 0.6415226572710238 0.7731162760590841 0.047950504633023076\n",
      "0.7141720195561041 0.40905939815185455\n",
      "8 0.6436932018620888 0.7662929181298465 0.04566303272511639\n",
      "0.6975322304371634 0.4295310821094732\n",
      "9 0.6384243154971092 0.7618323758240906 0.04536309157570551\n",
      "0.6781598529986173 0.4512872139064503\n",
      "10 0.6409622022638808 0.7671038411657259 0.04904756259921823\n",
      "0.6820872458183492 0.4654651638583578\n",
      "11 0.6433861123657446 0.7735984983682416 0.0494264021645518\n",
      "0.6918688123049692 0.439172239897209\n",
      "12 0.6413719278038096 0.7684054378332421 0.04837992434512305\n",
      "0.6941747663753474 0.4262450077649076\n",
      "13 0.6398351497051632 0.7644497599424818 0.04715822556378045\n",
      "0.6953533626715838 0.41663806433250794\n",
      "14 0.6422966483571955 0.7657854589521694 0.04679031902766756\n",
      "0.6752265500675567 0.4654263685534741\n",
      "15 0.6444134333994449 0.767697287753146 0.0485691616448087\n",
      "0.6805574013398374 0.45822477682301765\n",
      "16 0.6388765097488717 0.7684909082659293 0.04679878576536047\n",
      "0.6714103202268622 0.46123128156094795\n",
      "17 0.639127037655252 0.7701945033013137 0.04727676583664148\n",
      "0.6993925272586888 0.41101163332068946\n",
      "18 0.644442646898663 0.7738816514832356 0.047872822466518784\n",
      "0.6988510570964859 0.4185595942641165\n",
      "19 0.6433624631368271 0.7716333331942248 0.04695040943808565\n",
      "0.7097618397681797 0.43888461835791925\n",
      "20 0.642863173581772 0.7681707950388641 0.04942164801693535\n",
      "0.6823767446189527 0.4279770093880765\n",
      "21 0.6457861577295656 0.768903717928064 0.04766147801199313\n",
      "0.6966742675076456 0.4391776371552164\n",
      "22 0.6408267677321767 0.7629652070590792 0.04611016612570588\n",
      "0.6850297324589041 0.4410666206988664\n",
      "23 0.6400102969204705 0.7648685106134794 0.04823713592137822\n",
      "0.683263648083256 0.4431431501025671\n",
      "24 0.6411199988708277 0.7703240140014895 0.048840243121125705\n",
      "0.701116363020776 0.43899845424730344\n",
      "25 0.642110720259818 0.7662544976943355 0.048704357345074216\n",
      "0.6721245587057632 0.4539421206865446\n",
      "26 0.6362546249507823 0.7605628808079943 0.04750324737370288\n",
      "0.6893065975349044 0.43854908051809605\n",
      "27 0.6426869061739057 0.7687160880050593 0.04780482906478685\n",
      "0.688250339707874 0.4303447060333154\n",
      "28 0.638449610806476 0.7662203209580913 0.045566656234583953\n",
      "0.691995466860555 0.43129276624724777\n",
      "29 0.6408685817882706 0.766328735712268 0.046278094660668186\n",
      "0.6734560099695593 0.4648843648497144\n",
      "30 0.6379069460434494 0.7651571972781133 0.04790926024639815\n",
      "0.6843042514911782 0.42609408455170306\n",
      "31 0.6330887518588666 0.7613177989221402 0.04551832142553384\n",
      "0.6773971207447016 0.4565955516311495\n",
      "32 0.6419497939195894 0.7697612095960922 0.048447756737772824\n",
      "0.6869713780244973 0.44464650735479005\n",
      "33 0.6409389903816455 0.7625251015507363 0.0471934902481449\n",
      "0.6891041031485261 0.43994333257240126\n",
      "34 0.6353874247874175 0.7702722086557258 0.045863359780426015\n",
      "0.7042168340695321 0.40866413848461325\n",
      "35 0.6382600908473361 0.76521412957474 0.046502077888649254\n",
      "0.6960881393433012 0.42477913737863326\n",
      "36 0.6362925555482949 0.7663567663715048 0.04871647417221917\n",
      "0.7011849223024116 0.4171856523477139\n",
      "37 0.634698190856273 0.7624315710622787 0.046571196368824\n",
      "0.6790131613752399 0.4428900508436876\n",
      "38 0.6377155018341275 0.7613220103595306 0.04686671740630166\n",
      "0.6725903968704317 0.47146976048882117\n",
      "39 0.6378010342520423 0.7651817101189573 0.04553019192589364\n",
      "0.6837281647290009 0.43961694085690084\n",
      "40 0.63464792943665 0.7652853728482844 0.04777704369489368\n",
      "0.6844159256447788 0.453216850626625\n",
      "41 0.6345781635700383 0.7577889214364346 0.04533927298582775\n",
      "0.6675969792585758 0.4502391698929663\n",
      "42 0.6346206883515835 0.769999763754383 0.04812598236108329\n",
      "0.7049581798303972 0.390997660751891\n",
      "43 0.6427777480635752 0.7642745557021957 0.04874892503674475\n",
      "0.6946811327087751 0.4199250077228216\n",
      "44 0.6397887230844238 0.7652961001305283 0.04966112060440946\n",
      "0.6783780004269203 0.4474513447132673\n",
      "45 0.6446433211858134 0.7684073470743376 0.05117649790433679\n",
      "0.6989467799547567 0.4382878801061242\n",
      "46 0.6381863086739108 0.7620267024033696 0.04941783802781169\n",
      "0.6930303470085812 0.43452394794141325\n",
      "47 0.6376060834764221 0.7685576076822012 0.049278797630125444\n",
      "0.7052179043567014 0.4135320016716737\n",
      "48 0.6376107149163376 0.7686647954206179 0.04593868817731938\n",
      "0.6985572040524125 0.438455000627762\n",
      "49 0.6372347790593467 0.7712201212656367 0.04845080644703201\n",
      "0.6852016056419024 0.42473059532500557\n",
      "50 0.6407665497224544 0.7695023527582114 0.046703479462150754\n",
      "0.691129522369502 0.4440922626069188\n",
      "51 0.6297505535311507 0.7525199693507176 0.04434459688239357\n",
      "0.6688900929065256 0.45272472264028296\n",
      "52 0.6477745067436119 0.7761975434154373 0.050875239568324564\n",
      "0.6951211575860762 0.4567309680860928\n",
      "53 0.6356588647289683 0.766005037859271 0.043485858813898985\n",
      "0.6753049318139043 0.4567531223059843\n",
      "54 0.632063372099077 0.7636240356687766 0.04616772962428489\n",
      "0.6991016146129181 0.4352373109209169\n",
      "55 0.6370875515257863 0.7630903320092856 0.046441085803554404\n",
      "0.6739945272937745 0.46337293813729885\n",
      "56 0.6386698435727812 0.7648237137494184 0.046537273894850295\n",
      "0.6821625952577222 0.44607478726035377\n",
      "57 0.6344147750068913 0.7633754769140826 0.04578101013187317\n",
      "0.6868641490963525 0.44179666714537946\n",
      "58 0.6355719622586015 0.7674808124482979 0.05126946275260134\n",
      "0.69363441489475 0.43253994001881324\n",
      "59 0.6397151858293632 0.7634599061748224 0.045318098435271896\n",
      "0.6932490384091867 0.43539268013456445\n",
      "working on:  checkpointz\\to_slurm\\rnnattn_latent32\\300_rnnattn-128_peptide.ckpt \n",
      "\n",
      "analysis:  model_analyses\\train\\rnnattn-128_peptide_latent32_train\\saved_info.csv checkpoint:  checkpointz\\to_slurm\\rnnattn_latent32\\300_rnnattn-128_peptide.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s_renaud\\Documents\\GitHub\\MSCSAM_TBD\\main_model\\transvae\\trans_models.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.params['CHAR_WEIGHTS'] = torch.tensor(self.params['CHAR_WEIGHTS'], dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rnnattn-128_peptide\n",
      "0 0.8384522230030854 0.9235337355544967 0.15880356656005784\n",
      "0.832481072101788 0.6974085802290942\n",
      "1 0.8417565074869789 0.9230217551575141 0.15281305050312238\n",
      "0.8323442756632685 0.6979836797755965\n",
      "2 0.843503919424516 0.9230839383758853 0.1593008181870164\n",
      "0.8298654189297223 0.7191463520479036\n",
      "3 0.8416483789764045 0.9242148418609422 0.16388751840077198\n",
      "0.8334472246161361 0.6931978439832015\n",
      "4 0.8447562890740725 0.9254413002691716 0.1607512256527463\n",
      "0.8235639190069443 0.7142438140334104\n",
      "5 0.838393161102691 0.921605915776146 0.15352618756577555\n",
      "0.8210963540463259 0.7106035648829889\n",
      "6 0.8429855939290746 0.9238019951300753 0.15895373485704045\n",
      "0.8329007768327794 0.6955439241549366\n",
      "7 0.8364346972503807 0.922809033715651 0.15769221687705273\n",
      "0.8429978412159806 0.7129902278779192\n",
      "8 0.8470419451441928 0.9247824928963421 0.15995336373275584\n",
      "0.8322600832029171 0.7168976571833349\n",
      "9 0.841032968416944 0.9234040460156965 0.15658999993394287\n",
      "0.8299841428568207 0.7051222966338269\n",
      "10 0.8409588428131135 0.9214276016556985 0.15672719076347072\n",
      "0.8220648892286986 0.7082807942611102\n",
      "11 0.8408312515044581 0.9231876932349256 0.1586551325594035\n",
      "0.8268231981767105 0.6885020412910212\n",
      "12 0.8402459108667524 0.9232190020078627 0.15669911985992616\n",
      "0.8393663013190883 0.6831026720467506\n",
      "13 0.8385872936807762 0.9203782422564812 0.15580762232091816\n",
      "0.8279125513921884 0.6978504048606365\n",
      "14 0.8416470819810271 0.9226583699478554 0.15759413772526681\n",
      "0.8121240424359637 0.7092059281121833\n",
      "15 0.8350034397870365 0.9206194374014056 0.14954634924968477\n",
      "0.8171860087370146 0.6948726082745699\n",
      "16 0.832695485855614 0.9210760782815651 0.15992679502044133\n",
      "0.8210814223788183 0.7080815256234125\n",
      "17 0.8397185173627545 0.92456893649782 0.16113158514679488\n",
      "0.8359873365097089 0.6899862274509381\n",
      "18 0.8411165635236585 0.9229278985083214 0.1540659249377179\n",
      "0.8208169336102703 0.6997663122818858\n",
      "19 0.840375331980474 0.9228808668775836 0.16103942583566708\n",
      "0.8300112602344903 0.6887940699245553\n",
      "20 0.8437540137139842 0.9247947399442765 0.16155960401957478\n",
      "0.8305480198762638 0.7093899867044405\n",
      "21 0.8430548995687395 0.9237358997124137 0.16296202412564662\n",
      "0.8333968433418293 0.6963642166400261\n",
      "22 0.847701064007533 0.9239197328657542 0.1618001923935168\n",
      "0.82893820550144 0.7021719688517687\n",
      "23 0.8362290359159671 0.9201024022626483 0.1566683290263258\n",
      "0.831203558639876 0.682445935343075\n",
      "24 0.8334277231037731 0.9206712289640724 0.15500987963458351\n",
      "0.8265401715817988 0.6882900081367146\n",
      "25 0.8412696180281755 0.9228335950874209 0.15732223624522557\n",
      "0.8339320506389033 0.6834177800887125\n",
      "26 0.8506208069032587 0.9261797648961975 0.16349228656463538\n",
      "0.8384307543017397 0.6997759018983987\n",
      "27 0.8399710759306886 0.9207907488027567 0.15424607640133306\n",
      "0.8125821959559127 0.7161488418464069\n",
      "28 0.8424655344854244 0.9238907874745991 0.1574100179342379\n",
      "0.8357343949680045 0.7122513184895582\n",
      "29 0.8412054583014006 0.9262381962266748 0.15779108678157316\n",
      "0.8461292151970745 0.6917520344336443\n",
      "30 0.8392805774122142 0.9241435628761094 0.15987776170920934\n",
      "0.8410819104059936 0.6888384272543653\n",
      "31 0.8404729119083071 0.9232277357394884 0.15800474793810199\n",
      "0.8263446306004917 0.6987894389540754\n",
      "32 0.8402906616165869 0.9239453180582822 0.16031925921669382\n",
      "0.8381161146489728 0.6903253776746039\n",
      "33 0.8390475526822585 0.9237486766582909 0.1573230624713837\n",
      "0.8346671807410591 0.6855876453171932\n",
      "34 0.8390225918722437 0.9222396734143576 0.15617778580147842\n",
      "0.8215175460383095 0.7103185468145392\n",
      "35 0.8377838209804116 0.9220520947693687 0.15457262639534722\n",
      "0.8314071424950563 0.6912391904691655\n",
      "36 0.8417792433388415 0.9232607579221448 0.1566558689779702\n",
      "0.8342695118518892 0.7060658861808592\n",
      "37 0.8452660482014196 0.9244655741757615 0.16508963339056587\n",
      "0.8306670822743524 0.7120202210640106\n",
      "38 0.8455844322914396 0.9243035952086787 0.16140901602371355\n",
      "0.8326033489544208 0.6928763935368796\n",
      "39 0.835603397796142 0.9207088776178702 0.15526382975236494\n",
      "0.8285331110994748 0.6843308226497935\n",
      "40 0.8368030344384181 0.9199040726978549 0.1547879669846621\n",
      "0.8214860927905407 0.7049768115412063\n",
      "41 0.8492622722369955 0.9255189677837725 0.1628401993313041\n",
      "0.8322999392332964 0.7222383261017625\n",
      "42 0.8426723680285232 0.9230835857567451 0.15956836649133962\n",
      "0.8302913327079546 0.6831988940689881\n",
      "43 0.8523426567129158 0.9268279565735833 0.1662385474385824\n",
      "0.8233528893757273 0.7236709469728755\n",
      "44 0.8434712667625195 0.9236129770073367 0.15971524803095993\n",
      "0.8289915708896305 0.7216428933928105\n",
      "45 0.8447400284363458 0.9250042023202041 0.1639491620598192\n",
      "0.8306264680307631 0.7118107209310737\n",
      "46 0.8384860560045401 0.9230404983912479 0.15805395062855862\n",
      "0.8291508358229348 0.7046991931848577\n",
      "47 0.8362533785439765 0.9227802276444087 0.16315982485409217\n",
      "0.8323365451332857 0.6916141664791631\n",
      "48 0.8382061632291117 0.9233688582048813 0.1595736543088275\n",
      "0.8274839906159065 0.7130208156451542\n",
      "49 0.8441678969779346 0.9228532762830257 0.16178034444602812\n",
      "0.8411913214969746 0.6913891895249884\n",
      "50 0.8455972350322943 0.923407336729253 0.15685254963355452\n",
      "0.8276507448220045 0.7072921557934095\n",
      "51 0.8421227751389176 0.9257145511123981 0.16097120548293767\n",
      "0.8393014406859034 0.6780425339600575\n",
      "52 0.8404021242552996 0.9222927632109554 0.15889375081470922\n",
      "0.8210581761682073 0.7022677918957083\n",
      "53 0.8406056243876961 0.9241043516765229 0.16066210969147438\n",
      "0.8276858136752466 0.7009439681149521\n",
      "54 0.8397429970481459 0.9234000877645424 0.15888609939725465\n",
      "0.8247338539893319 0.6943774483907116\n",
      "55 0.8373778633994391 0.9238185574821364 0.15349189270060673\n",
      "0.8284770377494783 0.6975495569966909\n",
      "56 0.8403054789963803 0.9252143194506702 0.15778964598988476\n",
      "0.8370738886395189 0.6970350533447913\n",
      "57 0.8342564103898654 0.9202811789253992 0.1532714657752534\n",
      "0.8180306997075317 0.7042533169649612\n",
      "58 0.8433857866729544 0.9234519199918189 0.1564096058101136\n",
      "0.8353995445065658 0.6971407070598608\n",
      "59 0.8402794558183792 0.9213620735312887 0.15930754038560158\n",
      "0.826516351611085 0.7053759154239896\n",
      "working on:  checkpointz\\to_slurm\\rnnattn_latent64\\300_rnnattn-128_peptide.ckpt \n",
      "\n",
      "analysis:  model_analyses\\train\\rnnattn-128_peptide_latent64_train\\saved_info.csv checkpoint:  checkpointz\\to_slurm\\rnnattn_latent64\\300_rnnattn-128_peptide.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s_renaud\\Documents\\GitHub\\MSCSAM_TBD\\main_model\\transvae\\trans_models.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.params['CHAR_WEIGHTS'] = torch.tensor(self.params['CHAR_WEIGHTS'], dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rnnattn-128_peptide\n",
      "0 0.6758351569758408 0.7869056967822203 0.07011256083971461\n",
      "0.6755320177679631 0.587867033698456\n",
      "1 0.6753868326573226 0.7860161560736812 0.06967341767500244\n",
      "0.6770734691757805 0.5570332045184152\n",
      "2 0.6830995123899389 0.7909959869920774 0.07380796972181831\n",
      "0.6768028492138356 0.5722166976689214\n",
      "3 0.6742777512017197 0.783188369366096 0.06862435661495798\n",
      "0.6686786495403296 0.5798370090417484\n",
      "4 0.6811731453123369 0.7913896601610445 0.07370719144813473\n",
      "0.6791509002973708 0.5507305059583922\n",
      "5 0.6855068212133167 0.7907590211655326 0.07252069486010339\n",
      "0.6734582524568355 0.5890321761493424\n",
      "6 0.6758133890639908 0.7884472390123578 0.07043961742006029\n",
      "0.6819528989800033 0.5798200041222492\n",
      "7 0.6805628797866259 0.792284003450435 0.073385974461423\n",
      "0.677337069684977 0.5690963656108452\n",
      "8 0.6823006703547776 0.794138375108415 0.07568706297015772\n",
      "0.6763716608951797 0.559600398467311\n",
      "9 0.6788461127970735 0.796068835186766 0.07232721155185305\n",
      "0.6859989512929323 0.5502616765813392\n",
      "10 0.6743978254060523 0.7867349916548274 0.07182648076409837\n",
      "0.6806123020485932 0.5591306684344093\n",
      "11 0.6712252270657193 0.7821376891012439 0.06701294336571953\n",
      "0.6672284750964196 0.5847364516921993\n",
      "12 0.6727630977486203 0.7870518842816229 0.07119487358461836\n",
      "0.6805689182358562 0.5477531607740929\n",
      "13 0.6847112857699248 0.7925009772586813 0.07723120729721604\n",
      "0.6931969020939923 0.5591890108626469\n",
      "14 0.6784255603096165 0.7842728671406238 0.07206582176651181\n",
      "0.6885631073139746 0.5729249613430512\n",
      "15 0.6803830518736749 0.7944038604156409 0.07302048291489803\n",
      "0.6694990340651985 0.5857823647111757\n",
      "16 0.6728373034489386 0.786197886210598 0.06974522255965554\n",
      "0.6651170083479974 0.5732529273737633\n",
      "17 0.6758598713768308 0.7892218578929417 0.07150379979056866\n",
      "0.6758011710736693 0.5894825986639181\n",
      "18 0.6828906892292612 0.7877219042732312 0.07542711972463888\n",
      "0.6704161926884806 0.5944791569534003\n",
      "19 0.6777426645073527 0.7938513967711497 0.07354082301781566\n",
      "0.6745715188287678 0.6007223877318749\n",
      "20 0.6760758989097503 0.7878896964237354 0.07078253142702859\n",
      "0.671374633336318 0.5839056371344402\n",
      "21 0.6765904042506687 0.7859933621911667 0.06871977409007146\n",
      "0.6693266580715617 0.5749293120105387\n",
      "22 0.6724492806737647 0.7855959068104393 0.06967366344516134\n",
      "0.667976432220746 0.5916240336540838\n",
      "23 0.6685715847901494 0.7792899312757379 0.06504446085485091\n",
      "0.6724844147024556 0.5638882507243059\n",
      "24 0.6763761628931287 0.7897156056102377 0.0658191148548103\n",
      "0.6611738579422519 0.5893702018810936\n",
      "25 0.6805717675886808 0.7925572677226307 0.07188984568675892\n",
      "0.6810955241689028 0.5902210113838031\n",
      "26 0.67654421945664 0.792881417159444 0.07305984428330396\n",
      "0.6696189598374997 0.5648209499925768\n",
      "27 0.6742056577850273 0.7851049793864892 0.0686313193081515\n",
      "0.6654331198400192 0.5689388555661203\n",
      "28 0.6841207165092003 0.793061967870448 0.07276020033964777\n",
      "0.6671106081589036 0.5722391133401792\n",
      "29 0.6720887958825797 0.7769793841903706 0.06607835466906253\n",
      "0.6580742680911447 0.5971278431773841\n",
      "30 0.6790820186425358 0.7864786141513255 0.07453564016132547\n",
      "0.6746678515228854 0.5925528839039449\n",
      "31 0.6734920126611055 0.786694634487101 0.06915100736817247\n",
      "0.6749262914674286 0.5488003998481531\n",
      "32 0.6840805426498231 0.7942068128383766 0.07329353114171429\n",
      "0.7019600789350593 0.5594635072031695\n",
      "33 0.6780526996603234 0.7903931068900499 0.07195765831392276\n",
      "0.6876679991124376 0.579907072425632\n",
      "34 0.6761831997898207 0.7872338404035422 0.07221760772121091\n",
      "0.6615776030011178 0.5821447731692722\n",
      "35 0.6800450306997003 0.7850877255341274 0.0695448157117185\n",
      "0.6704121912756484 0.5562621786090723\n",
      "36 0.6787120129600227 0.791223545219341 0.07255585881036777\n",
      "0.6685372787949346 0.5742905183992557\n",
      "37 0.6699266533982619 0.7754690285420174 0.06339094937462735\n",
      "0.6660042979674577 0.5371716079305655\n",
      "38 0.6780230045216227 0.7910297930583671 0.07736750338249358\n",
      "0.6678515865991795 0.6038546432391039\n",
      "39 0.6792517029203945 0.7947705698921226 0.07112770290263029\n",
      "0.680513708856592 0.5754653025492962\n",
      "40 0.6788688202284601 0.7914325558102648 0.06932013027232985\n",
      "0.6786043287882662 0.5700161634851046\n",
      "41 0.6715427323729354 0.7855940123741141 0.07213890678246113\n",
      "0.6666651146373652 0.5776333665341629\n",
      "42 0.6841631372885008 0.7963041789751457 0.07448848162307665\n",
      "0.6950022788069232 0.59677702302673\n",
      "43 0.6752307722543467 0.7857194814966667 0.06998430995800761\n",
      "0.6669076241022016 0.5700543982767452\n",
      "44 0.6769393177265597 0.7877552784300615 0.06981682054849268\n",
      "0.6650933672745822 0.5912543503109855\n",
      "45 0.6800740477605619 0.7953211678006044 0.07465316901000078\n",
      "0.6966126788275898 0.5863517368406044\n",
      "46 0.676148092961043 0.7914377703221707 0.07337749150295847\n",
      "0.6735218809429369 0.5810695753743589\n",
      "47 0.6696045003681033 0.783642938669381 0.07008716998146029\n",
      "0.6629717182244819 0.5833697389906314\n",
      "48 0.6770671814589082 0.7901851572855458 0.06980309943628259\n",
      "0.6852185375197481 0.5615833398598384\n",
      "49 0.6803101042968065 0.792516771662954 0.07271963877181895\n",
      "0.6662162558877929 0.5773258037488727\n",
      "50 0.6745319077655814 0.7840330518878473 0.06986001169153723\n",
      "0.6555433547358112 0.5451184119069385\n",
      "51 0.6849263535656127 0.7963389239240768 0.07516531721284776\n",
      "0.686152929049763 0.5701543036364352\n",
      "52 0.6812214307366418 0.7937853207918862 0.07196389753792914\n",
      "0.6835645416136087 0.5729500597935953\n",
      "53 0.6834975104302108 0.7947477181294058 0.0733997418526345\n",
      "0.6751589297394062 0.5745715903479462\n",
      "54 0.6722198227145468 0.7861481608622573 0.06970572198620104\n",
      "0.6620872884652447 0.5846409470085465\n",
      "55 0.6734966649416176 0.7833071044731279 0.0716179475924994\n",
      "0.6611866630650667 0.5650390538460826\n",
      "56 0.6803133621715395 0.7886800920617745 0.07129755953694396\n",
      "0.6680108416775983 0.5822365007384736\n",
      "57 0.682961386866781 0.7948354448484249 0.07165546290151009\n",
      "0.6804586246275349 0.579403432052872\n",
      "58 0.6734913303811277 0.7844901661106893 0.07241709642298086\n",
      "0.6596333939297052 0.5901799913554922\n",
      "59 0.6726079799610805 0.7856016018453043 0.07176972837305327\n",
      "0.6867107652082216 0.5472651113297935\n",
      "working on:  checkpointz\\to_slurm\\rnn_latent128\\300_rnn-128_peptide.ckpt \n",
      "\n",
      "analysis:  model_analyses\\train\\rnn-128_peptide_latent128_train\\saved_info.csv checkpoint:  checkpointz\\to_slurm\\rnn_latent128\\300_rnn-128_peptide.ckpt\n",
      "rnn-128_peptide\n",
      "0 0.7200657670403401 0.7940281344844388 0.073915570628554\n",
      "0.711628879818319 0.6152150116385547\n",
      "1 0.7142788847473611 0.7911549552865855 0.07104918385509952\n",
      "0.7109385257513662 0.6249187580476816\n",
      "2 0.7238194761803437 0.7987223616829633 0.07497553954523055\n",
      "0.7000960861186503 0.6357368501881775\n",
      "3 0.7244415729782573 0.800817996947812 0.0720394251117378\n",
      "0.7292958722545573 0.618072304953817\n",
      "4 0.7185049334814985 0.794418872423042 0.07348115261660457\n",
      "0.7181476617473813 0.6438690308075077\n",
      "5 0.720997360854257 0.7971368133236627 0.07085606545496484\n",
      "0.7120540303935042 0.6367626437297305\n",
      "6 0.7158544634104013 0.7891043388023217 0.06692376805365463\n",
      "0.6981445848865905 0.6467138206841601\n",
      "7 0.724385405451556 0.8059417529625784 0.07474596139872104\n",
      "0.7155456835117133 0.6443393170010665\n",
      "8 0.7139859538800346 0.789170068951993 0.06847170547463152\n",
      "0.7222792767946233 0.5941964158843556\n",
      "9 0.7185741055231524 0.7955154301975376 0.07333991791326061\n",
      "0.7202486846317977 0.6183821941574905\n",
      "10 0.7257987159737993 0.7993028614612326 0.07693253021683429\n",
      "0.7024862827588357 0.6457683851835473\n",
      "11 0.7183846380580428 0.7936598352344664 0.07105501665012327\n",
      "0.713536107844668 0.6246498023018967\n",
      "12 0.7139430501320884 0.7900505139625772 0.07142646800752958\n",
      "0.7007525324411501 0.6426905011613162\n",
      "13 0.7198156636951955 0.8003194437686092 0.06808459850551261\n",
      "0.6999640641592892 0.6622878155128504\n",
      "14 0.7208141343996934 0.7959823493078941 0.07042804302115305\n",
      "0.7008341466289327 0.6575492973227037\n",
      "15 0.7268427896928638 0.8002615875854314 0.07241705528496858\n",
      "0.7240425917575889 0.6215031981079666\n",
      "16 0.7213145513872883 0.7981246432492728 0.07254366896728767\n",
      "0.7066649374886603 0.6349025443083474\n",
      "17 0.7141534849930229 0.792619902317665 0.06708230017692818\n",
      "0.705417752866507 0.622869086334878\n",
      "18 0.715009747219934 0.791444094784344 0.07223067224131761\n",
      "0.7117011495708414 0.6323019887405075\n",
      "19 0.715669921748255 0.7908081937193701 0.07005185344265337\n",
      "0.7073829860946617 0.6343276358009877\n",
      "20 0.7307679527982113 0.8013774479288657 0.07224667125733984\n",
      "0.7302480514092313 0.6232668967781423\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 0.7231452707688631 0.8008994169329635 0.07219270508577387\n",
      "0.7094068132341336 0.620197655732515\n",
      "22 0.7249411550298046 0.801411209766528 0.07059453956733039\n",
      "0.7014288208653037 0.6363140884176435\n",
      "23 0.7231157923913228 0.8006637952453343 0.07225086325447924\n",
      "0.6994610200552884 0.6419278285788512\n",
      "24 0.7143385871991591 0.7911855007095875 0.07090111731961897\n",
      "0.7120729931120775 0.6202146665944224\n",
      "25 0.7179405507420868 0.7920531148437532 0.07019550976923852\n",
      "0.7086439355896181 0.6157696747226269\n",
      "26 0.7100776709072179 0.7855327412812253 0.07305985064229865\n",
      "0.7100023213046724 0.6282561739545571\n",
      "27 0.7182134197749116 0.7984754155152832 0.07158769820514907\n",
      "0.7047074209846125 0.6471617442846525\n",
      "28 0.719461633353317 0.7939579305014665 0.07336575764158718\n",
      "0.6937691031740919 0.6406609345466325\n",
      "29 0.7180728564967899 0.7929192849569042 0.07367413676182694\n",
      "0.7179247520104536 0.6308781161238255\n",
      "30 0.7120663430836114 0.7910903332664088 0.06896750415841021\n",
      "0.6879362536416871 0.6563941828425452\n",
      "31 0.722427592980549 0.8013546545043136 0.07598212283193131\n",
      "0.7195156155655207 0.6169053417340455\n",
      "32 0.7167351373061672 0.7937522623107731 0.07113610698798788\n",
      "0.7195677720228002 0.614309595053156\n",
      "33 0.7112110108210145 0.790686524195054 0.06575927248762621\n",
      "0.7095268892053768 0.6266029153271051\n",
      "34 0.7188956546291281 0.7965711558918963 0.0732016554003147\n",
      "0.7118269930386969 0.6341357714736531\n",
      "35 0.7134381712858489 0.7931277590902124 0.06810547764394083\n",
      "0.727554121480345 0.6282406863488823\n",
      "36 0.7194562538931214 0.7953270594200262 0.07298355267903592\n",
      "0.7205303537988091 0.6082338235362519\n",
      "37 0.7252916702900597 0.7981597756736106 0.07630132012598159\n",
      "0.7390565168971461 0.5807983061031092\n",
      "38 0.7168251377338679 0.7959475373782884 0.07028697689827688\n",
      "0.7196078355217577 0.6114443413531996\n",
      "39 0.7240449082444648 0.7996307043323864 0.07164788269914993\n",
      "0.7318346963532261 0.6220908357260384\n",
      "40 0.7220398685630383 0.7964358884673595 0.07155099031368799\n",
      "0.7073505195591502 0.6357312773739972\n",
      "41 0.7179828355651993 0.7981435342706328 0.0679708009459653\n",
      "0.7194411203827444 0.6104953440666157\n",
      "42 0.7216988206548082 0.79462521577178 0.07022884822346467\n",
      "0.7140598096851862 0.6465904924445571\n",
      "43 0.7175701884217379 0.7949302095650859 0.07319467074977812\n",
      "0.6909001252528142 0.6664639297973214\n",
      "44 0.7211762943423081 0.8005845887352321 0.07144838316886348\n",
      "0.7215925011488161 0.6211778909410498\n",
      "45 0.7252750363790407 0.8004219330482626 0.06893019477265108\n",
      "0.7180215092371676 0.6269257504613099\n",
      "46 0.7272490939858026 0.7999904635414661 0.07300280450207691\n",
      "0.7187410438367816 0.6244795234708951\n",
      "47 0.7221617611045175 0.7995276504020137 0.06676672775213126\n",
      "0.7201418071203725 0.6043387787772373\n",
      "48 0.7214481527904525 0.7981305741328498 0.07382031477968264\n",
      "0.7123885562989474 0.6418276716735198\n",
      "49 0.7176084910541358 0.7957810695138334 0.07293397372525587\n",
      "0.7109456026832375 0.643581367874805\n",
      "50 0.7215112184082392 0.7948603717132219 0.07315943247900318\n",
      "0.7247039684045158 0.6063690323981326\n",
      "51 0.720464997528543 0.793820230873797 0.070258297943717\n",
      "0.7064603948260654 0.6170002874297472\n",
      "52 0.722637986338873 0.7968036878508326 0.06874044076156997\n",
      "0.709755511055485 0.6242086135855018\n",
      "53 0.7309494595990537 0.8048122563369036 0.0732630582252246\n",
      "0.7214932412769914 0.6024148721085336\n",
      "54 0.7190495752166947 0.793863805430704 0.07361171377872566\n",
      "0.7201631445694514 0.601402233468122\n",
      "55 0.7211989081877245 0.7941277642915676 0.07151370693009634\n",
      "0.7205658176249736 0.6215659505244475\n",
      "56 0.7226837361614595 0.7956005724653578 0.06995240856501934\n",
      "0.7098126964828527 0.6345841828973111\n",
      "57 0.7248759740483949 0.8000762329778147 0.07213062703826245\n",
      "0.7248230169088901 0.612825239279759\n",
      "58 0.7151044191318814 0.7950057652334821 0.06843043667612748\n",
      "0.727769396448975 0.6265092327461167\n",
      "59 0.7132988610631971 0.793507523568171 0.06659945063934326\n",
      "0.712645724512053 0.6305211162737971\n",
      "working on:  checkpointz\\to_slurm\\rnn_latent32\\300_rnn-128_peptide.ckpt \n",
      "\n",
      "analysis:  model_analyses\\train\\rnn-128_peptide_latent32_train\\saved_info.csv checkpoint:  checkpointz\\to_slurm\\rnn_latent32\\300_rnn-128_peptide.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s_renaud\\Documents\\GitHub\\MSCSAM_TBD\\main_model\\transvae\\trans_models.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.params['CHAR_WEIGHTS'] = torch.tensor(self.params['CHAR_WEIGHTS'], dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rnn-128_peptide\n",
      "0 0.6981357467923908 0.7848349001668847 0.06771276185508952\n",
      "0.6298446422878464 0.7022324704158764\n",
      "1 0.6917548319333615 0.7770062546785074 0.06662956547028716\n",
      "0.6224804292355276 0.708226759116535\n",
      "2 0.69061997512593 0.7779346448867838 0.0657289867221412\n",
      "0.6225011832436343 0.70343565095939\n",
      "3 0.695386182327458 0.7847403008445052 0.07029238496602844\n",
      "0.6293328679023815 0.7204226996495195\n",
      "4 0.6787471851046943 0.7688940708251428 0.06356574007515371\n",
      "0.605346536175954 0.7277901052224109\n",
      "5 0.6919817001087152 0.7820036563441894 0.06551060126330328\n",
      "0.619586574702642 0.7230833736836275\n",
      "6 0.6936411962770541 0.780159176643747 0.06824935859054569\n",
      "0.6254755891139726 0.7263469259177096\n",
      "7 0.6919908821461939 0.7781421800105283 0.06679116420221462\n",
      "0.6134675292433474 0.7089883998916859\n",
      "8 0.6836861927143582 0.7758615318483801 0.06144279270276254\n",
      "0.6227164760222259 0.7150821286483717\n",
      "9 0.6927336697321284 0.7820538988141534 0.068687337356924\n",
      "0.6508417750819124 0.6954039657119248\n",
      "10 0.6832197874924967 0.7733019295969287 0.06547894034262443\n",
      "0.6097745608154009 0.73680139100129\n",
      "11 0.6981713302548793 0.7853377248261828 0.06986196748862464\n",
      "0.6375655395235259 0.6808885846445039\n",
      "12 0.6947956435085154 0.7796683664227744 0.06799691099440813\n",
      "0.6264217404942176 0.7134465705914668\n",
      "13 0.6841978144022746 0.7719943553951655 0.06592520321813462\n",
      "0.6190792533679638 0.7090298179450674\n",
      "14 0.6961081468378427 0.7837288857765268 0.06716215025196523\n",
      "0.6288705171546063 0.7147502423684138\n",
      "15 0.691568026049445 0.7772853166100329 0.0690032261600373\n",
      "0.6095192695304638 0.7227958431372343\n",
      "16 0.6875952621381556 0.7765675197836945 0.062350204256211225\n",
      "0.616010437069659 0.7219534867961178\n",
      "17 0.6924680813437812 0.7817681858520226 0.06675508397627511\n",
      "0.6258557586667219 0.7164990592362321\n",
      "18 0.6884440115830239 0.775739644874819 0.06750236899352878\n",
      "0.6116329897748813 0.7290613669447257\n",
      "19 0.6933283633128514 0.7794738273712714 0.06650969569078936\n",
      "0.61561943032751 0.7175516696245237\n",
      "20 0.6960354521560834 0.7774444756066675 0.06453400329794655\n",
      "0.6261720007571436 0.7053113200179049\n",
      "21 0.6933354627126828 0.7785160841685201 0.06460713254115705\n",
      "0.6221252709255161 0.7202477610878324\n",
      "22 0.6891505328522012 0.7785939060235482 0.0674617678829896\n",
      "0.6113001505324489 0.7211581042044515\n",
      "23 0.6893982318173342 0.7744787773977988 0.06424314409170162\n",
      "0.6182948809797326 0.7248994232901865\n",
      "24 0.6916509593956334 0.7770606188123387 0.06524797464899475\n",
      "0.6168147446558521 0.7284969852689348\n",
      "25 0.699448426679885 0.787117092431211 0.06905093581743856\n",
      "0.6311405394255967 0.7081325126309694\n",
      "26 0.6970675548412583 0.7804376232941981 0.06786517947175431\n",
      "0.6217358039165306 0.7165232374747716\n",
      "27 0.690574806373762 0.7760361854376167 0.06507043126968239\n",
      "0.6188451543285974 0.7073320439842475\n",
      "28 0.6896221013960765 0.777986226124565 0.0682517411099464\n",
      "0.6328001973638503 0.692163340141944\n",
      "29 0.6908203333820055 0.7779954131453457 0.06890836312840046\n",
      "0.629767191360171 0.7100916475553549\n",
      "30 0.7005780665112935 0.7866959653570381 0.06785143306495411\n",
      "0.625961867991115 0.7160053237204547\n",
      "31 0.6999140826971069 0.7843327553133954 0.07073470593126749\n",
      "0.6115790803574108 0.7259102871115074\n",
      "32 0.6949846735022261 0.7806368882930264 0.06977441588461358\n",
      "0.6272455737332238 0.7197329567831408\n",
      "33 0.6937443770963085 0.7754741251739521 0.06701303631114691\n",
      "0.6228550407863167 0.7324925049762765\n",
      "34 0.6866643609323632 0.7734525278889014 0.06215152299131062\n",
      "0.6224881147317791 0.7172179472725462\n",
      "35 0.6862735688196725 0.7763490719973158 0.06594688666289163\n",
      "0.6146093687252056 0.7198531577242684\n",
      "36 0.691681825271372 0.7726531797494569 0.06368247771672451\n",
      "0.6192943546947434 0.700045633691725\n",
      "37 0.6990252663772504 0.7872748425418404 0.07264493329428198\n",
      "0.6252022119171197 0.7211652438067069\n",
      "38 0.6879529197811778 0.7784911845148894 0.06418291499650546\n",
      "0.6216332321759483 0.7066940608590166\n",
      "39 0.6949096109375412 0.7807215821914563 0.07209622849430952\n",
      "0.6287547589279955 0.7094360352474838\n",
      "40 0.6834847171571421 0.767747288344258 0.06314516490964558\n",
      "0.6080573632632977 0.7353660353871894\n",
      "41 0.6882180057024949 0.774195073149844 0.06598499312089359\n",
      "0.6133477743679445 0.7214847028989129\n",
      "42 0.6819430209719546 0.7733369857961907 0.06400372018432222\n",
      "0.6037572803262116 0.7129208004353415\n",
      "43 0.6911878875959167 0.7786196118207986 0.06653643214077483\n",
      "0.6266408484011623 0.7203925725501968\n",
      "44 0.6892473363798327 0.7828861349638432 0.06802392618493745\n",
      "0.6261091032727621 0.704636267724673\n",
      "45 0.6924051863792169 0.7771093685512118 0.061732297079341636\n",
      "0.6069322596658925 0.7325159347242289\n",
      "46 0.6902634944874309 0.7768099697095776 0.06327799431811491\n",
      "0.6045227772039081 0.7204543639581704\n",
      "47 0.6998612866569294 0.7847661776919159 0.06922782265449554\n",
      "0.6312768019256338 0.7146360979764099\n",
      "48 0.6991435738062178 0.7803978332594153 0.06885067283290706\n",
      "0.625953386736064 0.7151108770474457\n",
      "49 0.6882184873815884 0.7786766925278688 0.06705386416868966\n",
      "0.6075907449312739 0.7287798254447393\n",
      "50 0.6935254203792381 0.7818999929758936 0.06870078709960085\n",
      "0.6193999793589295 0.7066287880108615\n",
      "51 0.6881043294264773 0.7724145939657777 0.06429841600326956\n",
      "0.613726390455666 0.7097067135822018\n",
      "52 0.6967697511244195 0.7835070873924304 0.06853993189072556\n",
      "0.6158092623469513 0.7284762995374923\n",
      "53 0.705892086460687 0.7938299034240133 0.07325582125141125\n",
      "0.6235769807059581 0.734463996930042\n",
      "54 0.6900874660322371 0.7790117249287631 0.06389617886177583\n",
      "0.6158786046044153 0.7328444656241547\n",
      "55 0.6928296691540955 0.7799835813351048 0.06607337124371561\n",
      "0.6091461253987773 0.7164564517468128\n",
      "56 0.6884662950798353 0.7735469121795666 0.0611479343221598\n",
      "0.6239998653776806 0.6964728996497609\n",
      "57 0.6914709190064785 0.7767135097354317 0.06696358546203128\n",
      "0.630645278613841 0.7051719962089809\n",
      "58 0.6930119438794912 0.7816790801833305 0.07209378788514031\n",
      "0.6199192906142403 0.7127615162584935\n",
      "59 0.690661603217824 0.7768531848252194 0.06593739641929718\n",
      "0.6161517545261519 0.7178369490635405\n",
      "working on:  checkpointz\\to_slurm\\rnn_latent64\\300_rnn-128_peptide.ckpt \n",
      "\n",
      "analysis:  model_analyses\\train\\rnn-128_peptide_latent64_train\\saved_info.csv checkpoint:  checkpointz\\to_slurm\\rnn_latent64\\300_rnn-128_peptide.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s_renaud\\Documents\\GitHub\\MSCSAM_TBD\\main_model\\transvae\\trans_models.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.params['CHAR_WEIGHTS'] = torch.tensor(self.params['CHAR_WEIGHTS'], dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rnn-128_peptide\n",
      "0 0.7394327742096667 0.8202182901817343 0.08386264346223858\n",
      "0.7196669189916141 0.6323554017430095\n",
      "1 0.7244201558153726 0.8079786622972973 0.07287773650989368\n",
      "0.7173854359005294 0.6201927247321511\n",
      "2 0.7332780788637941 0.815682860088684 0.07635739668010741\n",
      "0.714196147186135 0.640002546286997\n",
      "3 0.7291025981068762 0.8127009417946975 0.07489854336791724\n",
      "0.7078419642152264 0.6202355081956057\n",
      "4 0.737595810939327 0.8216940385961413 0.07416593792958613\n",
      "0.7219723796784719 0.6373949529254983\n",
      "5 0.7311428494947155 0.8146513624232058 0.07465035286472758\n",
      "0.7150063753413735 0.6391460068738888\n",
      "6 0.7339686251176788 0.8147857395965904 0.07934849193877887\n",
      "0.7231982108833711 0.6248332928345535\n",
      "7 0.732371277409782 0.8192427476449344 0.0814260283127909\n",
      "0.7412001313096535 0.5872614114160537\n",
      "8 0.7358574121874946 0.8139223471491355 0.07535855955242082\n",
      "0.7326197788052962 0.6080793974885925\n",
      "9 0.7411789136413814 0.8227103745607501 0.08211789378537653\n",
      "0.7265306823405464 0.619013475720646\n",
      "10 0.7206105130963811 0.8084881766063362 0.0772557129651294\n",
      "0.7157844925864975 0.6095870376710849\n",
      "11 0.7364755151657049 0.8199997372371546 0.0778518412435564\n",
      "0.7209817689901981 0.6153423009860337\n",
      "12 0.7309855305876919 0.8127635974968394 0.07308471639313763\n",
      "0.721231979565597 0.5990623120696728\n",
      "13 0.738777722976752 0.8220941327302221 0.08039954147230143\n",
      "0.7295347063529136 0.6278038161094847\n",
      "14 0.7280624707957328 0.8107407374319394 0.0758836987823577\n",
      "0.7167724855736397 0.6291910410916948\n",
      "15 0.7327678441885507 0.8181420886958259 0.0754306848572963\n",
      "0.7443481921942555 0.5954167500163188\n",
      "16 0.7323293926302488 0.8136571587117247 0.07683679721449767\n",
      "0.7215609686088635 0.6112970357784411\n",
      "17 0.7322754829045639 0.8166216127546534 0.07693841364139922\n",
      "0.7395385592891439 0.604008208564524\n",
      "18 0.7201297300241619 0.8074036236809574 0.07128714460781384\n",
      "0.7253217985283464 0.5990573087488738\n",
      "19 0.7318744317028489 0.811884403300015 0.07714046800550094\n",
      "0.7238893998180671 0.5978915881120144\n",
      "20 0.7368532765341556 0.8182847992485268 0.07797248313705289\n",
      "0.7093378021775738 0.629079875254577\n",
      "21 0.7395898394014704 0.8183693671123531 0.0804544074987012\n",
      "0.7477976629339914 0.6113993709688548\n",
      "22 0.7256293629371365 0.8088034552257972 0.07910098310218362\n",
      "0.7092255193340851 0.6237197209988457\n",
      "23 0.735596609967436 0.8198254823844341 0.07819313386032761\n",
      "0.7485290805117439 0.5717935918312329\n",
      "24 0.7304474285438507 0.8190354066529456 0.07742015863123185\n",
      "0.7165361289580177 0.6411665164808235\n",
      "25 0.7299642404933882 0.8136556836852138 0.07673411267782186\n",
      "0.7081890413199918 0.6265705058208094\n",
      "26 0.7334636877170908 0.8125976085471944 0.07708040540459732\n",
      "0.7075155495206967 0.6446817667923459\n",
      "27 0.72846850688004 0.8088185055198944 0.07766895656991157\n",
      "0.7047856340642568 0.6122758858593328\n",
      "28 0.7317809008211336 0.812189670659096 0.07536388358291712\n",
      "0.7119778774524537 0.634224753364121\n",
      "29 0.7279844071939096 0.8124133241186824 0.07792341323344745\n",
      "0.6978190704264893 0.6572570404748866\n",
      "30 0.7406204164494354 0.8191340312883815 0.08233628903245156\n",
      "0.7199020617379452 0.6086936061358567\n",
      "31 0.7239446365831352 0.8107074208535933 0.07740586896919095\n",
      "0.7110368878292657 0.6202562687519175\n",
      "32 0.7354049082981091 0.8178038689526848 0.0772708348066282\n",
      "0.7174511376638384 0.6117768411762259\n",
      "33 0.7320858286691448 0.813633055794441 0.07567950642451109\n",
      "0.7291919479724909 0.6082314868512968\n",
      "34 0.7265594845360865 0.8136089905455434 0.07705032318103631\n",
      "0.7150498806100055 0.624474434533363\n",
      "35 0.7371453465385778 0.8178890487881779 0.07631695188493455\n",
      "0.7114935750826494 0.6252967914488936\n",
      "36 0.7336879325376726 0.8132865479955199 0.07731381648255457\n",
      "0.7119075210247381 0.6225394483054467\n",
      "37 0.7290678117453038 0.8115746476660255 0.07677236739798317\n",
      "0.7257010001738817 0.5969344924301623\n",
      "38 0.7247022743804119 0.8107236575382373 0.07643385943549964\n",
      "0.7077391134255556 0.6391892308184355\n",
      "39 0.737232642379369 0.8182233616632603 0.07686001075313276\n",
      "0.7299795082515783 0.5941614982385006\n",
      "40 0.7400939276493265 0.820927431238723 0.08140111154456277\n",
      "0.7460368547447521 0.5984497408949008\n",
      "41 0.729151227588216 0.8133891906084415 0.077138320648014\n",
      "0.7161777016561541 0.6101385973351914\n",
      "42 0.7302180595756763 0.8125740602119537 0.07550983676380377\n",
      "0.7117186513364196 0.6090524049465371\n",
      "43 0.7389453620697203 0.8173145013192438 0.07851332303539897\n",
      "0.7180457729440357 0.630000190107662\n",
      "44 0.7337712391182696 0.8143149709680157 0.07566873961434797\n",
      "0.7284467885286909 0.6008732147175961\n",
      "45 0.7320214856620281 0.8114585481668142 0.08036044924812559\n",
      "0.719101812787633 0.6208082457328954\n",
      "46 0.7301416070135763 0.8131882956928217 0.07672936854799284\n",
      "0.727124162337948 0.6134782576392503\n",
      "47 0.7329647703274855 0.815537037603387 0.0772985596316338\n",
      "0.7153549921644213 0.6273669888357525\n",
      "48 0.740804602530015 0.8185233708592989 0.08070229570555708\n",
      "0.7151148996586612 0.6281893741522879\n",
      "49 0.7314109708428218 0.8120421409181209 0.07736468015031796\n",
      "0.7209509894318793 0.6079819301131315\n",
      "50 0.7337999124138483 0.812161225412528 0.07540794392610439\n",
      "0.7319784175471218 0.5934594569162794\n",
      "51 0.739876217183451 0.822439776871734 0.08169904059265853\n",
      "0.7353312074132834 0.6147979874910863\n",
      "52 0.7335221008159217 0.8169929368657857 0.07783037225546727\n",
      "0.721704809500727 0.6433453432366132\n",
      "53 0.742567038759249 0.8200342070241375 0.07873801652887523\n",
      "0.7134538888245727 0.627964995146912\n",
      "54 0.7329167032263493 0.8146161716579922 0.0802561871420155\n",
      "0.7077245742587613 0.6536368348976753\n",
      "55 0.7386199042160921 0.8214169366053907 0.08175466291674117\n",
      "0.7427224038292992 0.6085989611137397\n",
      "56 0.7214806528148907 0.8080202405201224 0.07386913528118759\n",
      "0.7292189333271589 0.6138863781574582\n",
      "57 0.7268886656657708 0.8116134274864174 0.07654946332428392\n",
      "0.7368249916906078 0.5913624411887355\n",
      "58 0.734556225198986 0.8181175654156705 0.07980461636626172\n",
      "0.732512456207254 0.6129762284898825\n",
      "59 0.7288872063550671 0.8122859856225683 0.07792149364929775\n",
      "0.7188306761054281 0.6242599046022788\n"
     ]
    }
   ],
   "source": [
    "import coranking #coranking.readthedocs.io\n",
    "from coranking.metrics import trustworthiness, continuity, LCMC\n",
    "from transvae.snc import SNC #github.com/hj-n/steadiness-cohesiveness\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "from transvae import trans_models\n",
    "from transvae.transformer_models import TransVAE\n",
    "from transvae.rnn_models import RNN, RNNAttn\n",
    "from transvae.wae_models import WAE\n",
    "from transvae.aae_models import AAE\n",
    "from transvae.tvae_util import *\n",
    "from transvae import analysis\n",
    "import glob\n",
    "import re\n",
    "\n",
    "\n",
    "gpu = True\n",
    "\n",
    "example_data = 'data\\\\peptides\\\\datasets\\\\uniprot_v2\\\\peptide_train.txt'\n",
    "test_train='train'\n",
    "ckpt_list = glob.glob(\"\"+\"checkpointz\\\\to_slurm//**//*.ckpt\", recursive = True) #grab all checkpoints\n",
    "analyses_list = glob.glob(\"model_analyses\\\\train//**/*.csv\", recursive=True) #grab all analyses\n",
    "print('current working directory: ',os.getcwd())\n",
    "\n",
    "for i in range(len(ckpt_list)):\n",
    "    \n",
    "    #search the current directory for the model name and load that model\n",
    "    model_dic = {'trans':'TransVAE','aae':'AAE','rnnattn':'RNNAttn','rnn':'RNN','wae':'WAE'}\n",
    "    model_src = ckpt_list[i]\n",
    "    print('working on: ',model_src,'\\n')\n",
    "    model_name = list(filter(None,[key for key in model_dic.keys() if key in model_src.split('//')[-1]]))\n",
    "    model = locals()[model_dic[model_name[0]]](load_fn=model_src) #use locals to call model specific constructor\n",
    "    \n",
    "    #load the analysis file corresponding to the model from the CC outputs\n",
    "    for idx in range(len(analyses_list)):\n",
    "        if analyses_list[idx].split(\"\\\\\")[-2].find(model_src.split(\"\\\\\")[-2].split(\"_\")[0]) != -1 and analyses_list[idx].split(\"\\\\\")[-2].find(model_src.split(\"\\\\\")[-2].split(\"_\")[1]) != -1:\n",
    "            if analyses_list[idx].find(\"rnnattn\")  != -1 and model_src.find(\"rnnattn\") == -1: continue\n",
    "            save_dir = analyses_list[idx]\n",
    "            cur_analysis = pd.read_csv(save_dir)\n",
    "    print(\"analysis: \",save_dir, \"checkpoint: \",model_src)\n",
    "    save_df = cur_analysis #this will hold the number variables and save to CSV\n",
    "    \n",
    "    #load the true labels\n",
    "    data = pd.read_csv(example_data).to_numpy() \n",
    "    data_1D = data[:,0] #gets rid of extra dimension\n",
    "    \n",
    "    #moving into memory and entropy\n",
    "    if model.model_type =='aae':\n",
    "        mus, _, _ = model.calc_mems(data[:60_000], log=False, save=False) \n",
    "    elif model.model_type == 'wae':\n",
    "        mus, _, _ = model.calc_mems(data[:60_000], log=False, save=False) \n",
    "    else:\n",
    "        mems, mus, logvars = model.calc_mems(data[:60_000], log=False, save=False) #subset size 1200*35=42000 would be ok\n",
    "\n",
    "    #create random index and re-index ordered memory list creating n random sub-lists (ideally resulting in IID random lists)\n",
    "    random_idx = np.random.permutation(np.arange(stop=mus.shape[0]))\n",
    "    mus[:] = mus[random_idx]\n",
    "    data = data[random_idx]\n",
    "    \n",
    "    #need to perform PCA to be able to compare dimensionality reduction quality\n",
    "    pca = PCA(n_components=2)\n",
    "    pca_batch =pca.fit_transform(X=mus[:])\n",
    "    \n",
    "    #now ready to calculation dimensionality reduction accuracy with metrics\n",
    "    trust_subsamples = []\n",
    "    cont_subsamples = []\n",
    "    lcmc_subsamples = []\n",
    "    steadiness_subsamples = []\n",
    "    cohesiveness_subsamples = []\n",
    "    if 'test' in test_train: #different number of bootsraps for train vs test\n",
    "        n=15\n",
    "    else:\n",
    "        n=60\n",
    "    parameter = { \"k\": 50,\"alpha\": 0.1 } #for steadiness and cohesiveness\n",
    "    for s in range(n):\n",
    "        s_len = len(mus)//n\n",
    "        Q = coranking.coranking_matrix(mus[s_len*s:s_len*(s+1)], pca_batch[s_len*s:s_len*(s+1)])\n",
    "        trust_subsamples.append( np.mean(trustworthiness(Q, min_k=1, max_k=50)) )\n",
    "        cont_subsamples.append( np.mean(continuity(Q, min_k=1, max_k=50)) )\n",
    "        lcmc_subsamples.append( np.mean(LCMC(Q, min_k=1, max_k=50)) )\n",
    "        print(s,trust_subsamples[s],cont_subsamples[s],lcmc_subsamples[s])\n",
    "\n",
    "        metrics = SNC(raw=mus[s_len*s:s_len*(s+1)], emb=pca_batch[s_len*s:s_len*(s+1)], iteration=300, dist_parameter=parameter)\n",
    "        metrics.fit() #solve for steadiness and cohesiveness\n",
    "        steadiness_subsamples.append(metrics.steadiness())\n",
    "        cohesiveness_subsamples.append(metrics.cohesiveness())\n",
    "        print(metrics.steadiness(),metrics.cohesiveness())\n",
    "        Q=0 #trying to free RAM\n",
    "        metrics=0\n",
    "        torch.cuda.empty_cache() #free allocated CUDA memory\n",
    "\n",
    "    save_df = pd.concat([save_df,pd.DataFrame({'latent_to_PCA_trustworthiness':trust_subsamples})], axis=1)\n",
    "    save_df = pd.concat([save_df,pd.DataFrame({'latent_to_PCA_continuity':cont_subsamples})], axis=1)\n",
    "    save_df = pd.concat([save_df,pd.DataFrame({'latent_to_PCA_lcmc':lcmc_subsamples})], axis=1)\n",
    "    save_df = pd.concat([save_df,pd.DataFrame({'latent_to_PCA_steadiness':steadiness_subsamples})], axis=1)\n",
    "    save_df = pd.concat([save_df,pd.DataFrame({'latent_to_PCA_cohesiveness':cohesiveness_subsamples})], axis=1)  \n",
    "    \n",
    "    save_df.to_csv(save_dir, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcb4f9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
