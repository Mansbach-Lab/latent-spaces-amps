{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "from transvae import trans_models\n",
    "from transvae.transformer_models import TransVAE\n",
    "from transvae.rnn_models import RNN, RNNAttn\n",
    "from transvae.wae_models import WAE\n",
    "from transvae.aae_models import AAE\n",
    "from transvae.tvae_util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import IPython.display as Disp\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial will demonstrate how to visualize and interpret model memory and how to evaluate model performance on a number of metrics. Full scripts for training models, generating samples and calculating attention weights are provided and instructions on how to use those scripts are included in the README. The functions demonstrated in this tutorial do not have pre-written high throughput scripts but can be used on larger input sizes if desired. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Reconstruction Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A set of metrics on reconstruction accuracy of the different models is presented below. Some parameters need to be selected:\n",
    "<ul>\n",
    "    <li>data size: int --how many samples from the data to laod\n",
    "    <li>data selection: string  --training, testing, full_no_shuffle\n",
    "    <li>model_src: string --path to model checkpoint\n",
    "    <li>models : RNN, WAE, AAE, RNNAttn, TransVAE --model selectiong from listed\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_size = 200\n",
    "data_selection = \"full_no_shuffle\"\n",
    "model_src = \"checkpointz//trans_amp//1_16_2022//2000_trans1x-128_peptide.ckpt\"\n",
    "model = TransVAE(load_fn=model_src)\n",
    "gpu = False\n",
    "\n",
    "if \"full_no_shuffle\" in data_selection:\n",
    "    data = pd.read_csv('notebooks//example_data//peptide_combined_no_shuff.txt').to_numpy() \n",
    "elif \"training\" in data_selection:\n",
    "    data = pd.read_csv('notebooks//example_data//train_test//peptide_train.txt').to_numpy()\n",
    "elif \"testing\" in data_selection:\n",
    "    data = pd.read_csv('notebooks//example_data//train_test//peptide_test.txt').to_numpy()\n",
    "else:\n",
    "    data = pd.read_csv('notebooks//example_data//train_test//.txt').to_numpy() \n",
    "data_1D = data[:5000,0] #gets rid of extra dimension\n",
    "if gpu:\n",
    "    data.cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.params['BATCH_SIZE'] = 200\n",
    "reconstructed_seq, props = model.reconstruct(data[:5000], log=False, return_mems=False)\n",
    "for og_token, reconstructed_token in zip(data_1D, reconstructed_seq):\n",
    "    print('{} <- Original'.format(og_token))\n",
    "    print('{} <- Reconstruction'.format(reconstructed_token))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>MCC info:\n",
    "    <li>+1 represents a perfect prediction\n",
    "    <li>0 no better than random prediction\n",
    "    <li>âˆ’1 indicates total disagreement between prediction and observation.\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_props_data = pd.read_csv('notebooks//example_data//function_full_no_shuff.txt').to_numpy()\n",
    "true_props = true_props_data[3000:3200,0]\n",
    "prop_acc = calc_property_accuracies(props,true_props, MCC=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Token accuracies are accuracies per token, \n",
    "<ul>\n",
    "    <li>sequence accuracies are accuracies per sequence\n",
    "    <li>token accuracies are accuracies for each token averaged over all tokens in input dataset\n",
    "    <li>position accuracies are per sequence position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we tokenize the input and reconstructed smiles\n",
    "input_sequences = []\n",
    "for seq in data_1D:\n",
    "    input_sequences.append(peptide_tokenizer(seq))\n",
    "output_sequences = []\n",
    "for seq in reconstructed_seq:\n",
    "    output_sequences.append(peptide_tokenizer(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seq_accs, token_accs, position_accs = calc_reconstruction_accuracies(input_sequences, output_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_accs, token_accs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the accuracy on token position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(position_accs)\n",
    "plt.xlabel('Sequence Position')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On these 25 smiles, the `RNNAttn-256` model is above 95% accurate showing only one significant drop between sequence positions 35 and 40. However, this is a small sample size so it is not a good representation on how this model performs on molecules of this size in general. For this, you can read our analysis of model performance on the ZINC/PubChem datasets (shown below) or test your own models reconstruction accuracy on a larger sample size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Model Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The memory of a model is analogous to the probability distribution of molecular embeddings that it has learned during training. A single molecular embedding is the size 128 vector at the center of the variational bottleneck. Each model has a built-in method for calculating and returning the model memory for a set of input structures, `calc_mems()`. ***(note - we plot the mean vector rather than the reparameterized vector so we can identify and analyze the meaningful latent dimensions)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model.model_type =='aae':\n",
    "    mems, _, _ = model.calc_mems(data[:5000], log=False, save=False) \n",
    "elif model.model_type == 'wae':\n",
    "    mems, _, _ = model.calc_mems(data[:5000], log=False, save=False) \n",
    "else:\n",
    "    mems, mus, logvars = model.calc_mems(data[:5000], log=False, save=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the model memory by plotting a sample of molecular embeddings using `plt.imshow()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px \n",
    "video_mem = np.reshape(mems, (50,100,128))\n",
    "fig =px.imshow(video_mem, animation_frame=0)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,8))\n",
    "\n",
    "video_mus = np.reshape(mus, (50,100,128))\n",
    "fig =px.imshow(video_mus, animation_frame=0)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[0])\n",
    "fig = plt.figure(figsize=(20,8))\n",
    "\n",
    "plt.imshow(mus[0:2])\n",
    "\n",
    "plt.xticks()\n",
    "plt.yticks([])\n",
    "plt.show()\n",
    "print(data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_logvars = np.reshape(logvars, (50,100,128))\n",
    "fig =px.imshow(video_logvars, animation_frame=0)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the `RNNAttn-256` model we see the selective memory structure. Some latent dimensions are more meaningful than others. We can calculate exactly how much information is stored in each dimension with the Shannon information entropy. Typically, you would want to calculate the entropy for a larger sample than the 25 SMILES we are using in this tutorial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_entropy_mems  = calc_entropy(mems)\n",
    "vae_entropy_mus = calc_entropy(mus)\n",
    "vae_entropy_logvars = calc_entropy(logvars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6,3))\n",
    "\n",
    "plt.bar(range(len(vae_entropy_mems)), vae_entropy_mems)\n",
    "plt.xlabel('Latent Dimension')\n",
    "plt.ylabel('Entropy (bits)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6,3))\n",
    "\n",
    "plt.bar(range(len(vae_entropy_mus)), vae_entropy_mus)\n",
    "plt.xlabel('Latent Dimension')\n",
    "plt.ylabel('Entropy (bits)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6,3))\n",
    "\n",
    "plt.bar(range(len(vae_entropy_logvars)), vae_entropy_logvars)\n",
    "plt.xlabel('Latent Dimension')\n",
    "plt.ylabel('Entropy (bits)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that some dimensions have significantly more information contained across the 25 samples than others and they correspond with the selective memory visualization shown above. We can sum the entropy of all dimensions to find the full model entropy. Again, note that we would need a larger sample size to converge the model entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_entropy_mems = np.sum(vae_entropy_mems)\n",
    "print('The model contains {} nats of information'.format(round(total_entropy_mems, 2)))\n",
    "total_entropy_mus = np.sum(vae_entropy_mus)\n",
    "print('The model contains {} nats of information'.format(round(total_entropy_mus, 2)))\n",
    "total_entropy_logvars = np.nansum(vae_entropy_logvars)\n",
    "#print(vae_entropy_logvars)\n",
    "print('The model contains {} nats of information'.format(round(total_entropy_logvars, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
