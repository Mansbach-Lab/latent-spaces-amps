{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e72f7e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main function called /n\n",
      "train_parser function called /n\n",
      "train function called /n\n",
      "parser model_init called /n\n",
      "VAE_shell init called /n\n",
      "VAE_Shell train called /n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda\\envs\\amp21\\lib\\site-packages\\torch\\cuda\\__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  ..\\c10\\cuda\\CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aae loss !\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mG:\\GitHub\\MSCSAM_TBD\\outsourced_models\\vae\\debugging\\scripts\\train.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     97\u001b[0m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_parser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mG:\\GitHub\\MSCSAM_TBD\\outsourced_models\\vae\\debugging\\scripts\\train.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheckpoint\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[0mvae\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 91\u001b[1;33m     vae.train(train_mols, test_mols, train_props, test_props,\n\u001b[0m\u001b[0;32m     92\u001b[0m               epochs=args.epochs, save_freq=args.save_freq)\n\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\GitHub\\MSCSAM_TBD\\outsourced_models\\vae\\debugging\\transvae\\trans_models.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, train_mols, val_mols, train_props, val_props, epochs, save, save_freq, log, log_dir)\u001b[0m\n\u001b[0;32m    267\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'aae'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m#the aae loss takes the discriminator output, latent space and optimizer as input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m                         \u001b[0mx_out\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogvar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred_prop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdisc_out\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlatent_mem\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtgt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtgt_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 269\u001b[1;33m                         loss, bce, kld, prop_mse, disc_loss = aae_loss(src, x_out, mu, logvar,\n\u001b[0m\u001b[0;32m    270\u001b[0m                                                                   \u001b[0mtrue_prop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred_prop\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    271\u001b[0m                                                                   \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'CHAR_WEIGHTS'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\GitHub\\MSCSAM_TBD\\outsourced_models\\vae\\debugging\\transvae\\loss.py\u001b[0m in \u001b[0;36maae_loss\u001b[1;34m(x, x_out, mu, logvar, true_prop, pred_prop, weights, disc_out, latent_codes, opt, beta)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[0mdisc_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mdisc_generator_loss\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mdiscriminator_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m     \u001b[0mdisc_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#backpropagating\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m     \u001b[0mopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0md_opt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\Anaconda\\envs\\amp21\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[1;32m--> 221\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\Anaconda\\envs\\amp21\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[1;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "%run scripts/train.py --model aae --data_source zinc --epochs 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bef6e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.train import train\n",
    "from transvae import rnn_models\n",
    "from transvae import trans_models\n",
    "\n",
    "\n",
    "class Args():\n",
    "    checkpoint=None\n",
    "    adam_lr=3e-4\n",
    "    anneal_start=0\n",
    "    batch_chunks=5\n",
    "    batch_size=1000\n",
    "    beta=0.05\n",
    "    beta_init=1e-8\n",
    "    eps_scale=1\n",
    "    lr_scale=1\n",
    "    warmup_steps=10000\n",
    "    property_predictor=False\n",
    "    save_name=None\n",
    "    d_model=128\n",
    "    d_latent=128\n",
    "    d_property_predictor=256\n",
    "    depth_property_predictor=2\n",
    "    epochs=2\n",
    "    save_freq=5\n",
    "    model = \"rnn\"\n",
    "    data_source = \"zinc\"\n",
    "\n",
    "args = Args()\n",
    "train(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da6e9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_pickle(\"data\\char_dict_zinc.pkl\") #the dict is just a mapping for each possible smile token to an int\n",
    "weights = np.load(\"data\\char_weights_zinc.npy\") \n",
    "train = pd.read_csv('data\\zinc_complete\\zinc_train.txt').to_numpy()\n",
    "from transvae import tvae_util\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b5830c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(df, weights)\n",
    "\n",
    "print(\"with start and end: \",df)\n",
    "df = {i:df[i] for i in df if i!='<end>'}\n",
    "df = {i:df[i] for i in df if i!='<start>'}\n",
    "print(\"withOUT start and end: \",df)\n",
    "\n",
    "parameters_class = {\"NUM_CHAR\" : 24,  \"CHAR_DICT\" : df, \"MAX_LENGTH\" : 126 } \n",
    "params = parameters_class\n",
    "\n",
    "smiles = []\n",
    "for smile in train:\n",
    "    smiles.append(tvae_util.smi_tokenizer(smile[0]))\n",
    "print(smiles[0])\n",
    "\n",
    "char_weights = tvae_util.get_char_weights(smiles,params)\n",
    "print(char_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78c94bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fdd8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "pep = pd.read_csv('data/peptides/1_to_50_reviewed_seq_only.fasta').to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd242ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inspired by the Trans-Vae tokenizer\n",
    "def peptide_tokenizer(peptide):\n",
    "    \"Tokenizes SMILES string\"\n",
    "    #need to remove \"X\", \"B\", \"Z\", \"U\"\n",
    "    pattern =  \"(G|A|L|M|F|W|K|Q|E|S|P|V|I|C|Y|H|R|N|D|T|X|B|Z|U)\"\n",
    "    regezz = re.compile(pattern)\n",
    "    tokens = [token for token in regezz.findall(peptide)]\n",
    "    assert peptide == ''.join(tokens), (\"{} could not be joined\".format(peptide))\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5d56a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "peptides = []\n",
    "for peptide in pep:\n",
    "    peptides.append(peptide_tokenizer(peptide[0]))\n",
    "print(peptides[0], \"longest\", longest)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08c2f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pep_dict = {\"G\": 1, \"A\":2, \"L\":3,\"M\":4,\"F\":5,\"W\":6,\"K\":7,\"Q\":8,\"E\":9,\"S\":10,\"P\":11,\"V\":12,\"I\":13,\"C\":14,\"Y\":15,\"H\":16,\"R\":17,\"N\":18,\"D\":19,\"T\":20,\"X\":21,\"B\":22,\"Z\":23,\"U\":24}\n",
    "parameters = {\"NUM_CHAR\" : 24,  \"CHAR_DICT\" : pep_dict, \"MAX_LENGTH\" : 50 } \n",
    "\n",
    "\n",
    "char_weights = tvae_util.get_char_weights(peptides,parameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbc09a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transvae\n",
    "import os\n",
    "path = os.path.abspath(transvae.__file__)\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4212d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.parallel import DistributedDataParallel as DDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0d30b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run scripts/train.py --model rnn --data_source zinc --DDP True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfeb4017",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
