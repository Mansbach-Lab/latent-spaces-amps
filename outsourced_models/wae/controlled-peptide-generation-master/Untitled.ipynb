{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef70118e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda\\envs\\amp21\\lib\\site-packages\\torch\\cuda\\__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  ..\\c10\\cuda\\CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "2021-06-19 17:21:46,763 - INFO(root): Using device: cpu\n",
      "2021-06-19 17:21:46,765 - INFO(root): Random seed: 1238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_ROOT\t./data_processing/\n",
      "amp:\n",
      "  |- data_kwargs:\n",
      "  |-   |- data_format\tcsv\n",
      "  |-   |- data_path\t./data_processing/amp/\n",
      "  |-   |- fixed_vocab_path\t./data_processing/amp/vocab.dict\n",
      "  |-   |- iteratorspecs:\n",
      "  |-   |-   |- hld_ampneg:\n",
      "  |-   |-   |- hld_amppos:\n",
      "  |-   |-   |- hld_unl:\n",
      "  |-   |-   |- hld_vae:\n",
      "  |-   |-   |-   |- weighted_random_sample\tTrue\n",
      "  |-   |-   |- train_amp_lab:\n",
      "  |-   |-   |-   |- weighted_random_sample\tTrue\n",
      "  |-   |-   |- train_vae:\n",
      "  |-   |-   |-   |- weighted_random_sample\tTrue\n",
      "  |-   |- lower\tFalse\n",
      "  |-   |- split_seed\t1288\n",
      "  |- data_prefixes:\n",
      "  |-   |- dataset_lab\tamp_labeled\n",
      "  |-   |- dataset_type\tbio\n",
      "  |-   |- dataset_unl\tamp_unlabeled\n",
      "config_json\t\n",
      "data_kwargs:\n",
      "  |- data_format\tcsv\n",
      "  |- data_path\t./data_processing/amp/\n",
      "  |- fixed_vocab_path\t./data_processing/amp/vocab.dict\n",
      "  |- iteratorspecs:\n",
      "  |-   |- hld_ampneg:\n",
      "  |-   |- hld_amppos:\n",
      "  |-   |- hld_unl:\n",
      "  |-   |- hld_vae:\n",
      "  |-   |-   |- weighted_random_sample\tTrue\n",
      "  |-   |- train_amp_lab:\n",
      "  |-   |-   |- weighted_random_sample\tTrue\n",
      "  |-   |- train_vae:\n",
      "  |-   |-   |- weighted_random_sample\tTrue\n",
      "  |- lower\tFalse\n",
      "  |- split_seed\t1288\n",
      "data_prefixes:\n",
      "  |- dataset_lab\tamp_labeled\n",
      "  |- dataset_type\tbio\n",
      "  |- dataset_unl\tamp_unlabeled\n",
      "datapath\tdata\n",
      "dataset\tamp\n",
      "evals:\n",
      "  |- sample_modes:\n",
      "  |-   |- beam:\n",
      "  |-   |-   |- beam_size\t5\n",
      "  |-   |-   |- n_best\t3\n",
      "  |-   |-   |- sample_mode\tbeam\n",
      "  |- sample_size\t30\n",
      "full:\n",
      "  |- C_hard_sample_kwargs:\n",
      "  |-   |- sample_mode\tcategorical\n",
      "  |- G_soft_sample_kwargs:\n",
      "  |-   |- sample_mode\tnone_softmax\n",
      "  |- batch_size\t5\n",
      "  |- beta:\n",
      "  |-   |- end:\n",
      "  |-   |-   |- iter\t250000\n",
      "  |-   |-   |- val\t2.0\n",
      "  |-   |- start:\n",
      "  |-   |-   |- iter\t200000\n",
      "  |-   |-   |- val\t2.0\n",
      "  |- cheaplog_every\t10\n",
      "  |- chkpt_path\toutput\\default\\model_{}.pt\n",
      "  |- classifier_min_length\t5\n",
      "  |- clip_grad\t5.0\n",
      "  |- eval_path\toutput\\default\\full_eval.txt\n",
      "  |- expsvlog_every\t25\n",
      "  |- fasta_gen_samples_path\toutput\\default\\full_gen.fasta\n",
      "  |- fasta_pos_samples_path\toutput\\default\\pos_gen.fasta\n",
      "  |- gen_samples_path\toutput\\default\\full_gen.txt\n",
      "  |- interp_samples_path\toutput\\default\\full_interp.txt\n",
      "  |- lambda_c\t1.0\n",
      "  |- lambda_e\t0.1\n",
      "  |- lambda_logvar_KL\t0.001\n",
      "  |- lambda_logvar_L1\t0.0\n",
      "  |- lambda_u\t0.1\n",
      "  |- lambda_z\t0.1\n",
      "  |- lrC\t0.0003\n",
      "  |- lrE\t0.0003\n",
      "  |- lrG\t0.0003\n",
      "  |- n_iter\t100\n",
      "  |- pos_eval_path\toutput\\default\\full.pos_eval.txt\n",
      "  |- posz_samples_path\toutput\\default\\full_posz.txt\n",
      "  |- s_iter\t100\n",
      "  |- samez_samples_path\toutput\\default\\full_samez.txt\n",
      "  |- softmax_temp:\n",
      "  |-   |- end:\n",
      "  |-   |-   |- iter\t250000\n",
      "  |-   |-   |- val\t1.0\n",
      "  |-   |- start:\n",
      "  |-   |-   |- iter\t200000\n",
      "  |-   |-   |- val\t1.0\n",
      "  |- z_regu_loss\tmmdrf\n",
      "ignore_gpu\tFalse\n",
      "loadpath\t\n",
      "losses:\n",
      "  |- wae_mmd:\n",
      "  |-   |- kernel\tgaussian\n",
      "  |-   |- rf_dim\t500\n",
      "  |-   |- rf_resample\tFalse\n",
      "  |-   |- sigma\t7.0\n",
      "max_seq_len\t25\n",
      "model:\n",
      "  |- C_args:\n",
      "  |-   |- dropout\t0.5\n",
      "  |-   |- max_filter_width\t5\n",
      "  |-   |- min_filter_width\t3\n",
      "  |-   |- num_filters\t100\n",
      "  |- E_args:\n",
      "  |-   |- biGRU\tTrue\n",
      "  |-   |- h_dim\t80\n",
      "  |-   |- layers\t1\n",
      "  |-   |- p_dropout\t0.0\n",
      "  |- G_args:\n",
      "  |-   |- GRU_args:\n",
      "  |-   |-   |- p_out_dropout\t0.3\n",
      "  |-   |-   |- p_word_dropout\t0.3\n",
      "  |-   |-   |- skip_connetions\tFalse\n",
      "  |-   |- G_class\tgru\n",
      "  |-   |- deconv_args:\n",
      "  |-   |-   |- add_final_conv_layer\tTrue\n",
      "  |-   |-   |- kernel_size\t4\n",
      "  |-   |-   |- max_seq_len\t25\n",
      "  |-   |-   |- num_conv_layers\t2\n",
      "  |-   |-   |- num_deconv_layers\t3\n",
      "  |-   |-   |- num_filters\t100\n",
      "  |-   |-   |- temperature\t1.0\n",
      "  |-   |-   |- useRNN\tFalse\n",
      "  |-   |-   |- use_batch_norm\tTrue\n",
      "  |- c_dim\t2\n",
      "  |- emb_dim\t150\n",
      "  |- flow\t0\n",
      "  |- flow_type\t\n",
      "  |- freeze_embeddings\tFalse\n",
      "  |- z_dim\t100\n",
      "part\t0\n",
      "partN\t1\n",
      "phase\t-1\n",
      "resume_result_json\tFalse\n",
      "runname\tdefault\n",
      "savepath\toutput\\default\n",
      "savepath_toplevel\toutput\n",
      "seed\t1238\n",
      "shared:\n",
      "  |- batch_size\t5\n",
      "  |- cheaplog_every\t10\n",
      "  |- clip_grad\t5.0\n",
      "  |- expsvlog_every\t25\n",
      "  |- n_iter\t100\n",
      "tb_toplevel\ttb\n",
      "tbpath\ttb\\default\n",
      "tiny\tTrue\n",
      "vae:\n",
      "  |- batch_size\t5\n",
      "  |- beta:\n",
      "  |-   |- end:\n",
      "  |-   |-   |- iter\t40000\n",
      "  |-   |-   |- val\t2.0\n",
      "  |-   |- start:\n",
      "  |-   |-   |- iter\t0\n",
      "  |-   |-   |- val\t1.0\n",
      "  |- cheaplog_every\t10\n",
      "  |- chkpt_path\toutput\\default\\model_{}.pt\n",
      "  |- clip_grad\t5.0\n",
      "  |- eval_path\toutput\\default\\vae_eval.txt\n",
      "  |- expsvlog_every\t25\n",
      "  |- fasta_gen_samples_path\toutput\\default\\vae_gen.fasta\n",
      "  |- gen_samples_path\toutput\\default\\vae_gen.txt\n",
      "  |- lambda_logvar_KL\t0.001\n",
      "  |- lambda_logvar_L1\t0.0\n",
      "  |- lr\t0.001\n",
      "  |- n_iter\t100\n",
      "  |- s_iter\t0\n",
      "  |- z_regu_loss\tmmdrf\n",
      "vocab_path\toutput\\default\\vocab.dict\n",
      "Loading Dataset...\n",
      "fields ::  OrderedDict([('text', ('text', <torchtext.data.field.Field object at 0x0000022AFF49D0A0>)), ('amp', ('amp', <data_processing.dataset.AttributeField object at 0x0000022AFF49DC10>)), ('tox', ('tox', <data_processing.dataset.AttributeField object at 0x0000022AFF49D100>)), ('sol', ('sol', <data_processing.dataset.AttributeField object at 0x0000022AFF49DE20>)), ('anticancer', ('anticancer', <data_processing.dataset.AttributeField object at 0x0000022AFF49DCD0>)), ('antihyper', ('antihyper', <data_processing.dataset.AttributeField object at 0x0000022AFF49DD30>)), ('hormone', ('hormone', <data_processing.dataset.AttributeField object at 0x0000022AFF49DC40>))])\n",
      "Load csv file ./data_processing/amp/unlab.csv\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data_processing/amp/unlab.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mG:\\GitHub\\MSCSAM_TBD\\outsourced_models\\wae\\controlled-peptide-generation-master\\main.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;31m# DATA\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m dataset = AttributeDataLoader(mbsize=cfg.vae.batch_size, max_seq_len=cfg.max_seq_len,\n\u001b[0m\u001b[0;32m     53\u001b[0m                               \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m                               \u001b[0mattributes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcfg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattributes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\GitHub\\MSCSAM_TBD\\outsourced_models\\wae\\controlled-peptide-generation-master\\data_processing\\dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, mbsize, max_seq_len, data_path, data_format, lower, emb_dim, glove_cache, attributes, csv_files, split_seed, iteratorspecs, fixed_vocab_path, device)\u001b[0m\n\u001b[0;32m    257\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_seq_len\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax_seq_len\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 259\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMultiCsvReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcsv_files\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_seq_len\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfields\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    260\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_splits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0.8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msplit_seed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterators\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubsets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_subset_iterators\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miteratorspecs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmbsize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\GitHub\\MSCSAM_TBD\\outsourced_models\\wae\\controlled-peptide-generation-master\\data_processing\\dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, path, csv_files, max_seq_len, fields, csv_reader_params)\u001b[0m\n\u001b[0;32m     93\u001b[0m             \u001b[0mfn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcsv_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Load csv file'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parse_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcsv_reader_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     96\u001b[0m         \u001b[1;31m# Done with construction, make list of Example s from dicst of dicts\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Construct torchtext Example objects'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\GitHub\\MSCSAM_TBD\\outsourced_models\\wae\\controlled-peptide-generation-master\\data_processing\\dataset.py\u001b[0m in \u001b[0;36m_parse_csv\u001b[1;34m(self, fn, csv_reader_params)\u001b[0m\n\u001b[0;32m    108\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_parse_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcsv_reader_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m         \u001b[1;34m\"\"\" deviating from torchtext, just care about python 3 here \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"utf8\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m             \u001b[0mreader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDictReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcsv_reader_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mreader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data_processing/amp/unlab.csv'"
     ]
    }
   ],
   "source": [
    "%run -i main.py --tiny 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ffb8188",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda\\envs\\amp21\\lib\\site-packages\\torch\\cuda\\__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  ..\\c10\\cuda\\CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "2021-06-19 17:23:58,741 - INFO(root): Using device: cpu\n",
      "2021-06-19 17:23:58,742 - INFO(root): Random seed: 1238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_ROOT\t./data_processing/\n",
      "amp:\n",
      "  |- data_kwargs:\n",
      "  |-   |- data_format\tcsv\n",
      "  |-   |- data_path\t./data_processing/amp/\n",
      "  |-   |- fixed_vocab_path\t./data_processing/amp/vocab.dict\n",
      "  |-   |- iteratorspecs:\n",
      "  |-   |-   |- hld_ampneg:\n",
      "  |-   |-   |- hld_amppos:\n",
      "  |-   |-   |- hld_unl:\n",
      "  |-   |-   |- hld_vae:\n",
      "  |-   |-   |-   |- weighted_random_sample\tTrue\n",
      "  |-   |-   |- train_amp_lab:\n",
      "  |-   |-   |-   |- weighted_random_sample\tTrue\n",
      "  |-   |-   |- train_vae:\n",
      "  |-   |-   |-   |- weighted_random_sample\tTrue\n",
      "  |-   |- lower\tFalse\n",
      "  |-   |- split_seed\t1288\n",
      "  |- data_prefixes:\n",
      "  |-   |- dataset_lab\tamp_labeled\n",
      "  |-   |- dataset_type\tbio\n",
      "  |-   |- dataset_unl\tamp_unlabeled\n",
      "config_json\t\n",
      "data_kwargs:\n",
      "  |- data_format\tcsv\n",
      "  |- data_path\t./data_processing/amp/\n",
      "  |- fixed_vocab_path\t./data_processing/amp/vocab.dict\n",
      "  |- iteratorspecs:\n",
      "  |-   |- hld_ampneg:\n",
      "  |-   |- hld_amppos:\n",
      "  |-   |- hld_unl:\n",
      "  |-   |- hld_vae:\n",
      "  |-   |-   |- weighted_random_sample\tTrue\n",
      "  |-   |- train_amp_lab:\n",
      "  |-   |-   |- weighted_random_sample\tTrue\n",
      "  |-   |- train_vae:\n",
      "  |-   |-   |- weighted_random_sample\tTrue\n",
      "  |- lower\tFalse\n",
      "  |- split_seed\t1288\n",
      "data_prefixes:\n",
      "  |- dataset_lab\tamp_labeled\n",
      "  |- dataset_type\tbio\n",
      "  |- dataset_unl\tamp_unlabeled\n",
      "datapath\tdata\n",
      "dataset\tamp\n",
      "evals:\n",
      "  |- sample_modes:\n",
      "  |-   |- beam:\n",
      "  |-   |-   |- beam_size\t5\n",
      "  |-   |-   |- n_best\t3\n",
      "  |-   |-   |- sample_mode\tbeam\n",
      "  |- sample_size\t2000\n",
      "full:\n",
      "  |- C_hard_sample_kwargs:\n",
      "  |-   |- sample_mode\tcategorical\n",
      "  |- G_soft_sample_kwargs:\n",
      "  |-   |- sample_mode\tnone_softmax\n",
      "  |- batch_size\t32\n",
      "  |- beta:\n",
      "  |-   |- end:\n",
      "  |-   |-   |- iter\t250000\n",
      "  |-   |-   |- val\t2.0\n",
      "  |-   |- start:\n",
      "  |-   |-   |- iter\t200000\n",
      "  |-   |-   |- val\t2.0\n",
      "  |- cheaplog_every\t50\n",
      "  |- chkpt_path\toutput\\default\\model_{}.pt\n",
      "  |- classifier_min_length\t5\n",
      "  |- clip_grad\t5.0\n",
      "  |- eval_path\toutput\\default\\full_eval.txt\n",
      "  |- expsvlog_every\t2000\n",
      "  |- fasta_gen_samples_path\toutput\\default\\full_gen.fasta\n",
      "  |- fasta_pos_samples_path\toutput\\default\\pos_gen.fasta\n",
      "  |- gen_samples_path\toutput\\default\\full_gen.txt\n",
      "  |- interp_samples_path\toutput\\default\\full_interp.txt\n",
      "  |- lambda_c\t1.0\n",
      "  |- lambda_e\t0.1\n",
      "  |- lambda_logvar_KL\t0.001\n",
      "  |- lambda_logvar_L1\t0.0\n",
      "  |- lambda_u\t0.1\n",
      "  |- lambda_z\t0.1\n",
      "  |- lrC\t0.0003\n",
      "  |- lrE\t0.0003\n",
      "  |- lrG\t0.0003\n",
      "  |- n_iter\t50000\n",
      "  |- pos_eval_path\toutput\\default\\full.pos_eval.txt\n",
      "  |- posz_samples_path\toutput\\default\\full_posz.txt\n",
      "  |- s_iter\t200000\n",
      "  |- samez_samples_path\toutput\\default\\full_samez.txt\n",
      "  |- softmax_temp:\n",
      "  |-   |- end:\n",
      "  |-   |-   |- iter\t250000\n",
      "  |-   |-   |- val\t1.0\n",
      "  |-   |- start:\n",
      "  |-   |-   |- iter\t200000\n",
      "  |-   |-   |- val\t1.0\n",
      "  |- z_regu_loss\tmmdrf\n",
      "ignore_gpu\tFalse\n",
      "loadpath\t\n",
      "losses:\n",
      "  |- wae_mmd:\n",
      "  |-   |- kernel\tgaussian\n",
      "  |-   |- rf_dim\t500\n",
      "  |-   |- rf_resample\tFalse\n",
      "  |-   |- sigma\t7.0\n",
      "max_seq_len\t25\n",
      "model:\n",
      "  |- C_args:\n",
      "  |-   |- dropout\t0.5\n",
      "  |-   |- max_filter_width\t5\n",
      "  |-   |- min_filter_width\t3\n",
      "  |-   |- num_filters\t100\n",
      "  |- E_args:\n",
      "  |-   |- biGRU\tTrue\n",
      "  |-   |- h_dim\t80\n",
      "  |-   |- layers\t1\n",
      "  |-   |- p_dropout\t0.0\n",
      "  |- G_args:\n",
      "  |-   |- GRU_args:\n",
      "  |-   |-   |- p_out_dropout\t0.3\n",
      "  |-   |-   |- p_word_dropout\t0.3\n",
      "  |-   |-   |- skip_connetions\tFalse\n",
      "  |-   |- G_class\tgru\n",
      "  |-   |- deconv_args:\n",
      "  |-   |-   |- add_final_conv_layer\tTrue\n",
      "  |-   |-   |- kernel_size\t4\n",
      "  |-   |-   |- max_seq_len\t25\n",
      "  |-   |-   |- num_conv_layers\t2\n",
      "  |-   |-   |- num_deconv_layers\t3\n",
      "  |-   |-   |- num_filters\t100\n",
      "  |-   |-   |- temperature\t1.0\n",
      "  |-   |-   |- useRNN\tFalse\n",
      "  |-   |-   |- use_batch_norm\tTrue\n",
      "  |- c_dim\t2\n",
      "  |- emb_dim\t150\n",
      "  |- flow\t0\n",
      "  |- flow_type\t\n",
      "  |- freeze_embeddings\tFalse\n",
      "  |- z_dim\t100\n",
      "part\t0\n",
      "partN\t1\n",
      "phase\t1\n",
      "resume_result_json\tTrue\n",
      "runname\tdefault\n",
      "savepath\toutput\\default\n",
      "savepath_toplevel\toutput\n",
      "seed\t1238\n",
      "shared:\n",
      "  |- clip_grad\t5.0\n",
      "tb_toplevel\ttb\n",
      "tbpath\ttb\\default\n",
      "tiny\tFalse\n",
      "vae:\n",
      "  |- batch_size\t32\n",
      "  |- beta:\n",
      "  |-   |- end:\n",
      "  |-   |-   |- iter\t40000\n",
      "  |-   |-   |- val\t2.0\n",
      "  |-   |- start:\n",
      "  |-   |-   |- iter\t0\n",
      "  |-   |-   |- val\t1.0\n",
      "  |- cheaplog_every\t500\n",
      "  |- chkpt_path\toutput\\default\\model_{}.pt\n",
      "  |- clip_grad\t5.0\n",
      "  |- eval_path\toutput\\default\\vae_eval.txt\n",
      "  |- expsvlog_every\t20000\n",
      "  |- fasta_gen_samples_path\toutput\\default\\vae_gen.fasta\n",
      "  |- gen_samples_path\toutput\\default\\vae_gen.txt\n",
      "  |- lambda_logvar_KL\t0.001\n",
      "  |- lambda_logvar_L1\t0.0\n",
      "  |- lr\t0.001\n",
      "  |- n_iter\t200000\n",
      "  |- s_iter\t0\n",
      "  |- z_regu_loss\tmmdrf\n",
      "vocab_path\toutput\\default\\vocab.dict\n",
      "Loading Dataset...\n",
      "fields ::  OrderedDict([('text', ('text', <torchtext.data.field.Field object at 0x0000021178092400>)), ('amp', ('amp', <data_processing.dataset.AttributeField object at 0x0000021178092130>)), ('tox', ('tox', <data_processing.dataset.AttributeField object at 0x0000021178092160>)), ('sol', ('sol', <data_processing.dataset.AttributeField object at 0x00000211780923D0>)), ('anticancer', ('anticancer', <data_processing.dataset.AttributeField object at 0x00000211780924C0>)), ('antihyper', ('antihyper', <data_processing.dataset.AttributeField object at 0x00000211780925B0>)), ('hormone', ('hormone', <data_processing.dataset.AttributeField object at 0x0000021178092550>))])\n",
      "Load csv file ./data_processing/amp/unlab.csv\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data_processing/amp/unlab.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mG:\\GitHub\\MSCSAM_TBD\\outsourced_models\\wae\\controlled-peptide-generation-master\\main.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;31m# DATA\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m dataset = AttributeDataLoader(mbsize=cfg.vae.batch_size, max_seq_len=cfg.max_seq_len,\n\u001b[0m\u001b[0;32m     53\u001b[0m                               \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m                               \u001b[0mattributes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcfg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattributes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\GitHub\\MSCSAM_TBD\\outsourced_models\\wae\\controlled-peptide-generation-master\\data_processing\\dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, mbsize, max_seq_len, data_path, data_format, lower, emb_dim, glove_cache, attributes, csv_files, split_seed, iteratorspecs, fixed_vocab_path, device)\u001b[0m\n\u001b[0;32m    257\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_seq_len\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax_seq_len\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 259\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMultiCsvReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcsv_files\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_seq_len\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfields\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    260\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_splits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0.8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msplit_seed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterators\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubsets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_subset_iterators\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miteratorspecs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmbsize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\GitHub\\MSCSAM_TBD\\outsourced_models\\wae\\controlled-peptide-generation-master\\data_processing\\dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, path, csv_files, max_seq_len, fields, csv_reader_params)\u001b[0m\n\u001b[0;32m     93\u001b[0m             \u001b[0mfn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcsv_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Load csv file'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parse_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcsv_reader_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     96\u001b[0m         \u001b[1;31m# Done with construction, make list of Example s from dicst of dicts\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Construct torchtext Example objects'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\GitHub\\MSCSAM_TBD\\outsourced_models\\wae\\controlled-peptide-generation-master\\data_processing\\dataset.py\u001b[0m in \u001b[0;36m_parse_csv\u001b[1;34m(self, fn, csv_reader_params)\u001b[0m\n\u001b[0;32m    108\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_parse_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcsv_reader_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m         \u001b[1;34m\"\"\" deviating from torchtext, just care about python 3 here \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"utf8\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m             \u001b[0mreader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDictReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcsv_reader_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mreader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data_processing/amp/unlab.csv'"
     ]
    }
   ],
   "source": [
    "%run -i main.py --phase 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52342954",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
